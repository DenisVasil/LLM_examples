{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3i3JeZF+irYSyQIDl3sL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenisVasil/LLM_examples/blob/main/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Models (čata modeļi)"
      ],
      "metadata": {
        "id": "vBOJzQF4bGPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "wa0lp07tR4Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6c9f61-4169-47d1-8287-e10526826c90"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.0.9-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.8.4)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.33)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.10.6)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.25.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.26.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.3.6)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.9-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain_google_genai\n",
            "Successfully installed filetype-1.2.0 langchain_google_genai-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjzm9ulcRYUu",
        "outputId": "800cadd2-087d-4299-e49d-afa00e0382c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full result:\n",
            "content='81 divided by 9 is 9.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-24d7f7c1-7122-4f1b-9d5e-6bcd40e69444-0' usage_metadata={'input_tokens': 9, 'output_tokens': 11, 'total_tokens': 20, 'input_token_details': {'cache_read': 0}}\n",
            "Context:\n",
            "81 divided by 9 is 9.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "result = model.invoke(\"What is 81 divided by 9\")\n",
        "\n",
        "print(\"Full result:\")\n",
        "print(result)\n",
        "print(\"Context:\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Conversation\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problem\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\")\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "\n",
        "print(f\"Answer from AI: {result.content}\")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problem\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "    AIMessage(content=f\"{result.content}\"),\n",
        "    HumanMessage(content=\"Divide this result by 3\")\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "print(f\"New answer from AI: {result.content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EtoGySeR2l4",
        "outputId": "715d2325-2f93-44d5-9330-d13d485a3f7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from AI: 81 divided by 9 is 9.\n",
            "New answer from AI: 9 divided by 3 is 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory.chat_memory import InMemoryChatMessageHistory\n",
        "\n",
        "history = InMemoryChatMessageHistory()\n",
        "history.add_message({\"role\": \"user\", \"content\": \"Hello\"})\n",
        "history.add_message({\"role\": \"assistant\", \"content\": \"Hi there!\"})\n",
        "\n",
        "print(history.messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU4PBcJF2-2I",
        "outputId": "09ce95fe-b28b-4a6d-c18c-86849c589a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'user', 'content': 'Hello'}, {'role': 'assistant', 'content': 'Hi there!'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import HumanMessage, AIMessage # Import necessary classes\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "memory.chat_memory.add_message(HumanMessage(content=\"Hello\")) # Use HumanMessage\n",
        "memory.chat_memory.add_message(AIMessage(content=\"Hi there!\")) # Use AIMessage\n",
        "\n",
        "print(memory.load_memory_variables({}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hfMLbpa2glh",
        "outputId": "0f50fdd9-b75c-497c-86bc-6b614d6468e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'history': 'Human: Hello\\nAI: Hi there!'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-27d192231410>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chat\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "system_message = SystemMessage(content=\"You are helpful AI assistant\")\n",
        "chat_history.append(system_message)\n",
        "\n",
        "# chat loop\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    chat_history.append(HumanMessage(content=query))\n",
        "\n",
        "    result = model.invoke(chat_history)\n",
        "    response = result.content\n",
        "    chat_history.append(AIMessage(content=response))\n",
        "    print(f\"AI response: {response}\")\n",
        "\n",
        "print(\"----- Message history ------\")\n",
        "print(chat_history)"
      ],
      "metadata": {
        "id": "tuJXOC96R27m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44da7a89-18d0-404d-db50-9dcfb58cf338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: How many birds are there in Latvia?\n",
            "AI response: It's impossible to give an exact number of birds in Latvia at any given moment.  Bird populations are dynamic, constantly changing due to:\n",
            "\n",
            "* **Migration:** Latvia is on a major migration route, so the number of birds present fluctuates dramatically throughout the year.  Millions of birds pass through during spring and fall migrations.\n",
            "* **Breeding Cycles:** Bird numbers increase during breeding seasons as chicks hatch and fledge.\n",
            "* **Food Availability:** Bird populations are affected by the availability of food resources, which can vary seasonally and annually.\n",
            "* **Weather Conditions:** Harsh winters or extreme weather events can impact bird survival rates.\n",
            "* **Habitat Changes:** Loss of habitat due to human activities affects bird populations.\n",
            "\n",
            "While an exact count isn't feasible, ornithologists and researchers in Latvia *can* estimate populations of specific species through various methods like bird counts, ringing, and nest monitoring.  These estimates help track population trends and inform conservation efforts.\n",
            "\n",
            "So, instead of a total number, you can find information on estimated populations of *particular bird species* in Latvia by consulting resources from Latvian ornithological organizations or research institutions.\n",
            "You: Which birds are most common?\n",
            "AI response: Some of the most common bird species in Latvia include:\n",
            "\n",
            "* **Great Tit ( *Parus major* ):** A widespread and adaptable species found in various habitats, including forests, parks, and gardens.\n",
            "* **Chaffinch (*Fringilla coelebs*):**  A common finch found in woodlands, parks, and gardens.  Their cheerful song is a familiar sound.\n",
            "* **Willow Warbler (*Phylloscopus trochilus*):** A small, migratory warbler that breeds in a variety of habitats with trees or shrubs.\n",
            "* **Common Whitethroat (*Sylvia communis*):** Another common warbler found in scrubland, hedgerows, and open woodland.\n",
            "* **Robin (*Erithacus rubecula*):** A familiar garden bird known for its red breast and territorial behavior.\n",
            "* **Blackbird (*Turdus merula*):**  A widespread thrush found in woodlands, gardens, and parks.\n",
            "* **Starling (*Sturnus vulgaris*):** A highly adaptable species found in various habitats, often forming large flocks.\n",
            "* **House Sparrow (*Passer domesticus*):** A common bird closely associated with human settlements.\n",
            "* **Tree Sparrow (*Passer montanus*):**  Similar to the House Sparrow but more often found in rural areas and woodlands.\n",
            "* **White Wagtail (*Motacilla alba*):** A distinctive bird with a long tail, often seen near water.\n",
            "\n",
            "This is not an exhaustive list, and the prevalence of certain species can vary depending on the specific region and habitat within Latvia.  You can find more comprehensive lists and detailed information from Latvian ornithological organizations.\n",
            "You: Which are most rare?\n",
            "AI response: Some of the rarest breeding birds in Latvia include:\n",
            "\n",
            "* **Aquatic Warbler (*Acrocephalus paludicola*):** This globally threatened species uses sedge meadows for breeding and its population is declining. Latvia holds an important responsibility for its conservation.\n",
            "* **Corncrake (*Crex crex*):**  A secretive bird of grasslands and meadows, its numbers have declined significantly due to changes in agricultural practices.\n",
            "* **Lesser Spotted Eagle (*Clanga pomarina*):** A raptor that prefers mature forests for nesting, facing habitat loss and disturbance.\n",
            "* **Greater Spotted Eagle (*Clanga clanga*):** Another large raptor facing similar threats to the Lesser Spotted Eagle. Differentiating them can be challenging.\n",
            "* **European Roller (*Coracias garrulus*):**  A colorful bird found in open woodlands and farmland, its population is small and fragmented.\n",
            "* **Ortolan Bunting (*Emberiza hortulana*):**  A secretive bunting found in dry, open habitats, its numbers are declining.\n",
            "* **Black-tailed Godwit (*Limosa limosa*):** A large wader that breeds in wet meadows and marshes, facing habitat loss.\n",
            "\n",
            "\n",
            "It's important to note that \"rare\" can have different meanings.  Some species are naturally rare with small populations, while others have become rare due to population declines.  Also, some species may be rare breeders in Latvia but more common during migration.  For the most up-to-date and detailed information, consult resources from Latvian ornithological organizations and conservation groups.  They can provide specific data and insights into the status of these and other rare bird species in the country.\n",
            "You: exit\n",
            "----- Message history ------\n",
            "[SystemMessage(content='You are helpful AI assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='How many birds are there in Latvia?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"It's impossible to give an exact number of birds in Latvia at any given moment.  Bird populations are dynamic, constantly changing due to:\\n\\n* **Migration:** Latvia is on a major migration route, so the number of birds present fluctuates dramatically throughout the year.  Millions of birds pass through during spring and fall migrations.\\n* **Breeding Cycles:** Bird numbers increase during breeding seasons as chicks hatch and fledge.\\n* **Food Availability:** Bird populations are affected by the availability of food resources, which can vary seasonally and annually.\\n* **Weather Conditions:** Harsh winters or extreme weather events can impact bird survival rates.\\n* **Habitat Changes:** Loss of habitat due to human activities affects bird populations.\\n\\nWhile an exact count isn't feasible, ornithologists and researchers in Latvia *can* estimate populations of specific species through various methods like bird counts, ringing, and nest monitoring.  These estimates help track population trends and inform conservation efforts.\\n\\nSo, instead of a total number, you can find information on estimated populations of *particular bird species* in Latvia by consulting resources from Latvian ornithological organizations or research institutions.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Which birds are most common?', additional_kwargs={}, response_metadata={}), AIMessage(content='Some of the most common bird species in Latvia include:\\n\\n* **Great Tit ( *Parus major* ):** A widespread and adaptable species found in various habitats, including forests, parks, and gardens.\\n* **Chaffinch (*Fringilla coelebs*):**  A common finch found in woodlands, parks, and gardens.  Their cheerful song is a familiar sound.\\n* **Willow Warbler (*Phylloscopus trochilus*):** A small, migratory warbler that breeds in a variety of habitats with trees or shrubs.\\n* **Common Whitethroat (*Sylvia communis*):** Another common warbler found in scrubland, hedgerows, and open woodland.\\n* **Robin (*Erithacus rubecula*):** A familiar garden bird known for its red breast and territorial behavior.\\n* **Blackbird (*Turdus merula*):**  A widespread thrush found in woodlands, gardens, and parks.\\n* **Starling (*Sturnus vulgaris*):** A highly adaptable species found in various habitats, often forming large flocks.\\n* **House Sparrow (*Passer domesticus*):** A common bird closely associated with human settlements.\\n* **Tree Sparrow (*Passer montanus*):**  Similar to the House Sparrow but more often found in rural areas and woodlands.\\n* **White Wagtail (*Motacilla alba*):** A distinctive bird with a long tail, often seen near water.\\n\\nThis is not an exhaustive list, and the prevalence of certain species can vary depending on the specific region and habitat within Latvia.  You can find more comprehensive lists and detailed information from Latvian ornithological organizations.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Which are most rare?', additional_kwargs={}, response_metadata={}), AIMessage(content='Some of the rarest breeding birds in Latvia include:\\n\\n* **Aquatic Warbler (*Acrocephalus paludicola*):** This globally threatened species uses sedge meadows for breeding and its population is declining. Latvia holds an important responsibility for its conservation.\\n* **Corncrake (*Crex crex*):**  A secretive bird of grasslands and meadows, its numbers have declined significantly due to changes in agricultural practices.\\n* **Lesser Spotted Eagle (*Clanga pomarina*):** A raptor that prefers mature forests for nesting, facing habitat loss and disturbance.\\n* **Greater Spotted Eagle (*Clanga clanga*):** Another large raptor facing similar threats to the Lesser Spotted Eagle. Differentiating them can be challenging.\\n* **European Roller (*Coracias garrulus*):**  A colorful bird found in open woodlands and farmland, its population is small and fragmented.\\n* **Ortolan Bunting (*Emberiza hortulana*):**  A secretive bunting found in dry, open habitats, its numbers are declining.\\n* **Black-tailed Godwit (*Limosa limosa*):** A large wader that breeds in wet meadows and marshes, facing habitat loss.\\n\\n\\nIt\\'s important to note that \"rare\" can have different meanings.  Some species are naturally rare with small populations, while others have become rare due to population declines.  Also, some species may be rare breeders in Latvia but more common during migration.  For the most up-to-date and detailed information, consult resources from Latvian ornithological organizations and conservation groups.  They can provide specific data and insights into the status of these and other rare bird species in the country.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv1IsvPpfhs3",
        "outputId": "1364d5d8-c4f4-4986-cc15-468efc8d0665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.11/dist-packages (1.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Chat History\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.memory.chat_memory import InMemoryChatMessageHistory\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "import duckdb\n",
        "import os\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Constants\n",
        "DB_PATH = \"chat_history.db\"\n",
        "TABLE_NAME = \"chat_history\"\n",
        "SESSION_ID = \"user_session_new\"  # This could be a username or unique ID\n",
        "\n",
        "# Initialize DuckDB\n",
        "con = duckdb.connect(DB_PATH)\n",
        "con.execute(f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
        "    session_id TEXT,\n",
        "    role TEXT,\n",
        "    content TEXT\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Helper Functions\n",
        "\n",
        "\n",
        "def save_message_to_db(session_id, role, content):\n",
        "    \"\"\"Save a single message to the DuckDB database.\"\"\"\n",
        "    con.execute(f\"\"\"\n",
        "    INSERT INTO {TABLE_NAME} (session_id, role, content)\n",
        "    VALUES (?, ?, ?)\n",
        "    \"\"\", [session_id, role, content])\n",
        "\n",
        "\n",
        "def load_messages_from_db(session_id):\n",
        "    \"\"\"Load chat history for a specific session from the DuckDB database.\"\"\"\n",
        "    result = con.execute(f\"\"\"\n",
        "    SELECT role, content FROM {TABLE_NAME} WHERE session_id = ? ORDER BY rowid\n",
        "    \"\"\", [session_id]).fetchall()\n",
        "    return [HumanMessage(content=row[1]) if row[0] == \"user\" else AIMessage(content=row[1]) for row in result]\n",
        "\n",
        "\n",
        "# Initialize Chat Message History\n",
        "print(\"Initializing DuckDB Chat Message History...\")\n",
        "messages = load_messages_from_db(SESSION_ID)\n",
        "chat_history = InMemoryChatMessageHistory(messages=messages)\n",
        "\n",
        "print(\"Chat History Initialized.\")\n",
        "print(\"Current Chat History:\", [msg.content for msg in chat_history.messages])\n",
        "\n",
        "\n",
        "print(\"Start chatting with the AI. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    human_input = input(\"User: \")\n",
        "    if human_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Save and process user message\n",
        "    chat_history.add_user_message(human_input)\n",
        "    save_message_to_db(SESSION_ID, \"user\", human_input)\n",
        "\n",
        "    # Generate AI response\n",
        "    ai_response = model.invoke(chat_history.messages)\n",
        "    chat_history.add_ai_message(ai_response.content)\n",
        "    save_message_to_db(SESSION_ID, \"ai\", ai_response.content)\n",
        "\n",
        "    print(f\"AI: {ai_response.content}\")\n",
        "\n",
        "# Close DuckDB connection when done\n",
        "con.close()"
      ],
      "metadata": {
        "id": "n1LJKaPPdZrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6653fcab-af8e-484c-8ce1-b1530896fb08"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing DuckDB Chat Message History...\n",
            "Chat History Initialized.\n",
            "Current Chat History: []\n",
            "Start chatting with the AI. Type 'exit' to quit.\n",
            "User: How many bird species are there in Latvia?\n",
            "AI: While the total number of bird species *ever recorded* in Latvia is over 360, the number of *regularly occurring* species is closer to **around 300**.  This includes breeding species, regular migrants, and winter visitors.  The exact number fluctuates slightly based on taxonomic changes and ongoing research.\n",
            "User: Which species are the most common?\n",
            "AI: Some of the most common bird species in Latvia include:\n",
            "\n",
            "* **Great Tit (Parus major):**  A widespread and adaptable species found in various habitats, including forests, parks, and gardens.\n",
            "* **Willow Warbler (Phylloscopus trochilus):** A common summer migrant, filling woodlands with its distinctive song.\n",
            "* **Chaffinch (Fringilla coelebs):** Another abundant finch, often seen in gardens and woodlands.\n",
            "* **Common Starling (Sturnus vulgaris):**  Highly adaptable and found in a wide range of habitats, known for their murmurations.\n",
            "* **White Wagtail (Motacilla alba):** A familiar sight in urban and rural areas, often seen near water.\n",
            "* **Robin (Erithacus rubecula):** A widespread and recognizable species, found in woodlands, gardens, and parks.\n",
            "* **Blackbird (Turdus merula):** A common resident in various habitats.\n",
            "* **Blue Tit (Cyanistes caeruleus):** Another common garden bird, known for its vibrant blue and yellow plumage.\n",
            "* **Tree Pipit (Anthus trivialis):** A common summer visitor, often found in open woodlands and heathland.\n",
            "* **Chiffchaff (Phylloscopus collybita):** A small, migratory warbler, common in woodlands and gardens.\n",
            "\n",
            "\n",
            "It's important to note that \"common\" can vary based on habitat and time of year.  These species are generally widespread and frequently observed throughout Latvia.\n",
            "User: Which are most rare?\n",
            "AI: Some of the rarest bird species in Latvia, including those considered critically endangered or regionally extinct, are:\n",
            "\n",
            "* **Aquatic Warbler (Acrocephalus paludicola):**  Critically endangered globally, and while Latvia has historically been an important breeding ground, its population has severely declined.\n",
            "* **Corncrake (Crex crex):**  Declining across Europe, and though still present in Latvia, it is becoming increasingly rare.\n",
            "* **Greater Spotted Eagle (Clanga clanga):**  A rare breeder and passage migrant, facing threats from habitat loss.\n",
            "* **Lesser Spotted Eagle (Clanga pomarina):**  Also a rare breeder, with a declining population.\n",
            "* **European Roller (Coracias garrulus):**  A rare vagrant, with only occasional sightings in Latvia.\n",
            "* **Saker Falcon (Falco cherrug):**  A very rare vagrant, historically bred but now only occasionally seen.\n",
            "* **White-tailed Eagle (Haliaeetus albicilla):**  While its population is recovering, it's still considered relatively rare compared to other raptors.\n",
            "* **Black Stork (Ciconia nigra):**  Less common than the White Stork, and considered a rare breeder.\n",
            "\n",
            "\n",
            "The status of these species can change, and ongoing conservation efforts are crucial for their survival.  It's also worth noting that some species might be locally common in specific areas, even if they are generally rare across the country.\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph"
      ],
      "metadata": {
        "id": "DxoSX2WNgH0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "223c518d-cf58-453d-f190-7e5eb0dc3d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.70-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.33)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.12-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.51-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.3.5)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.10.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.12.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.2.70-py3-none-any.whl (149 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.12-py3-none-any.whl (38 kB)\n",
            "Downloading langgraph_sdk-0.1.51-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langgraph-sdk, langgraph-checkpoint, langgraph\n",
            "Successfully installed langgraph-0.2.70 langgraph-checkpoint-2.0.12 langgraph-sdk-0.1.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from google.colab import userdata\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "query = \"Hi! I'm Bob.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpc-FZkMgdnd",
        "outputId": "8213f524-564d-431d-e79a-7aee60530bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Bob! It's nice to meet you. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_ChzdzSkEUP",
        "outputId": "df26007f-0dce-4b94-aa21-7f294c61e7a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You told me your name is Bob.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from google.colab import userdata\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    input_messages = [HumanMessage(query)]\n",
        "    output = app.invoke({\"messages\": input_messages}, config)\n",
        "    output[\"messages\"][-1].pretty_print()  # output contains all messages in state\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J022__l1yw9C",
        "outputId": "06036cce-c3d2-4f08-bd74-45aa94190027"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: How many fish species are there in Baltic Sea?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "There are roughly **170** fish species found in the Baltic Sea, although the exact number can fluctuate slightly depending on the criteria used (e.g., whether to include occasional visitors).  It's important to note that the Baltic is a brackish environment, meaning fewer species tolerate its lower salinity compared to the fully saline North Sea.\n",
            "You: Which are the most commoon?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The most common fish species in the Baltic Sea include:\n",
            "\n",
            "* **Herring:**  Arguably the most abundant fish, crucial both ecologically and commercially.  Several different herring stocks exist within the Baltic.\n",
            "* **Sprat:** Another small, oily fish, similar to herring and also commercially important.\n",
            "* **Cod:**  A key predator and commercially valuable, though Baltic cod populations have faced challenges in recent years.\n",
            "* **Flounder:**  Several flounder species inhabit the Baltic, including European flounder and Baltic flounder.  They are bottom-dwellers.\n",
            "* **Perch:**  A common freshwater fish that also tolerates the brackish conditions of the Baltic, especially in coastal areas and estuaries.\n",
            "* **Pike:** Another predatory freshwater fish that can be found in less saline areas of the Baltic.\n",
            "* **Salmon:**  While not as abundant as herring or sprat, salmon are an important part of the Baltic ecosystem and are both wild-caught and farmed.\n",
            "* **Whitefish:**  Several whitefish species, related to salmon, are found in the Baltic.\n",
            "\n",
            "It's important to note that the distribution and abundance of these species can vary considerably throughout the Baltic Sea, depending on salinity, depth, and other environmental factors.\n",
            "You: Which are the most rare?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Pinpointing the *absolute* rarest fish in the Baltic Sea is difficult because population data is constantly changing and some species are naturally elusive.  However, several species are considered rare or threatened within the Baltic:\n",
            "\n",
            "* **European Eel:**  Critically endangered, their numbers have plummeted due to various factors including overfishing, habitat loss, and migration barriers.\n",
            "* **Atlantic Sturgeon:**  Historically present, now functionally extinct in the Baltic.  Reintroduction efforts are underway.\n",
            "* **Salmon (certain populations):**  While salmon are present, some specific *river stocks* of salmon are very threatened or endangered.  These are genetically distinct groups that spawn in particular rivers.\n",
            "* **Sea Lamprey:**  While not as commercially valuable as some other species, the sea lamprey faces challenges from habitat degradation and is considered near threatened in some parts of its range, including the Baltic.\n",
            "* **Fourhorn Sculpin:**  This small fish has very specific habitat requirements and is vulnerable to changes in water quality and bottom conditions.  It's not necessarily rare throughout its entire range but can be locally scarce in the Baltic.\n",
            "\n",
            "It's important to remember that \"rare\" can refer to low overall numbers, a restricted distribution within the Baltic, or a combination of both. The status of these species is also subject to change based on ongoing research and conservation efforts.  Consulting up-to-date assessments from organizations like HELCOM (Baltic Marine Environment Protection Commission) or the IUCN Red List provides the most current information.\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates (Veidnes Uzvednēm)"
      ],
      "metadata": {
        "id": "5VxAv0JigYas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4-8ZjukVdHV",
        "outputId": "72e6f183-4428-4962-f918-3ce4b66da200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='Tell me a joke about cats')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"user\", \"Tell me a joke about {topic}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"})\n",
        "# prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "# result = model.invoke(prompt)\n",
        "# print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvQ-HPWlXjrf",
        "outputId": "addebda8-f328-4bb5-e8f4-54fba94fe532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    MessagesPlaceholder(\"msgs\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\"), HumanMessage(content=\"How are you?\")]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvdq1nwUbzu1",
        "outputId": "8ab82149-0753-4483-e9ba-f4f99882f49b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}), HumanMessage(content='How are you?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "template = \"Tell me a joke about {topic}\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"------Prompt from template------\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "print(\"\\n------Result-------\")\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JnEN73kgSPd",
        "outputId": "8853ace1-07f3-46af-cf4a-5cd01726cc0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------Prompt from template------\n",
            "messages=[HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})]\n",
            "\n",
            "------Result-------\n",
            "Why was the cat sitting on the computer? \n",
            "\n",
            "To keep an eye on the mouse!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ChatPromptTemplate using a template string\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"-----Prompt from Template-----\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1g2XbEHg-vN",
        "outputId": "48d1b4db-f513-4dc5-ec30-f2c29e4d519c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt with Multiple Placeholders\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n02VDO8vhSrs",
        "outputId": "08446c69-d7ab-4b8f-a9ce-8e683452024f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny story about a panda.\\nAssistant:', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt with System and Human Messages (Using Tuples)\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
        "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtlIBKRbhnxN",
        "outputId": "25c2b707-9ace-4822-eb23-13884cfad627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with System and Human Messages (Tuple) -----\n",
            "\n",
            "messages=[SystemMessage(content='You are a comedian who tells jokes about lawyers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains (Ķēdes)"
      ],
      "metadata": {
        "id": "rcZW6V34iSTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"user\", \"Tell me a {adjective} joke\")],\n",
        ")\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke({\"adjective\": \"funny\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y1iSX6unU0an",
        "outputId": "0a2064b2-4d21-45e5-f84c-de5fbbf0b7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't scientists trust atoms? \\n\\nBecause they make up everything!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Chain\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "# StrOutputParser() ~ .content()\n",
        "# LangChain expression language\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "result = chain.invoke({\"topic\": \"fish\", \"joke_count\": 3})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv4Uq-suiVBN",
        "outputId": "8b66d935-7442-4abb-b419-58992a898759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Why did the fish blush? Because it saw the ocean floor!  (Pause for groans, then add)  It was a coral-reef-ly embarrassing moment.\n",
            "\n",
            "\n",
            "2. What do you call a fish with no eyes?  Fsh! (Say it really fast).\n",
            "\n",
            "\n",
            "3.  I went to a seafood disco last week...  It was legen-dairy!  (Pause) ...and there was a killer whale playing the decks.  He was *really* good at spinning the tunes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extended Chain\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
        "count_words = RunnableLambda(lambda x: f\"Word count {len(x.split())} \\n {x}\")\n",
        "\n",
        "chain = prompt_template | model | StrOutputParser() | uppercase_output | count_words\n",
        "\n",
        "result = chain.invoke({\"topic\": \"fish\", \"joke_count\": 3})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBkBF9oPjaVA",
        "outputId": "9c41925d-3614-423f-85ce-9f27e59ca3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count 58 \n",
            " 1. WHY DID THE FISH BLUSH?  BECAUSE IT SAW THE CLAM!  (PAUSE FOR GROANS, THEN ADD)  ...AND IT WAS SHELLFISH ABOUT IT.\n",
            "\n",
            "\n",
            "2. WHAT DO YOU CALL A FISH WITH NO EYES?  FSH!\n",
            "\n",
            "\n",
            "3.  I WENT TO A SEAFOOD DISCO LAST WEEK...  IT WAS ALRIGHT, BUT THERE WASN'T MUCH COD DANCING.  AND THE BASS WAS A BIT REPETITIVE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parallel\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Define prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert product reviewer.\"),\n",
        "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Define pros analysis step\n",
        "def analyze_pros(features):\n",
        "    pros_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the pros of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return pros_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Define cons analysis step\n",
        "def analyze_cons(features):\n",
        "    cons_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the cons of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return cons_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Combine pros and cons into a final review\n",
        "def combine_pros_cons(pros, cons):\n",
        "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
        "\n",
        "\n",
        "# Simplify branches with LCEL\n",
        "pros_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "cons_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = (\n",
        "    prompt_template\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
        "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
        ")\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
        "\n",
        "# Output\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "o1YDVByWj0vH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bea578f-5855-40fa-a8ee-ba9b32dea8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pros:\n",
            "You're right, providing a comprehensive review of a MacBook Pro without specifying the model is like reviewing \"a car\" – too broad to be useful.  The variations between models are substantial, impacting performance and value significantly.\n",
            "\n",
            "However, based on your provided core features, let's outline the *general* pros of those features, acknowledging that the degree to which these pros manifest will vary considerably depending on the specific model:\n",
            "\n",
            "\n",
            "**Pros of Core MacBook Pro Features:**\n",
            "\n",
            "* **High-Performance Processors (M-series or Intel):**\n",
            "    * **Pro:**  Seamless multitasking, rapid application launch times, effortless handling of demanding applications (video editing, 3D rendering, etc.).  This translates to increased productivity and a more fluid user experience.  The M-series chips offer particularly impressive performance and energy efficiency.\n",
            "\n",
            "* **Powerful Graphics (Integrated or Dedicated):**\n",
            "    * **Pro:**  Smooth video playback, excellent performance in graphically intensive applications, and the ability to play many games (though dedicated GPUs are needed for high-end gaming).  Professional content creators benefit immensely from dedicated GPUs.\n",
            "\n",
            "* **Robust Build Quality (Aluminum Unibody):**\n",
            "    * **Pro:**  Durability, resistance to scratches and dents, premium feel, and a sleek, professional aesthetic.  It's built to last and withstand the rigors of daily use.\n",
            "\n",
            "* **High-Resolution Display (Retina or mini-LED XDR):**\n",
            "    * **Pro:**  Crisp, vibrant images and text, making everything from web browsing to photo editing a pleasure.  The mini-LED XDR displays offer exceptional brightness, contrast, and color accuracy, ideal for professional work.\n",
            "\n",
            "* **Extensive Connectivity (Thunderbolt/USB-C and others):**\n",
            "    * **Pro:**  Flexibility in connecting peripherals, fast data transfer speeds, and the ability to charge devices quickly.  While fewer ports might seem limiting, the versatility of Thunderbolt/USB-C often compensates.\n",
            "\n",
            "* **Long Battery Life:**\n",
            "    * **Pro:**  All-day productivity without needing to constantly search for a power outlet.  This is a significant advantage for mobile users and those working on the go.\n",
            "\n",
            "* **macOS Operating System:**\n",
            "    * **Pro:**  User-friendly interface, intuitive design, strong security features, a vast library of apps, and seamless integration within the Apple ecosystem.  macOS is known for its stability and reliability.\n",
            "\n",
            "* **Touch Bar (Older Models):**  (While discontinued)\n",
            "    * **Pro (previously):** Context-sensitive controls could improve workflow in certain applications, though its usefulness was debated.\n",
            "\n",
            "\n",
            "To reiterate:  These are *general* pros. The actual experience will significantly depend on the specific model's processor, RAM, storage, display technology, and other specifications.  Please provide the exact model details for a truly expert and detailed review.\n",
            "\n",
            "Cons:\n",
            "You're right, providing a comprehensive review of a MacBook Pro without specifying the model is impossible.  The variations between models are substantial and impact the overall user experience significantly.  However, I can offer a generalized critique of the *core features* listed, highlighting potential downsides or drawbacks that apply across various models, even if the severity varies:\n",
            "\n",
            "**Core Features and Potential Downsides:**\n",
            "\n",
            "* **High-Performance Processors:** While powerful, the performance advantage comes at a cost – higher price and potentially higher power consumption (affecting battery life under heavy load).  The heat generated by these powerful processors can also be noticeable, sometimes leading to fan noise.  Furthermore, the reliance on Apple silicon creates ecosystem lock-in.\n",
            "\n",
            "* **Powerful Graphics:**  Dedicated GPUs in higher-end models are a boon for professionals, but they also contribute to increased cost, heat generation, and potentially higher power consumption.  For users who don't need this level of graphical power, it's an unnecessary expense.\n",
            "\n",
            "* **Robust Build Quality:** While generally durable, the unibody aluminum design can be prone to bending under significant pressure, and repairs can be expensive.  The lack of user-serviceable components also limits repair options and extends downtime.\n",
            "\n",
            "* **High-Resolution Display:**  Retina and mini-LED displays are excellent, but they contribute significantly to the laptop's cost.  The high brightness of mini-LED can sometimes cause discomfort for prolonged use in low-light environments.  Screen burn-in, while less common on modern displays, remains a potential concern.\n",
            "\n",
            "* **Extensive Connectivity:** The reduction in ports (especially on newer models) can be inconvenient, requiring dongles and adapters for connecting older peripherals.  This adds extra cost and can be cumbersome. While Thunderbolt/USB-C is fast, it's not universally compatible.\n",
            "\n",
            "* **Long Battery Life:** While generally good, battery life is highly dependent on usage.  Demanding tasks (video editing, gaming) will significantly reduce battery life, and battery degradation over time is inevitable.  Battery replacement can also be expensive.\n",
            "\n",
            "* **macOS Operating System:**  macOS is generally praised for its user-friendliness and stability, but it's exclusive to Apple hardware, limiting flexibility and potentially creating compatibility issues with certain software or peripherals.  Some users may find the ecosystem restrictive compared to Windows' open nature.\n",
            "\n",
            "* **Touch Bar (older models):** The Touch Bar, while innovative, was ultimately criticized for its limited functionality and inconsistent implementation.  Its removal in newer models suggests Apple acknowledged its shortcomings.\n",
            "\n",
            "\n",
            "In summary, while the MacBook Pro offers a compelling combination of power, portability, and build quality, it's not without its drawbacks.  The high price point, limited repairability, ecosystem lock-in, and potential inconveniences related to connectivity and the trade-offs between performance and battery life are crucial factors to consider before purchasing.  Providing a truly expert review requires specifying the exact model to address the nuances and unique strengths and weaknesses of that specific configuration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Branching\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableBranch\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define prompt templates for different feedback types\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the feedback classification template\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the runnable branches for handling feedback\n",
        "branches = RunnableBranch(\n",
        "    (\n",
        "        lambda x: \"positive\" in x,\n",
        "        positive_feedback_template | model | StrOutputParser()  # Positive feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"negative\" in x,\n",
        "        negative_feedback_template | model | StrOutputParser()  # Negative feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"neutral\" in x,\n",
        "        neutral_feedback_template | model | StrOutputParser()  # Neutral feedback chain\n",
        "    ),\n",
        "    escalate_feedback_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the classification chain\n",
        "classification_chain = classification_template | model | StrOutputParser()\n",
        "\n",
        "# Combine classification and response generation into one chain\n",
        "chain = classification_chain | branches\n",
        "\n",
        "# Run the chain with an example review\n",
        "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
        "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
        "\n",
        "review = \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "result = chain.invoke({\"feedback\": review})\n",
        "\n",
        "# Output the result\n",
        "print(result)"
      ],
      "metadata": {
        "id": "tQ2sjFYgkX--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce90c031-7005-4941-caeb-f9eafd465140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Several options, depending on your desired tone and relationship with the feedback provider:\n",
            "\n",
            "\n",
            "**Option 1 (Formal & Brief):**\n",
            "\n",
            "> Thank you for your feedback. We appreciate you taking the time to share your thoughts on [project/product/service]. We will take your comments into consideration as we move forward.\n",
            "\n",
            "\n",
            "**Option 2 (Slightly More Engaging):**\n",
            "\n",
            "> Thank you for your feedback on [project/product/service]. We value your input and appreciate you letting us know your experience.  We're always striving to improve, and your comments will help us in that process.\n",
            "\n",
            "\n",
            "**Option 3 (If you want to gently probe for more detail, suitable only if you have a good relationship):**\n",
            "\n",
            "> Thank you for your feedback on [project/product/service].  We appreciate you taking the time to share your thoughts.  While we're pleased to hear it met your expectations, we'd be grateful if you could elaborate on any areas where we could potentially improve or exceed expectations in the future.\n",
            "\n",
            "\n",
            "**Option 4 (If you suspect there's underlying dissatisfaction you want to address):**\n",
            "\n",
            "> Thank you for your feedback on [project/product/service]. We appreciate your honesty.  While your comments indicate a neutral experience, we're keen to understand if there's anything we could have done differently to make your experience more positive.  Could you perhaps share any specific details?\n",
            "\n",
            "\n",
            "Choose the option that best fits your context and relationship with the person who provided the feedback.  Avoid overly apologetic language unless there's a clear reason to apologize.  The goal is to acknowledge the feedback and show that you value their input, even if it's not overwhelmingly positive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Loaders (Dokumentu Ielādētāji)"
      ],
      "metadata": {
        "id": "IHVJMxYD54OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjtFqa7K6HkJ",
        "outputId": "d5fefa23-def7-4cb6-e910-037bf0f6cbbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.34 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.34-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.18 (from langchain-community)\n",
            "  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.6)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain<1.0.0,>=0.3.18->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.18-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.34-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.33\n",
            "    Uninstalling langchain-core-0.3.33:\n",
            "      Successfully uninstalled langchain-core-0.3.33\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.5\n",
            "    Uninstalling langchain-text-splitters-0.3.5:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.5\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.17\n",
            "    Uninstalling langchain-0.3.17:\n",
            "      Successfully uninstalled langchain-0.3.17\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.18 langchain-community-0.3.17 langchain-core-0.3.34 langchain-text-splitters-0.3.6 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "file_path = \"/content/diabetes.csv\"\n",
        "\n",
        "loader = CSVLoader(file_path=file_path)\n",
        "data = loader.load()\n",
        "\n",
        "for record in data[:2]:\n",
        "    #print(type(record))\n",
        "    print(record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toKXmvoW52Tb",
        "outputId": "d62ef9d1-39e6-4be0-cf77-2ce01b3b0d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Pregnancies: 6\n",
            "Glucose: 148\n",
            "BloodPressure: 72\n",
            "SkinThickness: 35\n",
            "Insulin: 0\n",
            "BMI: 33.6\n",
            "DiabetesPedigreeFunction: 0.627\n",
            "Age: 50\n",
            "Outcome: 1' metadata={'source': '/content/diabetes.csv', 'row': 0}\n",
            "page_content='Pregnancies: 1\n",
            "Glucose: 85\n",
            "BloodPressure: 66\n",
            "SkinThickness: 29\n",
            "Insulin: 0\n",
            "BMI: 26.6\n",
            "DiabetesPedigreeFunction: 0.351\n",
            "Age: 31\n",
            "Outcome: 0' metadata={'source': '/content/diabetes.csv', 'row': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2PH9ArP81cE",
        "outputId": "53961e93-53b5-4cfa-8d56-62beebf86945"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/300.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m297.0/300.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/invoice_1001329.pdf\")\n",
        "pages = []\n",
        "\n",
        "async for page in loader.alazy_load():\n",
        "    pages.append(page)\n",
        "print(f\"{pages[0].metadata}\\n\")\n",
        "print(pages[0].page_content)"
      ],
      "metadata": {
        "id": "AxwhF9aR88xT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260435f6-b331-4de2-f0fc-7d225285fdf8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'producer': 'Qt 4.8.6', 'creator': 'wkhtmltopdf 0.12.2', 'creationdate': '2023-05-05T01:13:00+00:00', 'title': 'Invoice Preview', 'source': '/content/invoice_1001329.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n",
            "\n",
            "ExcelCult\n",
            "India\n",
            "Bill To\n",
            "Ms.Santoshi\n",
            "Mumbai, India\n",
            "Santoshvarma0988@gmail.com\n",
            "9999999999\n",
            " \n",
            "  \n",
            "Invoice no.\n",
            "1001329\n",
            "Date\n",
            "5/4/2023\n",
            "Description\n",
            "Quantity\n",
            "Unit price\n",
            "Amount\n",
            "Total\n",
            "$2,200.00\n",
            "Office Chair\n",
            "2\n",
            "$1,100.00\n",
            "$2,200.00\n",
            "System Generated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents (Aģenti)"
      ],
      "metadata": {
        "id": "XomU3RqvlaS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU duckduckgo-search langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox6JxQStvW8j",
        "outputId": "2d091d32-252d-4b01-faf4-1e6342f55d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/3.3 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define the tool: DuckDuckGo search results\n",
        "ddg_search = DuckDuckGoSearchRun()\n",
        "\n",
        "# Set up the agent with the search tool\n",
        "tools = [Tool(\n",
        "    name=\"DuckDuckGoSearchRun\",\n",
        "    func=ddg_search.run,\n",
        "    description=\"Searches the web using DuckDuckGo.\"\n",
        ")]\n",
        "\n",
        "# Define a simple prompt template for asking a question\n",
        "prompt_template = \"Search the web for information about {query}\"\n",
        "\n",
        "# Initialize the agent with a basic agent type\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Use the agent to interact and fetch search results\n",
        "def get_search_results_using_agent(query: str):\n",
        "    query_formatted = prompt_template.format(query=query)\n",
        "    response = agent.run(query_formatted)\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "query = \"latest news on AI advancements\"\n",
        "search_results = get_search_results_using_agent(query)\n",
        "print(search_results)\n"
      ],
      "metadata": {
        "id": "_jf9DC-du94c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da4db1f-af74-4b49-ec05-ffe3774b5fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-f7b5cff39ffa>:32: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n",
            "<ipython-input-35-f7b5cff39ffa>:42: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = agent.run(query_formatted)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to use DuckDuckGoSearchRun to find the latest news on AI advancements.  I'll use a broad search term to get a range of results.\n",
            "\n",
            "Action: DuckDuckGoSearchRun\n",
            "Action Input: \"latest AI advancements news\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mGoogle shared its latest AI news from January 2025, highlighting advancements in products, research, and more. Gemini 2.0 Flash, a performance upgrade to the Gemini app, delivers faster responses and more capable help. Jan. 2, 2025 — Artificial intelligence has the potential to improve the analysis of medical image data. For example, algorithms based on deep learning can determine the location and size of tumors. Discover the latest AI advancements. From IBM's and Meta's new tools to groundbreaking research and new AI laws, our recent AI news keeps you informed on all fronts. Significant AI developments in August 2024 include innovations in artificial intelligence, regulatory discussions about AI, and notable releases. Key Takeaways Record generative AI funding: Investments in generative AI, which encompasses a range of AI-powered apps, tools, and services to generate text, images, videos, speech, music, and more, reached new ... EU flag, stock graph and \"AI ARTIFICIAL INTELLIGENCE\" words are seen in this illustration taken, May 21, 2024. REUTERS/Dado Ruvic/Illustration/File Photo February 4, 2025\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mQuestion: Search the web for information about latest news on AI advancements\n",
            "Thought: I need to use DuckDuckGoSearchRun to find the latest news on AI advancements.  I'll use a broad search term to get a range of results.\n",
            "\n",
            "Action: DuckDuckGoSearchRun\n",
            "Action Input: \"latest AI advancements news\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mGoogle shared its latest AI news from January 2025, highlighting advancements in products, research, and more. Gemini 2.0 Flash, a performance upgrade to the Gemini app, delivers faster responses and more capable help. Get the latest updates on artificial intelligence, machine learning, and emerging technologies from trusted sources. Daily updated artificial intelligence news and insights. Stay ahead with our curated AI news coverage. ... Latest AI News: February 8, 2025. Stay informed with our daily curated artificial intelligence news and insights. We bring ... 2025 is off to a racing start. From announcing strides in the new Gemini 2.0 model family to retailers accelerating with Cloud AI, we spent January investing in our partner ecosystem, open-source, and ways to make AI more useful.We've heard from people everywhere, from developers to CMOs, about the pressure to adapt the latest in AI with efficiency and speed - and the delicate balance of ... Jan. 2, 2025 — Artificial intelligence has the potential to improve the analysis of medical image data. For example, algorithms based on deep learning can determine the location and size of tumors. Find in-depth AI news from Engadget's experienced editorial team. We cover all the latest artificial intelligence advancements including AI chatbots, LLMs, AI image generation, AI voice generation ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought:The observations from the DuckDuckGo searches provide a good overview of recent AI advancements.  I can summarize the key areas of progress.\n",
            "\n",
            "Thought: I now know the final answer.\n",
            "\n",
            "Final Answer: Recent AI advancements highlighted in the news include significant progress in large language models (LLMs), such as Google's Gemini 2.0 Flash offering faster responses and improved capabilities.  There are also advancements in AI's application to medical image analysis, enabling more precise detection of tumors.  Furthermore, substantial investment continues in generative AI, encompassing tools and services that create text, images, videos, and more.  Finally,  regulatory discussions and new AI laws are emerging alongside these technological breakthroughs.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Recent AI advancements highlighted in the news include significant progress in large language models (LLMs), such as Google's Gemini 2.0 Flash offering faster responses and improved capabilities.  There are also advancements in AI's application to medical image analysis, enabling more precise detection of tumors.  Furthermore, substantial investment continues in generative AI, encompassing tools and services that create text, images, videos, and more.  Finally,  regulatory discussions and new AI laws are emerging alongside these technological breakthroughs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def load_data(query=None):  # Adding query parameter to make it compatible\n",
        "    # Load Iris dataset from sklearn\n",
        "    iris = load_iris()\n",
        "    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "    df['species'] = iris.target\n",
        "    return df\n",
        "\n",
        "# Function to clean data (simple imputation for missing values)\n",
        "def clean_data(df: pd.DataFrame, query=None):\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    df_clean = df.copy()\n",
        "    df_clean[:] = imputer.fit_transform(df)\n",
        "    return df_clean\n",
        "\n",
        "# Function to train a model (Random Forest)\n",
        "def train_model(df: pd.DataFrame, target_column: str, query=None):\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Random Forest model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Set up tools for the agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Load Data\",\n",
        "        func=load_data,\n",
        "        description=\"Loads the Iris dataset.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Clean Data\",\n",
        "        func=clean_data,\n",
        "        description=\"Cleans the dataset by handling missing values.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Train Model\",\n",
        "        func=train_model,\n",
        "        description=\"Trains a Random Forest model and returns the accuracy.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Initialize the agent with tools and LLM\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Automating a data science task\n",
        "def automate_data_science_task():\n",
        "    # Step 1: Load data\n",
        "    print(\"Loading data...\")\n",
        "    df = load_data()\n",
        "\n",
        "    # Step 2: Clean data\n",
        "    print(\"Cleaning data...\")\n",
        "    df_clean = clean_data(df)\n",
        "\n",
        "    # Step 3: Train model\n",
        "    print(\"Training model...\")\n",
        "    accuracy = train_model(df_clean, target_column=\"species\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Run the agent\n",
        "accuracy = automate_data_science_task()\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "X207mzlgyxUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02eedf1-04a7-4ee0-ecd1-5204ef73278c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Cleaning data...\n",
            "Training model...\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Create a tool\n",
        "@tool\n",
        "def super_function(input: int) -> int:\n",
        "    \"\"\"Applies a magic function to an input.\"\"\"\n",
        "    return input + 2\n",
        "\n",
        "\n",
        "tools = [super_function]\n",
        "\n",
        "\n",
        "query = \"what is the value of super_function(3)?\"\n",
        "\n",
        "langgraph_agent_executor = create_react_agent(model, tools)\n",
        "\n",
        "\n",
        "messages = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})\n",
        "{\n",
        "    \"input\": query,\n",
        "    \"output\": messages[\"messages\"][-1].content,\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkPUmwHfI6X8",
        "outputId": "670e629a-a59a-4fbb-9c9b-efe76cde632f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is the value of super_function(3)?',\n",
              " 'output': \"The value of `super_function(3)` is `{'output': 5}`.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools (Rīki)"
      ],
      "metadata": {
        "id": "78jpzxVBEnNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "# Let's inspect some of the attributes associated with the tool.\n",
        "print(multiply.name)\n",
        "print(multiply.description)\n",
        "print(multiply.args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1qMwQwuPoJ_",
        "outputId": "3d02f796-1245-4658-87e0-83b9d79a425f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multiply\n",
            "Multiply two numbers.\n",
            "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-experimental"
      ],
      "metadata": {
        "id": "_tKjxRFgEXHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "python_repl = PythonREPL()\n",
        "python_repl.run(\"print(1+1)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "sxMXY1ozDLfg",
        "outputId": "edd4f33c-cf7d-4b9b-ffa9-c27a5da6ef6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoice extractor (Rēķinu izvilkšanas lietotne)"
      ],
      "metadata": {
        "id": "4IAJVycUd1wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "ccBVPQO8QooB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "pdf_doc = \"/content/invoice_1001329.pdf\"\n",
        "pdf_reader = PdfReader(pdf_doc)\n",
        "text = \"\"\n",
        "for page in pdf_reader.pages:\n",
        "    text += page.extract_text()\n",
        "print(text)"
      ],
      "metadata": {
        "id": "ZzN1tiXfR7IA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d280ef3f-3e61-473a-b69a-7a36c3c537cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExcelCult\n",
            "India\n",
            "Bill To\n",
            "Ms.Santoshi\n",
            "Mumbai, India\n",
            "Santoshvarma0988@gmail.com\n",
            "9999999999\n",
            " \n",
            "  \n",
            "Invoice no.\n",
            "1001329\n",
            "Date\n",
            "5/4/2023\n",
            "Description\n",
            "Quantity\n",
            "Unit price\n",
            "Amount\n",
            "Total\n",
            "$2,200.00\n",
            "Office Chair\n",
            "2\n",
            "$1,100.00\n",
            "$2,200.00\n",
            "System Generated\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from pydantic import BaseModel, ValidationError\n",
        "from pypdf import PdfReader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# Set up your Google API key\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"Google API key not found in userdata.\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Define a Pydantic model for invoice data validation\n",
        "class InvoiceData(BaseModel):\n",
        "    Invoice_no: str\n",
        "    Description: str\n",
        "    Quantity: str\n",
        "    Date: str\n",
        "    Unit_price: str\n",
        "    Amount: float\n",
        "    Total: float\n",
        "    Email: str\n",
        "    Phone_number: str\n",
        "    Address: str\n",
        "\n",
        "# PDF file path\n",
        "pdf_path = \"/content/invoice_1001329.pdf\"\n",
        "\n",
        "# Extract text from PDF\n",
        "print(f\"Processing {pdf_path}...\")\n",
        "text = \"\"\n",
        "pdf_reader = PdfReader(pdf_path)\n",
        "for page in pdf_reader.pages:\n",
        "    text += page.extract_text()\n",
        "\n",
        "# Clean and prepare the text\n",
        "text = text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "# Construct the prompt\n",
        "prompt = f\"\"\"Extract all the following values: invoice no., Description, Quantity, date,\n",
        "Unit price, Amount, Total, email, phone number, and address from this data:\n",
        "\n",
        "{text}\n",
        "\n",
        "Expected output (JSON format):\n",
        "{{\n",
        "    \"Invoice_no\": \"1001329\",\n",
        "    \"Description\": \"Office Chair\",\n",
        "    \"Quantity\": \"2\",\n",
        "    \"Date\": \"5/4/2023\",\n",
        "    \"Unit_price\": \"1100.00\",\n",
        "    \"Amount\": 2200.00,\n",
        "    \"Total\": 2200.00,\n",
        "    \"Email\": \"Santoshvarma0988@gmail.com\",\n",
        "    \"Phone_number\": \"9999999999\",\n",
        "    \"Address\": \"Mumbai, India\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Debug: Print prompt to verify it\n",
        "print(\"Prompt sent to LLM:\\n\", prompt)\n",
        "\n",
        "# Initialize a DataFrame\n",
        "df = pd.DataFrame(columns=[\n",
        "    'Invoice_no', 'Description', 'Quantity', 'Date', 'Unit_price',\n",
        "    'Amount', 'Total', 'Email', 'Phone_number', 'Address'\n",
        "])\n",
        "\n",
        "# Generate response from LLM\n",
        "try:\n",
        "    llm_response = llm.invoke([HumanMessage(content=prompt)])  # Use `invoke()` instead of calling the model directly\n",
        "    llm_content = llm_response.content  # Access response content\n",
        "\n",
        "    # Clean response: Remove triple backticks and JSON keyword\n",
        "    cleaned_text = re.sub(r\"```json\\s*|\\s*```\", \"\", llm_content).strip()\n",
        "\n",
        "    # Validate and parse JSON response using Pydantic\n",
        "    invoice_data = InvoiceData.model_validate_json(cleaned_text)\n",
        "    print(\"Validated Data:\\n\", invoice_data)\n",
        "\n",
        "    # Convert Pydantic model to dict and append to DataFrame\n",
        "    new_data = pd.DataFrame([invoice_data.model_dump()])\n",
        "\n",
        "    # Append new data safely\n",
        "    if df.empty:\n",
        "        df = new_data\n",
        "    else:\n",
        "        df = pd.concat([df, new_data], ignore_index=True)\n",
        "\n",
        "except ValidationError as e:\n",
        "    print(\"Validation Error:\\n\", e)\n",
        "except Exception as e:\n",
        "    print(f\"Error generating LLM response: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = 'invoices_summary.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"CSV file created as '{csv_filename}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umD5hvBrLKhA",
        "outputId": "7f7c7dc4-24ee-4b68-ae5a-ee1ee7e57ba5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/invoice_1001329.pdf...\n",
            "Prompt sent to LLM:\n",
            " Extract all the following values: invoice no., Description, Quantity, date,\n",
            "Unit price, Amount, Total, email, phone number, and address from this data:\n",
            "\n",
            "ExcelCult India Bill To Ms.Santoshi Mumbai, India Santoshvarma0988@gmail.com 9999999999      Invoice no. 1001329 Date 5/4/2023 Description Quantity Unit price Amount Total $2,200.00 Office Chair 2 $1,100.00 $2,200.00 System Generated\n",
            "\n",
            "Expected output (JSON format):\n",
            "{\n",
            "    \"Invoice_no\": \"1001329\",\n",
            "    \"Description\": \"Office Chair\",\n",
            "    \"Quantity\": \"2\",\n",
            "    \"Date\": \"5/4/2023\",\n",
            "    \"Unit_price\": \"1100.00\",\n",
            "    \"Amount\": 2200.00,\n",
            "    \"Total\": 2200.00,\n",
            "    \"Email\": \"Santoshvarma0988@gmail.com\",\n",
            "    \"Phone_number\": \"9999999999\",\n",
            "    \"Address\": \"Mumbai, India\"\n",
            "}\n",
            "\n",
            "Validated Data:\n",
            " Invoice_no='1001329' Description='Office Chair' Quantity='2' Date='5/4/2023' Unit_price='1100.00' Amount=2200.0 Total=2200.0 Email='Santoshvarma0988@gmail.com' Phone_number='9999999999' Address='Mumbai, India'\n",
            "CSV file created as 'invoices_summary.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from pydantic import BaseModel, field_validator\n",
        "from typing import Optional\n",
        "from pypdf import PdfReader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# Set up Google API key\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"Google API key not found in userdata.\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Define a Pydantic model with relaxed validation\n",
        "class InvoiceData(BaseModel):\n",
        "    Invoice_no: str\n",
        "    Description: str\n",
        "    Quantity: str\n",
        "    Date: str\n",
        "    Unit_price: str\n",
        "    Amount: Optional[float] = None\n",
        "    Total: Optional[float] = None\n",
        "    Email: Optional[str] = None\n",
        "    Phone_number: Optional[str] = None\n",
        "    Address: Optional[str] = None\n",
        "\n",
        "    # Convert currency values to float\n",
        "    @field_validator(\"Amount\", \"Total\", mode=\"before\")\n",
        "    @classmethod\n",
        "    def parse_currency(cls, value):\n",
        "        if isinstance(value, str):\n",
        "            value = re.sub(r\"[^\\d.]\", \"\", value)  # Remove non-numeric characters\n",
        "        return float(value) if value else None\n",
        "\n",
        "# List of PDF file paths\n",
        "pdf_files = [\n",
        "    \"/content/invoice_1001329.pdf\",\n",
        "    \"/content/invoice_2001321.pdf\",\n",
        "    \"/content/invoice_3452334.pdf\",\n",
        "]\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "df = pd.DataFrame(columns=[\n",
        "    \"Invoice_no\", \"Description\", \"Quantity\", \"Date\", \"Unit_price\",\n",
        "    \"Amount\", \"Total\", \"Email\", \"Phone_number\", \"Address\"\n",
        "])\n",
        "\n",
        "# Loop through PDF files\n",
        "for pdf_path in pdf_files:\n",
        "    try:\n",
        "        print(f\"Processing {pdf_path}...\")\n",
        "\n",
        "        # Extract text from PDF\n",
        "        text = \"\"\n",
        "        pdf_reader = PdfReader(pdf_path)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() or \"\"  # Ensure it doesn't break if text extraction fails\n",
        "\n",
        "        text = text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "        # Construct the prompt\n",
        "        prompt = f\"\"\"Extract all the following values: invoice no., Description, Quantity, Date,\n",
        "        Unit price, Amount, Total, Email, Phone number, and Address from this data:\n",
        "\n",
        "        {text}\n",
        "\n",
        "        Expected output (JSON format):\n",
        "        {{\n",
        "            \"Invoice_no\": \"1001329\",\n",
        "            \"Description\": \"Office Chair\",\n",
        "            \"Quantity\": \"2\",\n",
        "            \"Date\": \"5/4/2023\",\n",
        "            \"Unit_price\": \"1100.00\",\n",
        "            \"Amount\": 2200.00,\n",
        "            \"Total\": 2200.00,\n",
        "            \"Email\": \"Santoshvarma0988@gmail.com\",\n",
        "            \"Phone_number\": \"9999999999\",\n",
        "            \"Address\": \"Mumbai, India\"\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        # Send prompt to LLM\n",
        "        llm_response = llm.invoke([HumanMessage(content=prompt)])\n",
        "        llm_content = llm_response.content\n",
        "\n",
        "        # Extract JSON using regex\n",
        "        json_match = re.search(r\"\\{.*\\}\", llm_content, re.DOTALL)\n",
        "        if not json_match:\n",
        "            print(f\"Error: Could not extract JSON from response for {pdf_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        cleaned_text = json_match.group(0)\n",
        "\n",
        "        # Ensure valid JSON format\n",
        "        try:\n",
        "            invoice_json = json.loads(cleaned_text)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error: LLM response is not valid JSON for {pdf_path}: {cleaned_text}\")\n",
        "            continue  # Skip this file\n",
        "\n",
        "        # Validate and parse JSON response using Pydantic\n",
        "        try:\n",
        "            invoice_data = InvoiceData(**invoice_json)\n",
        "        except Exception as e:\n",
        "            print(f\"Validation Error in {pdf_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        print(\"Validated Data:\\n\", invoice_data)\n",
        "\n",
        "        # Convert Pydantic model to dict and append to DataFrame\n",
        "        new_data = pd.DataFrame([invoice_data.model_dump()])\n",
        "        df = pd.concat([df, new_data], ignore_index=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdf_path}: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = \"multi_invoice_summary.csv\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"CSV file created as '{csv_filename}'.\")\n",
        "print(f\"Total invoices processed: {len(df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPOjhEiYO3hi",
        "outputId": "759913c7-3b1f-4344-ea40-a05ab357fe66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/invoice_1001329.pdf...\n",
            "Validated Data:\n",
            " Invoice_no='1001329' Description='Office Chair' Quantity='2' Date='5/4/2023' Unit_price='1100.00' Amount=2200.0 Total=2200.0 Email='Santoshvarma0988@gmail.com' Phone_number='9999999999' Address='Mumbai, India'\n",
            "Processing /content/invoice_2001321.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-512997803a29>:121: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, new_data], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validated Data:\n",
            " Invoice_no='2001321' Description='HP Laptop' Quantity='1' Date='5/4/2023' Unit_price='$500.00' Amount=500.0 Total=500.0 Email='sharathkumarraju@proton.me' Phone_number='8888888888' Address='Hyderabad, India'\n",
            "Processing /content/invoice_3452334.pdf...\n",
            "Validated Data:\n",
            " Invoice_no='3452334' Description='Graphics Card' Quantity='1' Date='5/5/2023' Unit_price='50.00' Amount=50.0 Total=50.0 Email='N/A' Phone_number='N/A' Address='N/A'\n",
            "CSV file created as 'multi_invoice_summary.csv'.\n",
            "Total invoices processed: 3\n"
          ]
        }
      ]
    }
  ]
}