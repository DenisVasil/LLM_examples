{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgPcUPNM1T06bE+he5YlG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenisVasil/LLM_examples/blob/main/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Models (čata modeļi)"
      ],
      "metadata": {
        "id": "vBOJzQF4bGPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "wa0lp07tR4Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjzm9ulcRYUu",
        "outputId": "f76a2ebb-323d-4623-cb2b-eb49088bfdd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full result:\n",
            "content='81 divided by 9 is 9.\\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-3cf63909-e10d-4f64-a87d-a47c2238f74f-0' usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21, 'input_token_details': {'cache_read': 0}}\n",
            "Context:\n",
            "81 divided by 9 is 9.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "result = model.invoke(\"What is 81 divided by 9\")\n",
        "\n",
        "print(\"Full result:\")\n",
        "print(result)\n",
        "print(\"Context:\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Conversation\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problem\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\")\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "\n",
        "print(f\"Answer from AI: {result.content}\")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problem\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "    AIMessage(content=f\"{result.content}\"),\n",
        "    HumanMessage(content=\"Divide this result by 3\")\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "print(f\"New answer from AI: {result.content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EtoGySeR2l4",
        "outputId": "bf753e24-b8ea-443d-cc31-cb618c83585e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from AI: 81 divided by 9 is 9.\n",
            "\n",
            "New answer from AI: 9 divided by 3 is 3.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chat\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "system_message = SystemMessage(content=\"You are helpful AI assistant\")\n",
        "chat_history.append(system_message)\n",
        "\n",
        "# chat loop\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    chat_history.append(HumanMessage(content=query))\n",
        "\n",
        "    result = model.invoke(chat_history)\n",
        "    response = result.content\n",
        "    chat_history.append(AIMessage(content=response))\n",
        "    print(f\"AI response: {response}\")\n",
        "\n",
        "print(\"----- Message history ------\")\n",
        "print(chat_history)"
      ],
      "metadata": {
        "id": "tuJXOC96R27m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv1IsvPpfhs3",
        "outputId": "f2a68ccd-fd7f-414a-dbb2-08d77961cb34"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (1.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Chat History\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.memory.chat_memory import InMemoryChatMessageHistory\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "import duckdb\n",
        "import os\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Constants\n",
        "DB_PATH = \"chat_history.db\"\n",
        "TABLE_NAME = \"chat_history\"\n",
        "SESSION_ID = \"user_session_new\"  # This could be a username or unique ID\n",
        "\n",
        "# Initialize DuckDB\n",
        "con = duckdb.connect(DB_PATH)\n",
        "con.execute(f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
        "    session_id TEXT,\n",
        "    role TEXT,\n",
        "    content TEXT\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Helper Functions\n",
        "\n",
        "\n",
        "def save_message_to_db(session_id, role, content):\n",
        "    \"\"\"Save a single message to the DuckDB database.\"\"\"\n",
        "    con.execute(f\"\"\"\n",
        "    INSERT INTO {TABLE_NAME} (session_id, role, content)\n",
        "    VALUES (?, ?, ?)\n",
        "    \"\"\", [session_id, role, content])\n",
        "\n",
        "\n",
        "def load_messages_from_db(session_id):\n",
        "    \"\"\"Load chat history for a specific session from the DuckDB database.\"\"\"\n",
        "    result = con.execute(f\"\"\"\n",
        "    SELECT role, content FROM {TABLE_NAME} WHERE session_id = ? ORDER BY rowid\n",
        "    \"\"\", [session_id]).fetchall()\n",
        "    return [HumanMessage(content=row[1]) if row[0] == \"user\" else AIMessage(content=row[1]) for row in result]\n",
        "\n",
        "\n",
        "# Initialize Chat Message History\n",
        "print(\"Initializing DuckDB Chat Message History...\")\n",
        "messages = load_messages_from_db(SESSION_ID)\n",
        "chat_history = InMemoryChatMessageHistory(messages=messages)\n",
        "\n",
        "print(\"Chat History Initialized.\")\n",
        "print(\"Current Chat History:\", [msg.content for msg in chat_history.messages])\n",
        "\n",
        "\n",
        "print(\"Start chatting with the AI. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    human_input = input(\"User: \")\n",
        "    if human_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Save and process user message\n",
        "    chat_history.add_user_message(human_input)\n",
        "    save_message_to_db(SESSION_ID, \"user\", human_input)\n",
        "\n",
        "    # Generate AI response\n",
        "    ai_response = model.invoke(chat_history.messages)\n",
        "    chat_history.add_ai_message(ai_response.content)\n",
        "    save_message_to_db(SESSION_ID, \"ai\", ai_response.content)\n",
        "\n",
        "    print(f\"AI: {ai_response.content}\")\n",
        "\n",
        "# Close DuckDB connection when done\n",
        "con.close()"
      ],
      "metadata": {
        "id": "n1LJKaPPdZrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph"
      ],
      "metadata": {
        "id": "DxoSX2WNgH0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from google.colab import userdata\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "query = \"Hi! I'm Bob.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpc-FZkMgdnd",
        "outputId": "e9a4e233-acad-47a3-9889-8e1d111f01cf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Bob! It's nice to meet you. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_ChzdzSkEUP",
        "outputId": "298c8919-15fc-45d2-c4d9-89a4f8566ec0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You told me your name is Bob.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from google.colab import userdata\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    input_messages = [HumanMessage(query)]\n",
        "    output = app.invoke({\"messages\": input_messages}, config)\n",
        "    output[\"messages\"][-1].pretty_print()  # output contains all messages in state\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J022__l1yw9C",
        "outputId": "0fa203eb-5aaf-4ae9-a596-fd35037ca79e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: My name is Bob\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Okay, Bob. How can I help you today?\n",
            "You: What is my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob.\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates (Veidnes Uzvednēm)"
      ],
      "metadata": {
        "id": "5VxAv0JigYas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4-8ZjukVdHV",
        "outputId": "30c1ab6d-27d4-472e-dbf3-16b6ca8263f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='Tell me a joke about cats')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"user\", \"Tell me a joke about {topic}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"})\n",
        "# prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "# result = model.invoke(prompt)\n",
        "# print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvQ-HPWlXjrf",
        "outputId": "0ca26120-0732-4b3d-c872-6fe85f0a7d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    MessagesPlaceholder(\"msgs\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\"), HumanMessage(content=\"How are you?\")]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvdq1nwUbzu1",
        "outputId": "60a30f42-cbe2-441d-e4a3-51863fe5e502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}), HumanMessage(content='How are you?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "template = \"Tell me a joke about {topic}\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"------Prompt from template------\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "print(\"\\n------Result-------\")\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JnEN73kgSPd",
        "outputId": "f2c759de-98d6-467d-8740-17dab5070bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------Prompt from template------\n",
            "messages=[HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})]\n",
            "\n",
            "------Result-------\n",
            "Why was the cat sitting on the computer?  To keep an eye on the mouse!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ChatPromptTemplate using a template string\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"-----Prompt from Template-----\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1g2XbEHg-vN",
        "outputId": "48f60085-382d-450c-e845-2e1f8ce0bf68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt with Multiple Placeholders\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n02VDO8vhSrs",
        "outputId": "8c74480d-eed8-4960-cd03-82a1e824d47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny story about a panda.\\nAssistant:', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt with System and Human Messages (Using Tuples)\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
        "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtlIBKRbhnxN",
        "outputId": "2448b65c-932d-4b71-ec49-8f649e0105f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with System and Human Messages (Tuple) -----\n",
            "\n",
            "messages=[SystemMessage(content='You are a comedian who tells jokes about lawyers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains (Ķēdes)"
      ],
      "metadata": {
        "id": "rcZW6V34iSTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"user\", \"Tell me a {adjective} joke\")],\n",
        ")\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke({\"adjective\": \"funny\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y1iSX6unU0an",
        "outputId": "cc830d4d-227b-4bab-fa7a-d171a39a632b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't scientists trust atoms? \\n\\nBecause they make up everything!\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Chain\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "# StrOutputParser() ~ .content()\n",
        "# LangChain expression language\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "result = chain.invoke({\"topic\": \"fish\", \"joke_count\": 3})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv4Uq-suiVBN",
        "outputId": "72218d00-eedf-4a9e-cf49-95e5cee386cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.  Why did the fish blush?  Because it saw the sea-bed!\n",
            "\n",
            "\n",
            "2. What do you call a fish with no eyes?  Fsh!\n",
            "\n",
            "\n",
            "3.  I went to a seafood disco last week...  It was legen-dairy!  (…and totally cod-awful, to be honest.  The DJ was a real sole-mate, though.)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extended Chain\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
        "count_words = RunnableLambda(lambda x: f\"Word count {len(x.split())} \\n {x}\")\n",
        "\n",
        "chain = prompt_template | model | StrOutputParser() | uppercase_output | count_words\n",
        "\n",
        "result = chain.invoke({\"topic\": \"fish\", \"joke_count\": 3})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBkBF9oPjaVA",
        "outputId": "9aa4bb96-4323-4349-a266-fae4dae46651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count 43 \n",
            " 1. WHY DID THE FISH BLUSH?  BECAUSE IT SAW THE OCEAN BOTTOM!\n",
            "\n",
            "2. WHAT DO YOU CALL A FISH WITH NO EYES?  FSH!\n",
            "\n",
            "3. I WENT TO A SEAFOOD DISCO LAST WEEK...  IT WAS LEGEN-DAIRY!  (…AND THERE WAS A REALLY GOOD COD SELECTION).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parallel\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Define prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert product reviewer.\"),\n",
        "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Define pros analysis step\n",
        "def analyze_pros(features):\n",
        "    pros_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the pros of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return pros_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Define cons analysis step\n",
        "def analyze_cons(features):\n",
        "    cons_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the cons of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return cons_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Combine pros and cons into a final review\n",
        "def combine_pros_cons(pros, cons):\n",
        "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
        "\n",
        "\n",
        "# Simplify branches with LCEL\n",
        "pros_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "cons_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = (\n",
        "    prompt_template\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
        "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
        ")\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
        "\n",
        "# Output\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "o1YDVByWj0vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Branching\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableBranch\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define prompt templates for different feedback types\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the feedback classification template\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the runnable branches for handling feedback\n",
        "branches = RunnableBranch(\n",
        "    (\n",
        "        lambda x: \"positive\" in x,\n",
        "        positive_feedback_template | model | StrOutputParser()  # Positive feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"negative\" in x,\n",
        "        negative_feedback_template | model | StrOutputParser()  # Negative feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"neutral\" in x,\n",
        "        neutral_feedback_template | model | StrOutputParser()  # Neutral feedback chain\n",
        "    ),\n",
        "    escalate_feedback_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the classification chain\n",
        "classification_chain = classification_template | model | StrOutputParser()\n",
        "\n",
        "# Combine classification and response generation into one chain\n",
        "chain = classification_chain | branches\n",
        "\n",
        "# Run the chain with an example review\n",
        "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
        "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
        "\n",
        "review = \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "result = chain.invoke({\"feedback\": review})\n",
        "\n",
        "# Output the result\n",
        "print(result)"
      ],
      "metadata": {
        "id": "tQ2sjFYgkX--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents (Aģenti)"
      ],
      "metadata": {
        "id": "XomU3RqvlaS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU duckduckgo-search langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox6JxQStvW8j",
        "outputId": "a8fe549b-140c-447a-d82f-158182b39b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.5 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define the tool: DuckDuckGo search results\n",
        "ddg_search = DuckDuckGoSearchRun()\n",
        "\n",
        "# Set up the agent with the search tool\n",
        "tools = [Tool(\n",
        "    name=\"DuckDuckGoSearchRun\",\n",
        "    func=ddg_search.run,\n",
        "    description=\"Searches the web using DuckDuckGo.\"\n",
        ")]\n",
        "\n",
        "# Define a simple prompt template for asking a question\n",
        "prompt_template = \"Search the web for information about {query}\"\n",
        "\n",
        "# Initialize the agent with a basic agent type\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Use the agent to interact and fetch search results\n",
        "def get_search_results_using_agent(query: str):\n",
        "    query_formatted = prompt_template.format(query=query)\n",
        "    response = agent.run(query_formatted)\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "query = \"latest news on AI advancements\"\n",
        "search_results = get_search_results_using_agent(query)\n",
        "print(search_results)\n"
      ],
      "metadata": {
        "id": "_jf9DC-du94c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def load_data(query=None):  # Adding query parameter to make it compatible\n",
        "    # Load Iris dataset from sklearn\n",
        "    iris = load_iris()\n",
        "    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "    df['species'] = iris.target\n",
        "    return df\n",
        "\n",
        "# Function to clean data (simple imputation for missing values)\n",
        "def clean_data(df: pd.DataFrame, query=None):\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    df_clean = df.copy()\n",
        "    df_clean[:] = imputer.fit_transform(df)\n",
        "    return df_clean\n",
        "\n",
        "# Function to train a model (Random Forest)\n",
        "def train_model(df: pd.DataFrame, target_column: str, query=None):\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Random Forest model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Set up tools for the agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Load Data\",\n",
        "        func=load_data,\n",
        "        description=\"Loads the Iris dataset.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Clean Data\",\n",
        "        func=clean_data,\n",
        "        description=\"Cleans the dataset by handling missing values.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Train Model\",\n",
        "        func=train_model,\n",
        "        description=\"Trains a Random Forest model and returns the accuracy.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Initialize the agent with tools and LLM\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Automating a data science task\n",
        "def automate_data_science_task():\n",
        "    # Step 1: Load data\n",
        "    print(\"Loading data...\")\n",
        "    df = load_data()\n",
        "\n",
        "    # Step 2: Clean data\n",
        "    print(\"Cleaning data...\")\n",
        "    df_clean = clean_data(df)\n",
        "\n",
        "    # Step 3: Train model\n",
        "    print(\"Training model...\")\n",
        "    accuracy = train_model(df_clean, target_column=\"species\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Run the agent\n",
        "accuracy = automate_data_science_task()\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "X207mzlgyxUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoice extractor (Rēķinu izvilkšanas lietotne)"
      ],
      "metadata": {
        "id": "4IAJVycUd1wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccBVPQO8QooB",
        "outputId": "534139dd-7f62-4842-dc53-ec1c2224c9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "pdf_doc = \"/content/invoice_1001329.pdf\"\n",
        "pdf_reader = PdfReader(pdf_doc)\n",
        "text = \"\"\n",
        "for page in pdf_reader.pages:\n",
        "    text += page.extract_text()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzN1tiXfR7IA",
        "outputId": "b98e5ac5-5c26-4e78-98a8-a650f0449870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExcelCult\n",
            "India\n",
            "Bill To\n",
            "Ms.Santoshi\n",
            "Mumbai, India\n",
            "Santoshvarma0988@gmail.com\n",
            "9999999999\n",
            " \n",
            "  \n",
            "Invoice no.\n",
            "1001329\n",
            "Date\n",
            "5/4/2023\n",
            "Description\n",
            "Quantity\n",
            "Unit price\n",
            "Amount\n",
            "Total\n",
            "$2,200.00\n",
            "Office Chair\n",
            "2\n",
            "$1,100.00\n",
            "$2,200.00\n",
            "System Generated\n",
            " \n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Extract data from one invoice\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from pypdf import PdfReader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from langchain.schema import HumanMessage  # Import HumanMessage\n",
        "\n",
        "# Set up your Google API key\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"Google API key not found in userdata.\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# PDF file path\n",
        "pdf_path = \"/content/invoice_1001329.pdf\"\n",
        "\n",
        "# Extract text from PDF\n",
        "print(f\"Processing {pdf_path}...\")\n",
        "text = \"\"\n",
        "pdf_reader = PdfReader(pdf_path)\n",
        "for page in pdf_reader.pages:\n",
        "    text += page.extract_text()\n",
        "\n",
        "# Clean and prepare the text\n",
        "text = text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "# Construct the prompt\n",
        "prompt = f\"\"\"Extract all the following values: invoice no., Description, Quantity, date,\n",
        "Unit price, Amount, Total, email, phone number, and address from this data:\n",
        "\n",
        "{text}\n",
        "\n",
        "Expected output (JSON format):\n",
        "{{\n",
        "    \"Invoice no.\": \"1001329\",\n",
        "    \"Description\": \"Office Chair\",\n",
        "    \"Quantity\": \"2\",\n",
        "    \"Date\": \"5/4/2023\",\n",
        "    \"Unit price\": \"1100.00\",\n",
        "    \"Amount\": \"2200.00\",\n",
        "    \"Total\": \"2200.00\",\n",
        "    \"Email\": \"Santoshvarma0988@gmail.com\",\n",
        "    \"Phone number\": \"9999999999\",\n",
        "    \"Address\": \"Mumbai, India\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Debug: Print prompt to verify it\n",
        "print(\"Prompt sent to LLM:\\n\", prompt)\n",
        "\n",
        "# Initialize a DataFrame to store extracted data\n",
        "df = pd.DataFrame({\n",
        "    'Invoice no.': pd.Series(dtype='str'),\n",
        "    'Description': pd.Series(dtype='str'),\n",
        "    'Quantity': pd.Series(dtype='str'),\n",
        "    'Date': pd.Series(dtype='str'),\n",
        "    'Unit price': pd.Series(dtype='str'),\n",
        "    'Amount': pd.Series(dtype='float'),\n",
        "    'Total': pd.Series(dtype='float'),\n",
        "    'Email': pd.Series(dtype='str'),\n",
        "    'Phone number': pd.Series(dtype='str'),\n",
        "    'Address': pd.Series(dtype='str')\n",
        "})\n",
        "\n",
        "# Generate response from LLM\n",
        "try:\n",
        "    llm_response = llm([HumanMessage(content=prompt)])  # Wrap prompt in HumanMessage\n",
        "    print(\"LLM Response:\\n\", llm_response)\n",
        "    llm_content = llm_response.content  # Access the 'content' attribute\n",
        "\n",
        "    # Extract JSON-like content\n",
        "    pattern = r'{(.+?)}'\n",
        "    match = re.search(pattern, llm_content, re.DOTALL)  # Apply regex to llm_content\n",
        "\n",
        "    if match:\n",
        "        extracted_text = '{' + match.group(1) + '}'\n",
        "        try:\n",
        "            # Safely parse JSON response\n",
        "            data_dict = json.loads(extracted_text)\n",
        "            print(\"Parsed Data:\\n\", data_dict)\n",
        "\n",
        "            # Append to DataFrame\n",
        "            df = pd.concat([df, pd.DataFrame([data_dict])], ignore_index=True)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Failed to parse JSON response.\")\n",
        "    else:\n",
        "        print(\"No match found for JSON data extraction.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error generating LLM response: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = 'invoices_summary.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"CSV file created as '{csv_filename}'.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j-9qwnRxbLw",
        "outputId": "d4bbda3e-871f-4d37-98cc-c2330c91a18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/invoice_1001329.pdf...\n",
            "Prompt sent to LLM:\n",
            " Extract all the following values: invoice no., Description, Quantity, date, \n",
            "Unit price, Amount, Total, email, phone number, and address from this data:\n",
            "\n",
            "ExcelCult India Bill To Ms.Santoshi Mumbai, India Santoshvarma0988@gmail.com 9999999999      Invoice no. 1001329 Date 5/4/2023 Description Quantity Unit price Amount Total $2,200.00 Office Chair 2 $1,100.00 $2,200.00 System Generated\n",
            "\n",
            "Expected output (JSON format): \n",
            "{\n",
            "    \"Invoice no.\": \"1001329\",\n",
            "    \"Description\": \"Office Chair\",\n",
            "    \"Quantity\": \"2\",\n",
            "    \"Date\": \"5/4/2023\",\n",
            "    \"Unit price\": \"1100.00\",\n",
            "    \"Amount\": \"2200.00\",\n",
            "    \"Total\": \"2200.00\",\n",
            "    \"Email\": \"Santoshvarma0988@gmail.com\",\n",
            "    \"Phone number\": \"9999999999\",\n",
            "    \"Address\": \"Mumbai, India\"\n",
            "}\n",
            "\n",
            "LLM Response:\n",
            " content='```json\\n{\\n  \"Invoice no.\": \"1001329\",\\n  \"Description\": \"Office Chair\",\\n  \"Quantity\": \"2\",\\n  \"Date\": \"5/4/2023\",\\n  \"Unit price\": \"1100.00\",\\n  \"Amount\": \"2200.00\",\\n  \"Total\": \"2200.00\",\\n  \"Email\": \"Santoshvarma0988@gmail.com\",\\n  \"Phone number\": \"9999999999\",\\n  \"Address\": \"Mumbai, India\"\\n}\\n```\\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-f2c56087-7656-4789-b37f-3137bb1c40cf-0' usage_metadata={'input_tokens': 283, 'output_tokens': 146, 'total_tokens': 429, 'input_token_details': {'cache_read': 0}}\n",
            "Parsed Data:\n",
            " {'Invoice no.': '1001329', 'Description': 'Office Chair', 'Quantity': '2', 'Date': '5/4/2023', 'Unit price': '1100.00', 'Amount': '2200.00', 'Total': '2200.00', 'Email': 'Santoshvarma0988@gmail.com', 'Phone number': '9999999999', 'Address': 'Mumbai, India'}\n",
            "CSV file created as 'invoices_summary.csv'.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Extract data from multiple invoices\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from pypdf import PdfReader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# Set up your Google API key\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"Google API key not found in userdata.\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# List of PDF file paths\n",
        "pdf_files = [\n",
        "    \"/content/invoice_1001329.pdf\",\n",
        "    \"/content/invoice_2001321.pdf\",\n",
        "    \"/content/invoice_3452334.pdf\",\n",
        "    # ... add more file paths\n",
        "]\n",
        "\n",
        "# Initialize a DataFrame to store extracted data\n",
        "df = pd.DataFrame({\n",
        "    'Invoice no.': pd.Series(dtype='str'),\n",
        "    'Description': pd.Series(dtype='str'),\n",
        "    'Quantity': pd.Series(dtype='str'),\n",
        "    'Date': pd.Series(dtype='str'),\n",
        "    'Unit price': pd.Series(dtype='str'),\n",
        "    'Amount': pd.Series(dtype='float'),\n",
        "    'Total': pd.Series(dtype='float'),\n",
        "    'Email': pd.Series(dtype='str'),\n",
        "    'Phone number': pd.Series(dtype='str'),\n",
        "    'Address': pd.Series(dtype='str')\n",
        "})\n",
        "\n",
        "# Loop through PDF file paths in the list\n",
        "for pdf_path in pdf_files:\n",
        "    # Extract text from PDF\n",
        "    print(f\"Processing {pdf_path}...\")\n",
        "    text = \"\"\n",
        "    pdf_reader = PdfReader(pdf_path)\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "\n",
        "    # Clean and prepare the text\n",
        "    text = text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    # Construct the prompt (same as before)\n",
        "    # ... (your prompt code here) ...\n",
        "\n",
        "    # Generate response from LLM and extract data\n",
        "    try:\n",
        "        # ... (your LLM response and data extraction code here) ...\n",
        "        # Append extracted data to DataFrame\n",
        "        df = pd.concat([df, pd.DataFrame([data_dict])], ignore_index=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdf_path}: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = 'multy_invoice_summary.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"CSV file created as '{csv_filename}'.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrdn0Qtpyu2X",
        "outputId": "4629d0e4-b8b0-4512-c159-c97aea0340ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/invoice_1001329.pdf...\n",
            "Processing /content/invoice_2001321.pdf...\n",
            "Processing /content/invoice_3452334.pdf...\n",
            "CSV file created as 'multy_invoice_summary.csv'.\n"
          ]
        }
      ]
    }
  ]
}