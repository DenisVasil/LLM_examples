{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWbwAkhAHgjn2bNpBk07kF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenisVasil/LLM_examples/blob/main/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Models (čata modeļi)"
      ],
      "metadata": {
        "id": "vBOJzQF4bGPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "wa0lp07tR4Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76521003-0429-4acc-8369-a5a0d6c91c0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.0.9-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.8.4)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.33)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.10.6)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.160.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.25.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.26.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.3.5)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.27.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain_google_genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_google_genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.0.9-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain_google_genai\n",
            "Successfully installed filetype-1.2.0 langchain_google_genai-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjzm9ulcRYUu",
        "outputId": "3a0bf440-39fa-49e3-aef1-5ab7b7584f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full result:\n",
            "content='81 divided by 9 is 9.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-175648f4-fe16-49a2-86ea-d6516608aca8-0' usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21, 'input_token_details': {'cache_read': 0}}\n",
            "Context:\n",
            "81 divided by 9 is 9.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "result = model.invoke(\"What is 81 divided by 9\")\n",
        "\n",
        "print(\"Full result:\")\n",
        "print(result)\n",
        "print(\"Context:\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Conversation\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problem\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\")\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "\n",
        "print(f\"Answer from AI: {result.content}\")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problem\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "    AIMessage(content=f\"{result.content}\"),\n",
        "    HumanMessage(content=\"Divide this result by 3\")\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "print(f\"New answer from AI: {result.content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EtoGySeR2l4",
        "outputId": "512787b4-12a9-404b-b674-6065b4799aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer from AI: 81 divided by 9 is 9.\n",
            "New answer from AI: 9 divided by 3 is 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory.chat_memory import InMemoryChatMessageHistory\n",
        "\n",
        "history = InMemoryChatMessageHistory()\n",
        "history.add_message({\"role\": \"user\", \"content\": \"Hello\"})\n",
        "history.add_message({\"role\": \"assistant\", \"content\": \"Hi there!\"})\n",
        "\n",
        "print(history.messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU4PBcJF2-2I",
        "outputId": "32d40ac8-eb88-4b52-a850-f5c98477e2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'user', 'content': 'Hello'}, {'role': 'assistant', 'content': 'Hi there!'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import HumanMessage, AIMessage # Import necessary classes\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "memory.chat_memory.add_message(HumanMessage(content=\"Hello\")) # Use HumanMessage\n",
        "memory.chat_memory.add_message(AIMessage(content=\"Hi there!\")) # Use AIMessage\n",
        "\n",
        "print(memory.load_memory_variables({}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hfMLbpa2glh",
        "outputId": "12f0c42b-7894-4a9a-a303-1a48c3bb3c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'history': 'Human: Hello\\nAI: Hi there!'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-27d192231410>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chat\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "system_message = SystemMessage(content=\"You are helpful AI assistant\")\n",
        "chat_history.append(system_message)\n",
        "\n",
        "# chat loop\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    chat_history.append(HumanMessage(content=query))\n",
        "\n",
        "    result = model.invoke(chat_history)\n",
        "    response = result.content\n",
        "    chat_history.append(AIMessage(content=response))\n",
        "    print(f\"AI response: {response}\")\n",
        "\n",
        "print(\"----- Message history ------\")\n",
        "print(chat_history)"
      ],
      "metadata": {
        "id": "tuJXOC96R27m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe60bba-a1ac-465b-9dd5-4632ac6f5462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: How many birds are there in Latvia?\n",
            "AI response: It's impossible to give an exact number of birds in Latvia at any given moment.  Bird populations are dynamic and constantly changing due to:\n",
            "\n",
            "* **Migration:**  Latvia is on a major migration route, so the number of birds present fluctuates dramatically throughout the year.  Millions of birds pass through during spring and autumn.\n",
            "* **Breeding Cycles:** Bird numbers increase during breeding seasons as chicks hatch and fledge.\n",
            "* **Food Availability:** Bird populations are affected by the availability of food resources, which can vary seasonally and annually.\n",
            "* **Predation and other environmental factors:** Weather conditions, disease, and predator populations all impact bird numbers.\n",
            "\n",
            "While a precise count isn't feasible, ornithologists and conservation organizations in Latvia monitor bird populations through various surveys and studies.  These provide estimates of population sizes for different species and help track long-term trends.  You could contact the Latvian Ornithological Society or similar organizations for more information on specific species and their estimated populations within the country.\n",
            "You: Which birds species are the most common here?\n",
            "AI response: Some of the most common bird species in Latvia include:\n",
            "\n",
            "* **Great Tit ( *Parus major* ):** A widespread and easily recognizable garden bird.\n",
            "* **Chaffinch (*Fringilla coelebs*):** Another common garden bird, known for its cheerful song.\n",
            "* **Willow Warbler (*Phylloscopus trochilus*):** A small, migratory warbler that breeds in Latvia.\n",
            "* **White Wagtail (*Motacilla alba*):** Often seen near water, wagging its tail characteristically.\n",
            "* **Eurasian Wren (*Troglodytes troglodytes*):** A tiny bird with a loud voice, found in a variety of habitats.\n",
            "* **Robin (*Erithacus rubecula*):** A familiar garden bird with a reddish-orange breast.\n",
            "* **Blackbird (*Turdus merula*):** A common blackbird, often found in gardens and parks.\n",
            "* **Common Starling (*Sturnus vulgaris*):** A highly adaptable bird found in large flocks, especially in urban areas.\n",
            "* **House Sparrow (*Passer domesticus*):** A small, brown bird commonly found around human settlements.\n",
            "* **Rook (*Corvus frugilegus*):** A large, black bird often seen in fields and agricultural areas.\n",
            "\n",
            "\n",
            "This is not an exhaustive list, and the prevalence of certain species can vary depending on the specific habitat and time of year.  However, these are some of the birds you are most likely to encounter in Latvia.\n",
            "You: exit\n",
            "----- Message history ------\n",
            "[SystemMessage(content='You are helpful AI assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='How many birds are there in Latvia?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"It's impossible to give an exact number of birds in Latvia at any given moment.  Bird populations are dynamic and constantly changing due to:\\n\\n* **Migration:**  Latvia is on a major migration route, so the number of birds present fluctuates dramatically throughout the year.  Millions of birds pass through during spring and autumn.\\n* **Breeding Cycles:** Bird numbers increase during breeding seasons as chicks hatch and fledge.\\n* **Food Availability:** Bird populations are affected by the availability of food resources, which can vary seasonally and annually.\\n* **Predation and other environmental factors:** Weather conditions, disease, and predator populations all impact bird numbers.\\n\\nWhile a precise count isn't feasible, ornithologists and conservation organizations in Latvia monitor bird populations through various surveys and studies.  These provide estimates of population sizes for different species and help track long-term trends.  You could contact the Latvian Ornithological Society or similar organizations for more information on specific species and their estimated populations within the country.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Which birds species are the most common here?', additional_kwargs={}, response_metadata={}), AIMessage(content='Some of the most common bird species in Latvia include:\\n\\n* **Great Tit ( *Parus major* ):** A widespread and easily recognizable garden bird.\\n* **Chaffinch (*Fringilla coelebs*):** Another common garden bird, known for its cheerful song.\\n* **Willow Warbler (*Phylloscopus trochilus*):** A small, migratory warbler that breeds in Latvia.\\n* **White Wagtail (*Motacilla alba*):** Often seen near water, wagging its tail characteristically.\\n* **Eurasian Wren (*Troglodytes troglodytes*):** A tiny bird with a loud voice, found in a variety of habitats.\\n* **Robin (*Erithacus rubecula*):** A familiar garden bird with a reddish-orange breast.\\n* **Blackbird (*Turdus merula*):** A common blackbird, often found in gardens and parks.\\n* **Common Starling (*Sturnus vulgaris*):** A highly adaptable bird found in large flocks, especially in urban areas.\\n* **House Sparrow (*Passer domesticus*):** A small, brown bird commonly found around human settlements.\\n* **Rook (*Corvus frugilegus*):** A large, black bird often seen in fields and agricultural areas.\\n\\n\\nThis is not an exhaustive list, and the prevalence of certain species can vary depending on the specific habitat and time of year.  However, these are some of the birds you are most likely to encounter in Latvia.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv1IsvPpfhs3",
        "outputId": "f2a68ccd-fd7f-414a-dbb2-08d77961cb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (1.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Chat History\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.memory.chat_memory import InMemoryChatMessageHistory\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "import duckdb\n",
        "import os\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Constants\n",
        "DB_PATH = \"chat_history.db\"\n",
        "TABLE_NAME = \"chat_history\"\n",
        "SESSION_ID = \"user_session_new\"  # This could be a username or unique ID\n",
        "\n",
        "# Initialize DuckDB\n",
        "con = duckdb.connect(DB_PATH)\n",
        "con.execute(f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
        "    session_id TEXT,\n",
        "    role TEXT,\n",
        "    content TEXT\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Helper Functions\n",
        "\n",
        "\n",
        "def save_message_to_db(session_id, role, content):\n",
        "    \"\"\"Save a single message to the DuckDB database.\"\"\"\n",
        "    con.execute(f\"\"\"\n",
        "    INSERT INTO {TABLE_NAME} (session_id, role, content)\n",
        "    VALUES (?, ?, ?)\n",
        "    \"\"\", [session_id, role, content])\n",
        "\n",
        "\n",
        "def load_messages_from_db(session_id):\n",
        "    \"\"\"Load chat history for a specific session from the DuckDB database.\"\"\"\n",
        "    result = con.execute(f\"\"\"\n",
        "    SELECT role, content FROM {TABLE_NAME} WHERE session_id = ? ORDER BY rowid\n",
        "    \"\"\", [session_id]).fetchall()\n",
        "    return [HumanMessage(content=row[1]) if row[0] == \"user\" else AIMessage(content=row[1]) for row in result]\n",
        "\n",
        "\n",
        "# Initialize Chat Message History\n",
        "print(\"Initializing DuckDB Chat Message History...\")\n",
        "messages = load_messages_from_db(SESSION_ID)\n",
        "chat_history = InMemoryChatMessageHistory(messages=messages)\n",
        "\n",
        "print(\"Chat History Initialized.\")\n",
        "print(\"Current Chat History:\", [msg.content for msg in chat_history.messages])\n",
        "\n",
        "\n",
        "print(\"Start chatting with the AI. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    human_input = input(\"User: \")\n",
        "    if human_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Save and process user message\n",
        "    chat_history.add_user_message(human_input)\n",
        "    save_message_to_db(SESSION_ID, \"user\", human_input)\n",
        "\n",
        "    # Generate AI response\n",
        "    ai_response = model.invoke(chat_history.messages)\n",
        "    chat_history.add_ai_message(ai_response.content)\n",
        "    save_message_to_db(SESSION_ID, \"ai\", ai_response.content)\n",
        "\n",
        "    print(f\"AI: {ai_response.content}\")\n",
        "\n",
        "# Close DuckDB connection when done\n",
        "con.close()"
      ],
      "metadata": {
        "id": "n1LJKaPPdZrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph"
      ],
      "metadata": {
        "id": "DxoSX2WNgH0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from google.colab import userdata\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "query = \"Hi! I'm Bob.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpc-FZkMgdnd",
        "outputId": "e9a4e233-acad-47a3-9889-8e1d111f01cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Bob! It's nice to meet you. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_ChzdzSkEUP",
        "outputId": "298c8919-15fc-45d2-c4d9-89a4f8566ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You told me your name is Bob.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from google.colab import userdata\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    input_messages = [HumanMessage(query)]\n",
        "    output = app.invoke({\"messages\": input_messages}, config)\n",
        "    output[\"messages\"][-1].pretty_print()  # output contains all messages in state\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J022__l1yw9C",
        "outputId": "0fa203eb-5aaf-4ae9-a596-fd35037ca79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: My name is Bob\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Okay, Bob. How can I help you today?\n",
            "You: What is my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob.\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates (Veidnes Uzvednēm)"
      ],
      "metadata": {
        "id": "5VxAv0JigYas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4-8ZjukVdHV",
        "outputId": "30c1ab6d-27d4-472e-dbf3-16b6ca8263f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='Tell me a joke about cats')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"user\", \"Tell me a joke about {topic}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"})\n",
        "# prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "# result = model.invoke(prompt)\n",
        "# print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvQ-HPWlXjrf",
        "outputId": "0ca26120-0732-4b3d-c872-6fe85f0a7d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    MessagesPlaceholder(\"msgs\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\"), HumanMessage(content=\"How are you?\")]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvdq1nwUbzu1",
        "outputId": "60a30f42-cbe2-441d-e4a3-51863fe5e502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}), HumanMessage(content='How are you?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "template = \"Tell me a joke about {topic}\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"------Prompt from template------\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "print(\"\\n------Result-------\")\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JnEN73kgSPd",
        "outputId": "f2c759de-98d6-467d-8740-17dab5070bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------Prompt from template------\n",
            "messages=[HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})]\n",
            "\n",
            "------Result-------\n",
            "Why was the cat sitting on the computer?  To keep an eye on the mouse!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ChatPromptTemplate using a template string\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"-----Prompt from Template-----\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1g2XbEHg-vN",
        "outputId": "48f60085-382d-450c-e845-2e1f8ce0bf68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt with Multiple Placeholders\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n02VDO8vhSrs",
        "outputId": "8c74480d-eed8-4960-cd03-82a1e824d47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with Multiple Placeholders -----\n",
            "\n",
            "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny story about a panda.\\nAssistant:', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt with System and Human Messages (Using Tuples)\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
        "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtlIBKRbhnxN",
        "outputId": "2448b65c-932d-4b71-ec49-8f649e0105f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with System and Human Messages (Tuple) -----\n",
            "\n",
            "messages=[SystemMessage(content='You are a comedian who tells jokes about lawyers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains (Ķēdes)"
      ],
      "metadata": {
        "id": "rcZW6V34iSTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"user\", \"Tell me a {adjective} joke\")],\n",
        ")\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke({\"adjective\": \"funny\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "y1iSX6unU0an",
        "outputId": "cc830d4d-227b-4bab-fa7a-d171a39a632b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't scientists trust atoms? \\n\\nBecause they make up everything!\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Chain\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "# StrOutputParser() ~ .content()\n",
        "# LangChain expression language\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "result = chain.invoke({\"topic\": \"fish\", \"joke_count\": 3})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv4Uq-suiVBN",
        "outputId": "72218d00-eedf-4a9e-cf49-95e5cee386cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.  Why did the fish blush?  Because it saw the sea-bed!\n",
            "\n",
            "\n",
            "2. What do you call a fish with no eyes?  Fsh!\n",
            "\n",
            "\n",
            "3.  I went to a seafood disco last week...  It was legen-dairy!  (…and totally cod-awful, to be honest.  The DJ was a real sole-mate, though.)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extended Chain\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
        "count_words = RunnableLambda(lambda x: f\"Word count {len(x.split())} \\n {x}\")\n",
        "\n",
        "chain = prompt_template | model | StrOutputParser() | uppercase_output | count_words\n",
        "\n",
        "result = chain.invoke({\"topic\": \"fish\", \"joke_count\": 3})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBkBF9oPjaVA",
        "outputId": "9aa4bb96-4323-4349-a266-fae4dae46651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count 43 \n",
            " 1. WHY DID THE FISH BLUSH?  BECAUSE IT SAW THE OCEAN BOTTOM!\n",
            "\n",
            "2. WHAT DO YOU CALL A FISH WITH NO EYES?  FSH!\n",
            "\n",
            "3. I WENT TO A SEAFOOD DISCO LAST WEEK...  IT WAS LEGEN-DAIRY!  (…AND THERE WAS A REALLY GOOD COD SELECTION).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parallel\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Define prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert product reviewer.\"),\n",
        "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Define pros analysis step\n",
        "def analyze_pros(features):\n",
        "    pros_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the pros of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return pros_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Define cons analysis step\n",
        "def analyze_cons(features):\n",
        "    cons_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the cons of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return cons_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Combine pros and cons into a final review\n",
        "def combine_pros_cons(pros, cons):\n",
        "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
        "\n",
        "\n",
        "# Simplify branches with LCEL\n",
        "pros_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "cons_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = (\n",
        "    prompt_template\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
        "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
        ")\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
        "\n",
        "# Output\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "o1YDVByWj0vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Branching\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableBranch\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define prompt templates for different feedback types\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the feedback classification template\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the runnable branches for handling feedback\n",
        "branches = RunnableBranch(\n",
        "    (\n",
        "        lambda x: \"positive\" in x,\n",
        "        positive_feedback_template | model | StrOutputParser()  # Positive feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"negative\" in x,\n",
        "        negative_feedback_template | model | StrOutputParser()  # Negative feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"neutral\" in x,\n",
        "        neutral_feedback_template | model | StrOutputParser()  # Neutral feedback chain\n",
        "    ),\n",
        "    escalate_feedback_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the classification chain\n",
        "classification_chain = classification_template | model | StrOutputParser()\n",
        "\n",
        "# Combine classification and response generation into one chain\n",
        "chain = classification_chain | branches\n",
        "\n",
        "# Run the chain with an example review\n",
        "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
        "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
        "\n",
        "review = \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "result = chain.invoke({\"feedback\": review})\n",
        "\n",
        "# Output the result\n",
        "print(result)"
      ],
      "metadata": {
        "id": "tQ2sjFYgkX--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Loaders (Dokumentu Ielādētāji)"
      ],
      "metadata": {
        "id": "IHVJMxYD54OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjtFqa7K6HkJ",
        "outputId": "901a8010-ad26-4783-df5f-7e19fad2a7e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.17-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.34 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.34-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.18 (from langchain-community)\n",
            "  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.37)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.5)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain<1.0.0,>=0.3.18->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.18->langchain-community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.18->langchain-community) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.17-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.18-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.34-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.33\n",
            "    Uninstalling langchain-core-0.3.33:\n",
            "      Successfully uninstalled langchain-core-0.3.33\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.5\n",
            "    Uninstalling langchain-text-splitters-0.3.5:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.5\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.17\n",
            "    Uninstalling langchain-0.3.17:\n",
            "      Successfully uninstalled langchain-0.3.17\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.18 langchain-community-0.3.17 langchain-core-0.3.34 langchain-text-splitters-0.3.6 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "file_path = \"/content/diabetes.csv\"\n",
        "\n",
        "loader = CSVLoader(file_path=file_path)\n",
        "data = loader.load()\n",
        "\n",
        "for record in data[:2]:\n",
        "    #print(type(record))\n",
        "    print(record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toKXmvoW52Tb",
        "outputId": "c908f381-fa01-4e36-d763-0ca678329ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Pregnancies: 6\n",
            "Glucose: 148\n",
            "BloodPressure: 72\n",
            "SkinThickness: 35\n",
            "Insulin: 0\n",
            "BMI: 33.6\n",
            "DiabetesPedigreeFunction: 0.627\n",
            "Age: 50\n",
            "Outcome: 1' metadata={'source': '/content/diabetes.csv', 'row': 0}\n",
            "page_content='Pregnancies: 1\n",
            "Glucose: 85\n",
            "BloodPressure: 66\n",
            "SkinThickness: 29\n",
            "Insulin: 0\n",
            "BMI: 26.6\n",
            "DiabetesPedigreeFunction: 0.351\n",
            "Age: 31\n",
            "Outcome: 0' metadata={'source': '/content/diabetes.csv', 'row': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2PH9ArP81cE",
        "outputId": "6aa4956c-72d8-4a51-e7db-fa1f131a4c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/298.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/invoice_1001329.pdf\")\n",
        "pages = []\n",
        "\n",
        "async for page in loader.alazy_load():\n",
        "    pages.append(page)\n",
        "print(f\"{pages[0].metadata}\\n\")\n",
        "print(pages[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxwhF9aR88xT",
        "outputId": "875dfc8d-1726-4c6c-b6bf-2616cdcc5d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': '/content/invoice_1001329.pdf', 'page': 0}\n",
            "\n",
            "ExcelCult\n",
            "India\n",
            "Bill To\n",
            "Ms.Santoshi\n",
            "Mumbai, India\n",
            "Santoshvarma0988@gmail.com\n",
            "9999999999\n",
            " \n",
            "  \n",
            "Invoice no.\n",
            "1001329\n",
            "Date\n",
            "5/4/2023\n",
            "Description\n",
            "Quantity\n",
            "Unit price\n",
            "Amount\n",
            "Total\n",
            "$2,200.00\n",
            "Office Chair\n",
            "2\n",
            "$1,100.00\n",
            "$2,200.00\n",
            "System Generated\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents (Aģenti)"
      ],
      "metadata": {
        "id": "XomU3RqvlaS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU duckduckgo-search langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox6JxQStvW8j",
        "outputId": "a8fe549b-140c-447a-d82f-158182b39b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.5 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define the tool: DuckDuckGo search results\n",
        "ddg_search = DuckDuckGoSearchRun()\n",
        "\n",
        "# Set up the agent with the search tool\n",
        "tools = [Tool(\n",
        "    name=\"DuckDuckGoSearchRun\",\n",
        "    func=ddg_search.run,\n",
        "    description=\"Searches the web using DuckDuckGo.\"\n",
        ")]\n",
        "\n",
        "# Define a simple prompt template for asking a question\n",
        "prompt_template = \"Search the web for information about {query}\"\n",
        "\n",
        "# Initialize the agent with a basic agent type\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Use the agent to interact and fetch search results\n",
        "def get_search_results_using_agent(query: str):\n",
        "    query_formatted = prompt_template.format(query=query)\n",
        "    response = agent.run(query_formatted)\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "query = \"latest news on AI advancements\"\n",
        "search_results = get_search_results_using_agent(query)\n",
        "print(search_results)\n"
      ],
      "metadata": {
        "id": "_jf9DC-du94c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def load_data(query=None):  # Adding query parameter to make it compatible\n",
        "    # Load Iris dataset from sklearn\n",
        "    iris = load_iris()\n",
        "    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "    df['species'] = iris.target\n",
        "    return df\n",
        "\n",
        "# Function to clean data (simple imputation for missing values)\n",
        "def clean_data(df: pd.DataFrame, query=None):\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    df_clean = df.copy()\n",
        "    df_clean[:] = imputer.fit_transform(df)\n",
        "    return df_clean\n",
        "\n",
        "# Function to train a model (Random Forest)\n",
        "def train_model(df: pd.DataFrame, target_column: str, query=None):\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Random Forest model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Set up tools for the agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Load Data\",\n",
        "        func=load_data,\n",
        "        description=\"Loads the Iris dataset.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Clean Data\",\n",
        "        func=clean_data,\n",
        "        description=\"Cleans the dataset by handling missing values.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Train Model\",\n",
        "        func=train_model,\n",
        "        description=\"Trains a Random Forest model and returns the accuracy.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Initialize the agent with tools and LLM\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Automating a data science task\n",
        "def automate_data_science_task():\n",
        "    # Step 1: Load data\n",
        "    print(\"Loading data...\")\n",
        "    df = load_data()\n",
        "\n",
        "    # Step 2: Clean data\n",
        "    print(\"Cleaning data...\")\n",
        "    df_clean = clean_data(df)\n",
        "\n",
        "    # Step 3: Train model\n",
        "    print(\"Training model...\")\n",
        "    accuracy = train_model(df_clean, target_column=\"species\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Run the agent\n",
        "accuracy = automate_data_science_task()\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "X207mzlgyxUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Create a tool\n",
        "@tool\n",
        "def super_function(input: int) -> int:\n",
        "    \"\"\"Applies a magic function to an input.\"\"\"\n",
        "    return input + 2\n",
        "\n",
        "\n",
        "tools = [super_function]\n",
        "\n",
        "\n",
        "query = \"what is the value of super_function(3)?\"\n",
        "\n",
        "langgraph_agent_executor = create_react_agent(model, tools)\n",
        "\n",
        "\n",
        "messages = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})\n",
        "{\n",
        "    \"input\": query,\n",
        "    \"output\": messages[\"messages\"][-1].content,\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkPUmwHfI6X8",
        "outputId": "cae3121f-2c2d-431f-bff1-eb42db68e7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is the value of super_function(3)?',\n",
              " 'output': 'The value of `super_function(3)` is `{\"output\": 5}`.'}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools (Rīki)"
      ],
      "metadata": {
        "id": "78jpzxVBEnNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "# Let's inspect some of the attributes associated with the tool.\n",
        "print(multiply.name)\n",
        "print(multiply.description)\n",
        "print(multiply.args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1qMwQwuPoJ_",
        "outputId": "7b1277a7-89ff-45b4-e741-dd7c6bca933a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multiply\n",
            "Multiply two numbers.\n",
            "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-experimental"
      ],
      "metadata": {
        "id": "_tKjxRFgEXHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "python_repl = PythonREPL()\n",
        "python_repl.run(\"print(1+1)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sxMXY1ozDLfg",
        "outputId": "d608bd38-1a64-480f-9346-32d6febef793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoice extractor (Rēķinu izvilkšanas lietotne)"
      ],
      "metadata": {
        "id": "4IAJVycUd1wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccBVPQO8QooB",
        "outputId": "32254b72-e900-41a7-a5f2-ca9708848fdd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.2.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.2.0-py3-none-any.whl (298 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/298.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "pdf_doc = \"/content/invoice_1001329.pdf\"\n",
        "pdf_reader = PdfReader(pdf_doc)\n",
        "text = \"\"\n",
        "for page in pdf_reader.pages:\n",
        "    text += page.extract_text()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzN1tiXfR7IA",
        "outputId": "0b30efa6-bf1a-4d9b-828f-9660593122fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExcelCult\n",
            "India\n",
            "Bill To\n",
            "Ms.Santoshi\n",
            "Mumbai, India\n",
            "Santoshvarma0988@gmail.com\n",
            "9999999999\n",
            " \n",
            "  \n",
            "Invoice no.\n",
            "1001329\n",
            "Date\n",
            "5/4/2023\n",
            "Description\n",
            "Quantity\n",
            "Unit price\n",
            "Amount\n",
            "Total\n",
            "$2,200.00\n",
            "Office Chair\n",
            "2\n",
            "$1,100.00\n",
            "$2,200.00\n",
            "System Generated\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from pydantic import BaseModel, ValidationError\n",
        "from pypdf import PdfReader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# Set up your Google API key\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"Google API key not found in userdata.\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Define a Pydantic model for invoice data validation\n",
        "class InvoiceData(BaseModel):\n",
        "    Invoice_no: str\n",
        "    Description: str\n",
        "    Quantity: str\n",
        "    Date: str\n",
        "    Unit_price: str\n",
        "    Amount: float\n",
        "    Total: float\n",
        "    Email: str\n",
        "    Phone_number: str\n",
        "    Address: str\n",
        "\n",
        "# PDF file path\n",
        "pdf_path = \"/content/invoice_1001329.pdf\"\n",
        "\n",
        "# Extract text from PDF\n",
        "print(f\"Processing {pdf_path}...\")\n",
        "text = \"\"\n",
        "pdf_reader = PdfReader(pdf_path)\n",
        "for page in pdf_reader.pages:\n",
        "    text += page.extract_text()\n",
        "\n",
        "# Clean and prepare the text\n",
        "text = text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "# Construct the prompt\n",
        "prompt = f\"\"\"Extract all the following values: invoice no., Description, Quantity, date,\n",
        "Unit price, Amount, Total, email, phone number, and address from this data:\n",
        "\n",
        "{text}\n",
        "\n",
        "Expected output (JSON format):\n",
        "{{\n",
        "    \"Invoice_no\": \"1001329\",\n",
        "    \"Description\": \"Office Chair\",\n",
        "    \"Quantity\": \"2\",\n",
        "    \"Date\": \"5/4/2023\",\n",
        "    \"Unit_price\": \"1100.00\",\n",
        "    \"Amount\": 2200.00,\n",
        "    \"Total\": 2200.00,\n",
        "    \"Email\": \"Santoshvarma0988@gmail.com\",\n",
        "    \"Phone_number\": \"9999999999\",\n",
        "    \"Address\": \"Mumbai, India\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Debug: Print prompt to verify it\n",
        "print(\"Prompt sent to LLM:\\n\", prompt)\n",
        "\n",
        "# Initialize a DataFrame\n",
        "df = pd.DataFrame(columns=[\n",
        "    'Invoice_no', 'Description', 'Quantity', 'Date', 'Unit_price',\n",
        "    'Amount', 'Total', 'Email', 'Phone_number', 'Address'\n",
        "])\n",
        "\n",
        "# Generate response from LLM\n",
        "try:\n",
        "    llm_response = llm.invoke([HumanMessage(content=prompt)])  # Use `invoke()` instead of calling the model directly\n",
        "    llm_content = llm_response.content  # Access response content\n",
        "\n",
        "    # Clean response: Remove triple backticks and JSON keyword\n",
        "    cleaned_text = re.sub(r\"```json\\s*|\\s*```\", \"\", llm_content).strip()\n",
        "\n",
        "    # Validate and parse JSON response using Pydantic\n",
        "    invoice_data = InvoiceData.model_validate_json(cleaned_text)\n",
        "    print(\"Validated Data:\\n\", invoice_data)\n",
        "\n",
        "    # Convert Pydantic model to dict and append to DataFrame\n",
        "    new_data = pd.DataFrame([invoice_data.model_dump()])\n",
        "\n",
        "    # Append new data safely\n",
        "    if df.empty:\n",
        "        df = new_data\n",
        "    else:\n",
        "        df = pd.concat([df, new_data], ignore_index=True)\n",
        "\n",
        "except ValidationError as e:\n",
        "    print(\"Validation Error:\\n\", e)\n",
        "except Exception as e:\n",
        "    print(f\"Error generating LLM response: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = 'invoices_summary.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"CSV file created as '{csv_filename}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umD5hvBrLKhA",
        "outputId": "d0074a93-2c0f-4090-abe4-105b91e074cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/invoice_1001329.pdf...\n",
            "Prompt sent to LLM:\n",
            " Extract all the following values: invoice no., Description, Quantity, date,\n",
            "Unit price, Amount, Total, email, phone number, and address from this data:\n",
            "\n",
            "ExcelCult India Bill To Ms.Santoshi Mumbai, India Santoshvarma0988@gmail.com 9999999999      Invoice no. 1001329 Date 5/4/2023 Description Quantity Unit price Amount Total $2,200.00 Office Chair 2 $1,100.00 $2,200.00 System Generated\n",
            "\n",
            "Expected output (JSON format):\n",
            "{\n",
            "    \"Invoice_no\": \"1001329\",\n",
            "    \"Description\": \"Office Chair\",\n",
            "    \"Quantity\": \"2\",\n",
            "    \"Date\": \"5/4/2023\",\n",
            "    \"Unit_price\": \"1100.00\",\n",
            "    \"Amount\": 2200.00,\n",
            "    \"Total\": 2200.00,\n",
            "    \"Email\": \"Santoshvarma0988@gmail.com\",\n",
            "    \"Phone_number\": \"9999999999\",\n",
            "    \"Address\": \"Mumbai, India\"\n",
            "}\n",
            "\n",
            "Validated Data:\n",
            " Invoice_no='1001329' Description='Office Chair' Quantity='2' Date='5/4/2023' Unit_price='1100.00' Amount=2200.0 Total=2200.0 Email='Santoshvarma0988@gmail.com' Phone_number='9999999999' Address='Mumbai, India'\n",
            "CSV file created as 'invoices_summary.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from pydantic import BaseModel, ValidationError\n",
        "from pypdf import PdfReader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# Set up your Google API key\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"Google API key not found in userdata.\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Define a Pydantic model for invoice data validation\n",
        "class InvoiceData(BaseModel):\n",
        "    Invoice_no: str\n",
        "    Description: str\n",
        "    Quantity: str\n",
        "    Date: str\n",
        "    Unit_price: str\n",
        "    Amount: float\n",
        "    Total: float\n",
        "    Email: str\n",
        "    Phone_number: str\n",
        "    Address: str\n",
        "\n",
        "# List of PDF file paths\n",
        "pdf_files = [\n",
        "    \"/content/invoice_1001329.pdf\",\n",
        "    \"/content/invoice_2001321.pdf\",\n",
        "    \"/content/invoice_3452334.pdf\",\n",
        "    # Add more file paths as needed\n",
        "]\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "df = pd.DataFrame(columns=[\n",
        "    'Invoice_no', 'Description', 'Quantity', 'Date', 'Unit_price',\n",
        "    'Amount', 'Total', 'Email', 'Phone_number', 'Address'\n",
        "])\n",
        "\n",
        "# Loop through PDF files\n",
        "for pdf_path in pdf_files:\n",
        "    try:\n",
        "        print(f\"Processing {pdf_path}...\")\n",
        "\n",
        "        # Extract text from PDF\n",
        "        text = \"\"\n",
        "        pdf_reader = PdfReader(pdf_path)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "\n",
        "        # Clean and prepare the text\n",
        "        text = text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "        # Construct the prompt\n",
        "        prompt = f\"\"\"Extract all the following values: invoice no., Description, Quantity, Date,\n",
        "        Unit price, Amount, Total, Email, Phone number, and Address from this data:\n",
        "\n",
        "        {text}\n",
        "\n",
        "        Expected output (JSON format):\n",
        "        {{\n",
        "            \"Invoice_no\": \"1001329\",\n",
        "            \"Description\": \"Office Chair\",\n",
        "            \"Quantity\": \"2\",\n",
        "            \"Date\": \"5/4/2023\",\n",
        "            \"Unit_price\": \"1100.00\",\n",
        "            \"Amount\": 2200.00,\n",
        "            \"Total\": 2200.00,\n",
        "            \"Email\": \"Santoshvarma0988@gmail.com\",\n",
        "            \"Phone_number\": \"9999999999\",\n",
        "            \"Address\": \"Mumbai, India\"\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        # Send prompt to LLM\n",
        "        llm_response = llm.invoke([HumanMessage(content=prompt)])\n",
        "        llm_content = llm_response.content\n",
        "\n",
        "        # Clean LLM response (removing markdown formatting)\n",
        "        cleaned_text = re.sub(r\"```json\\s*|\\s*```\", \"\", llm_content).strip()\n",
        "\n",
        "        # Validate and parse JSON response using Pydantic\n",
        "        invoice_data = InvoiceData.model_validate_json(cleaned_text)\n",
        "        print(\"Validated Data:\\n\", invoice_data)\n",
        "\n",
        "        # Convert Pydantic model to dict and append to DataFrame\n",
        "        new_data = pd.DataFrame([invoice_data.model_dump()])\n",
        "\n",
        "        # Append data safely\n",
        "        if df.empty:\n",
        "            df = new_data\n",
        "        else:\n",
        "            df = pd.concat([df, new_data], ignore_index=True)\n",
        "\n",
        "    except ValidationError as e:\n",
        "        print(f\"Validation Error in {pdf_path}:\\n\", e)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdf_path}: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = 'multi_invoice_summary.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"CSV file created as '{csv_filename}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF2xPZ7bMClu",
        "outputId": "35ac15cd-e377-41f7-d348-f25a9968ad62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/invoice_1001329.pdf...\n",
            "Validated Data:\n",
            " Invoice_no='1001329' Description='Office Chair' Quantity='2' Date='5/4/2023' Unit_price='1100.00' Amount=2200.0 Total=2200.0 Email='Santoshvarma0988@gmail.com' Phone_number='9999999999' Address='Mumbai, India'\n",
            "Processing /content/invoice_2001321.pdf...\n",
            "Validation Error in /content/invoice_2001321.pdf:\n",
            " 2 validation errors for InvoiceData\n",
            "Amount\n",
            "  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='$500.00', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing\n",
            "Total\n",
            "  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='$500.00', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing\n",
            "Processing /content/invoice_3452334.pdf...\n",
            "Validation Error in /content/invoice_3452334.pdf:\n",
            " 1 validation error for InvoiceData\n",
            "  Invalid JSON: expected value at line 1 column 1 [type=json_invalid, input_value='The provided text does *...r, and address details.', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.10/v/json_invalid\n",
            "CSV file created as 'multi_invoice_summary.csv'.\n"
          ]
        }
      ]
    }
  ]
}