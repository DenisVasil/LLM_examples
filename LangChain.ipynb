{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3i3JeZF+irYSyQIDl3sL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenisVasil/LLM_examples/blob/main/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Models (čata modeļi)"
      ],
      "metadata": {
        "id": "vBOJzQF4bGPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "id": "wa0lp07tR4Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjzm9ulcRYUu"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "result = model.invoke(\"What is 81 divided by 9\")\n",
        "\n",
        "print(\"Full result:\")\n",
        "print(result)\n",
        "print(\"Context:\")\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Conversation\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problem\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\")\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "\n",
        "print(f\"Answer from AI: {result.content}\")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Solve the following math problem\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
        "    AIMessage(content=f\"{result.content}\"),\n",
        "    HumanMessage(content=\"Divide this result by 3\")\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "print(f\"New answer from AI: {result.content}\")\n"
      ],
      "metadata": {
        "id": "1EtoGySeR2l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory.chat_memory import InMemoryChatMessageHistory\n",
        "\n",
        "history = InMemoryChatMessageHistory()\n",
        "history.add_message({\"role\": \"user\", \"content\": \"Hello\"})\n",
        "history.add_message({\"role\": \"assistant\", \"content\": \"Hi there!\"})\n",
        "\n",
        "print(history.messages)"
      ],
      "metadata": {
        "id": "hU4PBcJF2-2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema import HumanMessage, AIMessage # Import necessary classes\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "memory.chat_memory.add_message(HumanMessage(content=\"Hello\")) # Use HumanMessage\n",
        "memory.chat_memory.add_message(AIMessage(content=\"Hi there!\")) # Use AIMessage\n",
        "\n",
        "print(memory.load_memory_variables({}))"
      ],
      "metadata": {
        "id": "0hfMLbpa2glh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "system_message = SystemMessage(content=\"You are helpful AI assistant\")\n",
        "chat_history.append(system_message)\n",
        "\n",
        "# chat loop\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    chat_history.append(HumanMessage(content=query))\n",
        "\n",
        "    result = model.invoke(chat_history)\n",
        "    response = result.content\n",
        "    chat_history.append(AIMessage(content=response))\n",
        "    print(f\"AI response: {response}\")\n",
        "\n",
        "print(\"----- Message history ------\")\n",
        "print(chat_history)"
      ],
      "metadata": {
        "id": "tuJXOC96R27m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckdb"
      ],
      "metadata": {
        "id": "tv1IsvPpfhs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Chat History\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.memory.chat_memory import InMemoryChatMessageHistory\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "import duckdb\n",
        "import os\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Constants\n",
        "DB_PATH = \"chat_history.db\"\n",
        "TABLE_NAME = \"chat_history\"\n",
        "SESSION_ID = \"user_session_new\"  # This could be a username or unique ID\n",
        "\n",
        "# Initialize DuckDB\n",
        "con = duckdb.connect(DB_PATH)\n",
        "con.execute(f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
        "    session_id TEXT,\n",
        "    role TEXT,\n",
        "    content TEXT\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Helper Functions\n",
        "\n",
        "\n",
        "def save_message_to_db(session_id, role, content):\n",
        "    \"\"\"Save a single message to the DuckDB database.\"\"\"\n",
        "    con.execute(f\"\"\"\n",
        "    INSERT INTO {TABLE_NAME} (session_id, role, content)\n",
        "    VALUES (?, ?, ?)\n",
        "    \"\"\", [session_id, role, content])\n",
        "\n",
        "\n",
        "def load_messages_from_db(session_id):\n",
        "    \"\"\"Load chat history for a specific session from the DuckDB database.\"\"\"\n",
        "    result = con.execute(f\"\"\"\n",
        "    SELECT role, content FROM {TABLE_NAME} WHERE session_id = ? ORDER BY rowid\n",
        "    \"\"\", [session_id]).fetchall()\n",
        "    return [HumanMessage(content=row[1]) if row[0] == \"user\" else AIMessage(content=row[1]) for row in result]\n",
        "\n",
        "\n",
        "# Initialize Chat Message History\n",
        "print(\"Initializing DuckDB Chat Message History...\")\n",
        "messages = load_messages_from_db(SESSION_ID)\n",
        "chat_history = InMemoryChatMessageHistory(messages=messages)\n",
        "\n",
        "print(\"Chat History Initialized.\")\n",
        "print(\"Current Chat History:\", [msg.content for msg in chat_history.messages])\n",
        "\n",
        "\n",
        "print(\"Start chatting with the AI. Type 'exit' to quit.\")\n",
        "\n",
        "while True:\n",
        "    human_input = input(\"User: \")\n",
        "    if human_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Save and process user message\n",
        "    chat_history.add_user_message(human_input)\n",
        "    save_message_to_db(SESSION_ID, \"user\", human_input)\n",
        "\n",
        "    # Generate AI response\n",
        "    ai_response = model.invoke(chat_history.messages)\n",
        "    chat_history.add_ai_message(ai_response.content)\n",
        "    save_message_to_db(SESSION_ID, \"ai\", ai_response.content)\n",
        "\n",
        "    print(f\"AI: {ai_response.content}\")\n",
        "\n",
        "# Close DuckDB connection when done\n",
        "con.close()"
      ],
      "metadata": {
        "id": "n1LJKaPPdZrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph"
      ],
      "metadata": {
        "id": "DxoSX2WNgH0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from google.colab import userdata\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "query = \"Hi! I'm Bob.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
      ],
      "metadata": {
        "id": "Xpc-FZkMgdnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "r_ChzdzSkEUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from google.colab import userdata\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
        "\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    input_messages = [HumanMessage(query)]\n",
        "    output = app.invoke({\"messages\": input_messages}, config)\n",
        "    output[\"messages\"][-1].pretty_print()  # output contains all messages in state\n"
      ],
      "metadata": {
        "id": "J022__l1yw9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates (Veidnes Uzvednēm)"
      ],
      "metadata": {
        "id": "5VxAv0JigYas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"})"
      ],
      "metadata": {
        "id": "z4-8ZjukVdHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"user\", \"Tell me a joke about {topic}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"})\n",
        "# prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "# result = model.invoke(prompt)\n",
        "# print(result.content)"
      ],
      "metadata": {
        "id": "XvQ-HPWlXjrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    MessagesPlaceholder(\"msgs\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\"), HumanMessage(content=\"How are you?\")]})"
      ],
      "metadata": {
        "id": "Tvdq1nwUbzu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "template = \"Tell me a joke about {topic}\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"------Prompt from template------\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "print(\"\\n------Result-------\")\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "id": "9JnEN73kgSPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ChatPromptTemplate using a template string\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "template = \"Tell me a joke about {topic}.\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(\"-----Prompt from Template-----\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "g1g2XbEHg-vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt with Multiple Placeholders\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "n02VDO8vhSrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt with System and Human Messages (Using Tuples)\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
        "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "FtlIBKRbhnxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains (Ķēdes)"
      ],
      "metadata": {
        "id": "rcZW6V34iSTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"user\", \"Tell me a {adjective} joke\")],\n",
        ")\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke({\"adjective\": \"funny\"})"
      ],
      "metadata": {
        "id": "y1iSX6unU0an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Chain\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "# StrOutputParser() ~ .content()\n",
        "# LangChain expression language\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "result = chain.invoke({\"topic\": \"fish\", \"joke_count\": 3})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "Yv4Uq-suiVBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extended Chain\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
        "count_words = RunnableLambda(lambda x: f\"Word count {len(x.split())} \\n {x}\")\n",
        "\n",
        "chain = prompt_template | model | StrOutputParser() | uppercase_output | count_words\n",
        "\n",
        "result = chain.invoke({\"topic\": \"fish\", \"joke_count\": 3})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "tBkBF9oPjaVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parallel\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Define prompt template\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert product reviewer.\"),\n",
        "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Define pros analysis step\n",
        "def analyze_pros(features):\n",
        "    pros_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the pros of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return pros_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Define cons analysis step\n",
        "def analyze_cons(features):\n",
        "    cons_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer.\"),\n",
        "            (\n",
        "                \"human\",\n",
        "                \"Given these features: {features}, list the cons of these features.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    return cons_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "# Combine pros and cons into a final review\n",
        "def combine_pros_cons(pros, cons):\n",
        "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\"\n",
        "\n",
        "\n",
        "# Simplify branches with LCEL\n",
        "pros_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "cons_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the combined chain using LangChain Expression Language (LCEL)\n",
        "chain = (\n",
        "    prompt_template\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
        "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
        ")\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"product_name\": \"MacBook Pro\"})\n",
        "\n",
        "# Output\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "o1YDVByWj0vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Branching\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableBranch\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define prompt templates for different feedback types\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the feedback classification template\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the runnable branches for handling feedback\n",
        "branches = RunnableBranch(\n",
        "    (\n",
        "        lambda x: \"positive\" in x,\n",
        "        positive_feedback_template | model | StrOutputParser()  # Positive feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"negative\" in x,\n",
        "        negative_feedback_template | model | StrOutputParser()  # Negative feedback chain\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"neutral\" in x,\n",
        "        neutral_feedback_template | model | StrOutputParser()  # Neutral feedback chain\n",
        "    ),\n",
        "    escalate_feedback_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Create the classification chain\n",
        "classification_chain = classification_template | model | StrOutputParser()\n",
        "\n",
        "# Combine classification and response generation into one chain\n",
        "chain = classification_chain | branches\n",
        "\n",
        "# Run the chain with an example review\n",
        "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
        "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
        "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\"\n",
        "\n",
        "review = \"The product is okay. It works as expected but nothing exceptional.\"\n",
        "result = chain.invoke({\"feedback\": review})\n",
        "\n",
        "# Output the result\n",
        "print(result)"
      ],
      "metadata": {
        "id": "tQ2sjFYgkX--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Loaders (Dokumentu Ielādētāji)"
      ],
      "metadata": {
        "id": "IHVJMxYD54OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "qjtFqa7K6HkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "file_path = \"/content/diabetes.csv\"\n",
        "\n",
        "loader = CSVLoader(file_path=file_path)\n",
        "data = loader.load()\n",
        "\n",
        "for record in data[:2]:\n",
        "    #print(type(record))\n",
        "    print(record)"
      ],
      "metadata": {
        "id": "toKXmvoW52Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU pypdf"
      ],
      "metadata": {
        "id": "s2PH9ArP81cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/invoice_1001329.pdf\")\n",
        "pages = []\n",
        "\n",
        "async for page in loader.alazy_load():\n",
        "    pages.append(page)\n",
        "print(f\"{pages[0].metadata}\\n\")\n",
        "print(pages[0].page_content)"
      ],
      "metadata": {
        "id": "AxwhF9aR88xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents (Aģenti)"
      ],
      "metadata": {
        "id": "XomU3RqvlaS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU duckduckgo-search langchain-community"
      ],
      "metadata": {
        "id": "ox6JxQStvW8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "# Define the tool: DuckDuckGo search results\n",
        "ddg_search = DuckDuckGoSearchRun()\n",
        "\n",
        "# Set up the agent with the search tool\n",
        "tools = [Tool(\n",
        "    name=\"DuckDuckGoSearchRun\",\n",
        "    func=ddg_search.run,\n",
        "    description=\"Searches the web using DuckDuckGo.\"\n",
        ")]\n",
        "\n",
        "# Define a simple prompt template for asking a question\n",
        "prompt_template = \"Search the web for information about {query}\"\n",
        "\n",
        "# Initialize the agent with a basic agent type\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Use the agent to interact and fetch search results\n",
        "def get_search_results_using_agent(query: str):\n",
        "    query_formatted = prompt_template.format(query=query)\n",
        "    response = agent.run(query_formatted)\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "query = \"latest news on AI advancements\"\n",
        "search_results = get_search_results_using_agent(query)\n",
        "print(search_results)\n"
      ],
      "metadata": {
        "id": "_jf9DC-du94c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def load_data(query=None):  # Adding query parameter to make it compatible\n",
        "    # Load Iris dataset from sklearn\n",
        "    iris = load_iris()\n",
        "    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "    df['species'] = iris.target\n",
        "    return df\n",
        "\n",
        "# Function to clean data (simple imputation for missing values)\n",
        "def clean_data(df: pd.DataFrame, query=None):\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    df_clean = df.copy()\n",
        "    df_clean[:] = imputer.fit_transform(df)\n",
        "    return df_clean\n",
        "\n",
        "# Function to train a model (Random Forest)\n",
        "def train_model(df: pd.DataFrame, target_column: str, query=None):\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a Random Forest model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Set up tools for the agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Load Data\",\n",
        "        func=load_data,\n",
        "        description=\"Loads the Iris dataset.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Clean Data\",\n",
        "        func=clean_data,\n",
        "        description=\"Cleans the dataset by handling missing values.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Train Model\",\n",
        "        func=train_model,\n",
        "        description=\"Trains a Random Forest model and returns the accuracy.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Initialize the agent with tools and LLM\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Automating a data science task\n",
        "def automate_data_science_task():\n",
        "    # Step 1: Load data\n",
        "    print(\"Loading data...\")\n",
        "    df = load_data()\n",
        "\n",
        "    # Step 2: Clean data\n",
        "    print(\"Cleaning data...\")\n",
        "    df_clean = clean_data(df)\n",
        "\n",
        "    # Step 3: Train model\n",
        "    print(\"Training model...\")\n",
        "    accuracy = train_model(df_clean, target_column=\"species\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Run the agent\n",
        "accuracy = automate_data_science_task()\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "X207mzlgyxUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize Chat Model\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "\n",
        "# Create a tool\n",
        "@tool\n",
        "def super_function(input: int) -> int:\n",
        "    \"\"\"Applies a magic function to an input.\"\"\"\n",
        "    return input + 2\n",
        "\n",
        "\n",
        "tools = [super_function]\n",
        "\n",
        "\n",
        "query = \"what is the value of super_function(3)?\"\n",
        "\n",
        "langgraph_agent_executor = create_react_agent(model, tools)\n",
        "\n",
        "\n",
        "messages = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})\n",
        "{\n",
        "    \"input\": query,\n",
        "    \"output\": messages[\"messages\"][-1].content,\n",
        "}"
      ],
      "metadata": {
        "id": "mkPUmwHfI6X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools (Rīki)"
      ],
      "metadata": {
        "id": "78jpzxVBEnNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "# Let's inspect some of the attributes associated with the tool.\n",
        "print(multiply.name)\n",
        "print(multiply.description)\n",
        "print(multiply.args)"
      ],
      "metadata": {
        "id": "F1qMwQwuPoJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-experimental"
      ],
      "metadata": {
        "id": "_tKjxRFgEXHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import Tool\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "python_repl = PythonREPL()\n",
        "python_repl.run(\"print(1+1)\")\n"
      ],
      "metadata": {
        "id": "sxMXY1ozDLfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invoice extractor (Rēķinu izvilkšanas lietotne)"
      ],
      "metadata": {
        "id": "4IAJVycUd1wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "ccBVPQO8QooB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "pdf_doc = \"/content/invoice_1001329.pdf\"\n",
        "pdf_reader = PdfReader(pdf_doc)\n",
        "text = \"\"\n",
        "for page in pdf_reader.pages:\n",
        "    text += page.extract_text()\n",
        "print(text)"
      ],
      "metadata": {
        "id": "ZzN1tiXfR7IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from pydantic import BaseModel, ValidationError\n",
        "from pypdf import PdfReader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# Set up your Google API key\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"Google API key not found in userdata.\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Define a Pydantic model for invoice data validation\n",
        "class InvoiceData(BaseModel):\n",
        "    Invoice_no: str\n",
        "    Description: str\n",
        "    Quantity: str\n",
        "    Date: str\n",
        "    Unit_price: str\n",
        "    Amount: float\n",
        "    Total: float\n",
        "    Email: str\n",
        "    Phone_number: str\n",
        "    Address: str\n",
        "\n",
        "# PDF file path\n",
        "pdf_path = \"/content/invoice_1001329.pdf\"\n",
        "\n",
        "# Extract text from PDF\n",
        "print(f\"Processing {pdf_path}...\")\n",
        "text = \"\"\n",
        "pdf_reader = PdfReader(pdf_path)\n",
        "for page in pdf_reader.pages:\n",
        "    text += page.extract_text()\n",
        "\n",
        "# Clean and prepare the text\n",
        "text = text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "# Construct the prompt\n",
        "prompt = f\"\"\"Extract all the following values: invoice no., Description, Quantity, date,\n",
        "Unit price, Amount, Total, email, phone number, and address from this data:\n",
        "\n",
        "{text}\n",
        "\n",
        "Expected output (JSON format):\n",
        "{{\n",
        "    \"Invoice_no\": \"1001329\",\n",
        "    \"Description\": \"Office Chair\",\n",
        "    \"Quantity\": \"2\",\n",
        "    \"Date\": \"5/4/2023\",\n",
        "    \"Unit_price\": \"1100.00\",\n",
        "    \"Amount\": 2200.00,\n",
        "    \"Total\": 2200.00,\n",
        "    \"Email\": \"Santoshvarma0988@gmail.com\",\n",
        "    \"Phone_number\": \"9999999999\",\n",
        "    \"Address\": \"Mumbai, India\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# Debug: Print prompt to verify it\n",
        "print(\"Prompt sent to LLM:\\n\", prompt)\n",
        "\n",
        "# Initialize a DataFrame\n",
        "df = pd.DataFrame(columns=[\n",
        "    'Invoice_no', 'Description', 'Quantity', 'Date', 'Unit_price',\n",
        "    'Amount', 'Total', 'Email', 'Phone_number', 'Address'\n",
        "])\n",
        "\n",
        "# Generate response from LLM\n",
        "try:\n",
        "    llm_response = llm.invoke([HumanMessage(content=prompt)])  # Use `invoke()` instead of calling the model directly\n",
        "    llm_content = llm_response.content  # Access response content\n",
        "\n",
        "    # Clean response: Remove triple backticks and JSON keyword\n",
        "    cleaned_text = re.sub(r\"```json\\s*|\\s*```\", \"\", llm_content).strip()\n",
        "\n",
        "    # Validate and parse JSON response using Pydantic\n",
        "    invoice_data = InvoiceData.model_validate_json(cleaned_text)\n",
        "    print(\"Validated Data:\\n\", invoice_data)\n",
        "\n",
        "    # Convert Pydantic model to dict and append to DataFrame\n",
        "    new_data = pd.DataFrame([invoice_data.model_dump()])\n",
        "\n",
        "    # Append new data safely\n",
        "    if df.empty:\n",
        "        df = new_data\n",
        "    else:\n",
        "        df = pd.concat([df, new_data], ignore_index=True)\n",
        "\n",
        "except ValidationError as e:\n",
        "    print(\"Validation Error:\\n\", e)\n",
        "except Exception as e:\n",
        "    print(f\"Error generating LLM response: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = 'invoices_summary.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"CSV file created as '{csv_filename}'.\")\n"
      ],
      "metadata": {
        "id": "umD5hvBrLKhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from pydantic import BaseModel, field_validator\n",
        "from typing import Optional\n",
        "from pypdf import PdfReader\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "# Set up Google API key\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"Google API key not found in userdata.\")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Define a Pydantic model with relaxed validation\n",
        "class InvoiceData(BaseModel):\n",
        "    Invoice_no: str\n",
        "    Description: str\n",
        "    Quantity: str\n",
        "    Date: str\n",
        "    Unit_price: str\n",
        "    Amount: Optional[float] = None\n",
        "    Total: Optional[float] = None\n",
        "    Email: Optional[str] = None\n",
        "    Phone_number: Optional[str] = None\n",
        "    Address: Optional[str] = None\n",
        "\n",
        "    # Convert currency values to float\n",
        "    @field_validator(\"Amount\", \"Total\", mode=\"before\")\n",
        "    @classmethod\n",
        "    def parse_currency(cls, value):\n",
        "        if isinstance(value, str):\n",
        "            value = re.sub(r\"[^\\d.]\", \"\", value)  # Remove non-numeric characters\n",
        "        return float(value) if value else None\n",
        "\n",
        "# List of PDF file paths\n",
        "pdf_files = [\n",
        "    \"/content/invoice_1001329.pdf\",\n",
        "    \"/content/invoice_2001321.pdf\",\n",
        "    \"/content/invoice_3452334.pdf\",\n",
        "]\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "df = pd.DataFrame(columns=[\n",
        "    \"Invoice_no\", \"Description\", \"Quantity\", \"Date\", \"Unit_price\",\n",
        "    \"Amount\", \"Total\", \"Email\", \"Phone_number\", \"Address\"\n",
        "])\n",
        "\n",
        "# Loop through PDF files\n",
        "for pdf_path in pdf_files:\n",
        "    try:\n",
        "        print(f\"Processing {pdf_path}...\")\n",
        "\n",
        "        # Extract text from PDF\n",
        "        text = \"\"\n",
        "        pdf_reader = PdfReader(pdf_path)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() or \"\"  # Ensure it doesn't break if text extraction fails\n",
        "\n",
        "        text = text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "        # Construct the prompt\n",
        "        prompt = f\"\"\"Extract all the following values: invoice no., Description, Quantity, Date,\n",
        "        Unit price, Amount, Total, Email, Phone number, and Address from this data:\n",
        "\n",
        "        {text}\n",
        "\n",
        "        Expected output (JSON format):\n",
        "        {{\n",
        "            \"Invoice_no\": \"1001329\",\n",
        "            \"Description\": \"Office Chair\",\n",
        "            \"Quantity\": \"2\",\n",
        "            \"Date\": \"5/4/2023\",\n",
        "            \"Unit_price\": \"1100.00\",\n",
        "            \"Amount\": 2200.00,\n",
        "            \"Total\": 2200.00,\n",
        "            \"Email\": \"Santoshvarma0988@gmail.com\",\n",
        "            \"Phone_number\": \"9999999999\",\n",
        "            \"Address\": \"Mumbai, India\"\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        # Send prompt to LLM\n",
        "        llm_response = llm.invoke([HumanMessage(content=prompt)])\n",
        "        llm_content = llm_response.content\n",
        "\n",
        "        # Extract JSON using regex\n",
        "        json_match = re.search(r\"\\{.*\\}\", llm_content, re.DOTALL)\n",
        "        if not json_match:\n",
        "            print(f\"Error: Could not extract JSON from response for {pdf_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        cleaned_text = json_match.group(0)\n",
        "\n",
        "        # Ensure valid JSON format\n",
        "        try:\n",
        "            invoice_json = json.loads(cleaned_text)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error: LLM response is not valid JSON for {pdf_path}: {cleaned_text}\")\n",
        "            continue  # Skip this file\n",
        "\n",
        "        # Validate and parse JSON response using Pydantic\n",
        "        try:\n",
        "            invoice_data = InvoiceData(**invoice_json)\n",
        "        except Exception as e:\n",
        "            print(f\"Validation Error in {pdf_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        print(\"Validated Data:\\n\", invoice_data)\n",
        "\n",
        "        # Convert Pydantic model to dict and append to DataFrame\n",
        "        new_data = pd.DataFrame([invoice_data.model_dump()])\n",
        "        df = pd.concat([df, new_data], ignore_index=True)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {pdf_path}: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = \"multi_invoice_summary.csv\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(f\"CSV file created as '{csv_filename}'.\")\n",
        "print(f\"Total invoices processed: {len(df)}\")"
      ],
      "metadata": {
        "id": "IPOjhEiYO3hi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}