{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbnOqgP8A4GZjx2QC/14qK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenisVasil/LLM_examples/blob/main/Llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw_CNAC8xWxr"
      },
      "outputs": [],
      "source": [
        "! pip install llama-index-llms-gemini llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install llama-index-embeddings-gemini"
      ],
      "metadata": {
        "id": "4zxvU-7ytSLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install llama-index 'google-generativeai>=0.3.0' matplotlib"
      ],
      "metadata": {
        "id": "j_73lilRtVT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Models (čata modeļi)"
      ],
      "metadata": {
        "id": "s0i6-5mW20nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "resp = llm.complete(\"Write a poem about a magic backpack\")\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "N1jbAjFLxglL",
        "outputId": "433684ae-fc98-4dab-89f0-92988004b834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A backpack worn, of woven thread,\n",
            "Not leather tough, nor canvas spread,\n",
            "But shimmering silk, a mystic hue,\n",
            "With starlight stitched, and morning dew.\n",
            "\n",
            "No zippers found, no clasps to see,\n",
            "Just whispered words, a key to free\n",
            "Its hidden depths, a boundless space,\n",
            "Where time and tide find no embrace.\n",
            "\n",
            "A crumpled map, a dragon's scale,\n",
            "A phoenix feather, a whispered tale,\n",
            "A tiny vial, a moonbeam bright,\n",
            "All held within, in fading light.\n",
            "\n",
            "A wish unspoken, softly breathed,\n",
            "And from its folds, a treasure wreathed,\n",
            "A rainbow bridge, a silver stream,\n",
            "A sleeping giant, a waking dream.\n",
            "\n",
            "So carry it close, this magic hold,\n",
            "A story whispered, brave and bold,\n",
            "For in its depths, adventure sleeps,\n",
            "And every wonder, softly keeps.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
        "    ChatMessage(\n",
        "        role=\"user\", content=\"Help me decide what to have for dinner.\"\n",
        "    ),\n",
        "]\n",
        "resp = llm.chat(messages)\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "QxKeHp4Mz2Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "messages = []  # Initialize an empty list to store messages\n",
        "\n",
        "while True:\n",
        "    text_input = input(\"User: \")\n",
        "    if text_input == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Create a ChatMessage object for the user's input\n",
        "    user_message = ChatMessage(role=\"user\", content=text_input)\n",
        "    messages.append(user_message)  # Add user message to history\n",
        "\n",
        "    response = llm.chat(messages)  # Pass the message history to llm.chat\n",
        "    print(f\"Agent: {response}\")\n",
        "\n",
        "    # Add assistant's response to history\n",
        "    messages.append(ChatMessage(role=\"assistant\", content=response.message.content))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "20Uscqh1vJ7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "resp = llm.stream_complete(\n",
        "    \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n",
        ")\n",
        "\n",
        "for r in resp:\n",
        "    print(r.text, end=\"\")"
      ],
      "metadata": {
        "id": "cVN3IcXoz5z7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "1630ee1b-78e7-4102-ede5-384cd68be794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The story of Sourcrust, the bread creature, is really interesting. It all started when a mischievous baker, old Barnaby Buttercup, accidentally dropped a sourdough starter into a vat of enchanted, glowing goo he'd been brewing for a particularly ambitious rye bread.  The goo, a byproduct of his experiments with moonbeams and forgotten baking secrets, reacted violently.  It bubbled, hissed, and pulsed with an eerie, yeasty light before solidifying into a surprisingly humanoid form – Sourcrust.  He wasn't quite bread, not quite goo, but a bizarre, sentient amalgamation of both, with a crusty exterior, a surprisingly soft, doughy interior, and eyes that glowed with the same ethereal light as the enchanted goo.  He smelled faintly of rye and had a perpetually grumpy expression, hence the name.  Barnaby, initially terrified, quickly realized Sourcrust possessed a surprisingly gentle soul, albeit one with a penchant for mischief and a bottomless appetite for butter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "18NrXAxW3HaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# get API key and create embeddings\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "embeddings = embed_model.get_text_embedding(\"Google Gemini Embeddings.\")\n",
        "print(f\"Dimension of embeddings: {len(embeddings)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "596B0AbiS0rk",
        "outputId": "0f0b8ff0-de40-4d08-c764-a3147d944e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of embeddings: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering txt\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the first programs Paul Graham tried writing?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "B9M8JwM_bHvH",
        "outputId": "316d39b8-555e-4475-afba-54c445da418d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham's first programming attempts were on an IBM 1401, using an early version of Fortran.  These programs were written on punch cards, and their output was printed on a loud printer.  Due to limitations in input methods and his mathematical knowledge at the time, the programs were quite basic.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "167b1ada-2170-499d-8e56-f995ef3ebba2",
        "id": "5UnF1TEBjKTa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The design goals of AUTO GEN STUDIO center around three core objectives:  rapid prototyping, developer tooling, and reusable templates.  Rapid prototyping focuses on creating a development environment where developers can quickly define agent configurations and assemble them into multi-agent workflows.  Developer tooling aims to provide resources that help developers understand and debug agent behaviors, thereby improving multi-agent systems.  Finally, reusable templates offer a gallery of shareable templates to jumpstart agent workflow creation, establishing shared standards and best practices within the field.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persisting the RAG\n",
        "import os.path\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "# check if storage already exists\n",
        "PERSIST_DIR = \"./storage\"\n",
        "if not os.path.exists(PERSIST_DIR):\n",
        "    # load the documents and create the index\n",
        "    documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # store it for later\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "else:\n",
        "    # load the existing index\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d160daac-306d-4bee-8f52-f71c13059d2f",
        "id": "EKETBF9NlkLV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The design goals of AUTO GEN STUDIO center around three core objectives:  rapid prototyping, developer tooling, and reusable templates.  Rapid prototyping aims to create a development environment where developers can quickly define agent configurations and assemble them into multi-agent workflows.  Developer tooling focuses on providing resources to understand and debug agent behaviors, improving multi-agent systems.  Reusable templates offer a gallery of shareable templates to jumpstart agent workflow creation, establishing shared standards and best practices for wider adoption of multi-agent solutions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "iP4sIGr75evr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-vector-stores-chroma"
      ],
      "metadata": {
        "id": "UV0xqMdD-D0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vectorstore\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "import os.path\n",
        "\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "# load some documents\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "\n",
        "# initialize client, setting path to save data\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# create collection\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "\n",
        "# assign chroma as the vector_store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# create your index\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")\n",
        "\n",
        "# create a query engine and query\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the first programs Paul Graham tried writing?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "QPUO2huq56Vx",
        "outputId": "82a51e4d-38ff-4ccd-82d4-0ae2d203e988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author's first programming attempts were on an IBM 1401, using an early version of Fortran.  These programs were written on punch cards, and their output was printed on a loud printer.  Due to limitations in input methods and the author's mathematical knowledge at the time, the programs were quite basic and not particularly memorable.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query existng vectorstore\n",
        "\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "# initialize client\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# get collection\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "\n",
        "# assign chroma as the vector_store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# load your index from stored vectors\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store, storage_context=storage_context\n",
        ")\n",
        "\n",
        "# create a query engine\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Which programming languages did Paul use?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "yHOp83M-_bII",
        "outputId": "389732de-0bdc-4ef4-982d-d69c453bfdee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author used Lisp (specifically, Bel, a Lisp he created), and Arc.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the vectortore exists and retrieve the data\n",
        "\n",
        "import os\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the Google API key\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Set up Gemini embeddings and LLM\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "# Path to the ChromaDB store\n",
        "chroma_db_path = \"./chroma_db\"\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "db = chromadb.PersistentClient(path=chroma_db_path)\n",
        "\n",
        "# Get or create the collection\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "\n",
        "# Assign Chroma as the vector store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Check if vector store exists by looking for the ChromaDB path\n",
        "if not os.path.exists(chroma_db_path):\n",
        "    # Create vector store if it doesn't exist\n",
        "    print(\"Vector store not found. Creating a new one...\")\n",
        "    # Load documents from a directory\n",
        "    documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "\n",
        "    # Create a new index\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents, storage_context=storage_context\n",
        "    )\n",
        "else:\n",
        "    # Load the existing vector store\n",
        "    print(\"Vector store found. Loading from existing store...\")\n",
        "    index = VectorStoreIndex.from_vector_store(\n",
        "        vector_store, storage_context=storage_context\n",
        "    )\n",
        "\n",
        "# Create a query engine\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Query the index\n",
        "query = \"What are the first programs Paul Graham tried writing?\"\n",
        "response = query_engine.query(query)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "peagUr88BPUF",
        "outputId": "a7074482-fc1f-4bde-e739-45c3bda0cbab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store found. Loading from existing store...\n",
            "Paul Graham's first programming attempts were on an IBM 1401, using an early version of Fortran.  He wrote programs that didn't require input, such as calculating approximations of pi, because the only input option was data from punched cards, which he didn't have access to.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbots (čatboti)"
      ],
      "metadata": {
        "id": "qgXgnlvE3ODu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"best\", llm=llm, verbose=True)\n",
        "response = chat_engine.chat(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "amERfyUVzA_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"best\", llm=llm, verbose=True)\n",
        "\n",
        "while True:\n",
        "    text_input = input(\"User: \")\n",
        "    if text_input == \"exit\":\n",
        "        break\n",
        "    response = chat_engine.chat(text_input)\n",
        "    print(f\"Agent: {response}\")"
      ],
      "metadata": {
        "id": "-BW2Gfgh3kN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents (Aģenti)"
      ],
      "metadata": {
        "id": "wLzlhD7SDUU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "# initialize the tools\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "# initializing the agent\n",
        "\n",
        "agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\n",
        "\n",
        "response = agent.chat(\"What is 20+(2*4)? Use a tool to calculate every step.\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "IXagIK_5DTTC",
        "outputId": "c8e1dfc4-1ff4-4416-842b-66c5462ac163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step 67e12878-d88a-4bed-8e82-76d68ecb14de. Step input: What is 20+(2*4)? Use a tool to calculate every step.\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to calculate 2 * 4 first, then add the result to 20.\n",
            "Action: multiply\n",
            "Action Input: {'a': 2, 'b': 4}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 8\n",
            "\u001b[0m> Running step 7ea6a509-241b-4809-8581-92086dd62179. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I have the result of 2 * 4, which is 8. Now I need to add 20 and 8.\n",
            "Action: add\n",
            "Action Input: {'a': 20, 'b': 8}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 28\n",
            "\u001b[0m> Running step 0358f6c2-7318-40b3-ab44-9c50d40b4d23. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: 28\n",
            "\u001b[0m28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents with RAG (Aģenti ar RAG)"
      ],
      "metadata": {
        "id": "rQ5Hhl_i3OpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering txt\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "# get API key and create embeddings\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# settings\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "\n",
        "# function tools\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
        "    return a * b\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
        "    return a + b\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "# rag pipeline\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "budget_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine,\n",
        "    name=\"canadian_budget_2023\",\n",
        "    description=\"A RAG engine with some basic facts about the 2023 Canadian federal budget.\",\n",
        ")\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    [multiply_tool, add_tool, budget_tool], verbose=True\n",
        ")\n",
        "\n",
        "response = agent.chat(\n",
        "    \"What is the total amount of the 2023 Canadian federal budget multiplied by 3? Go step by step, using a tool to do any math.\"\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "8IELeGxuEJ0o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "d674225c-76fa-4f38-ad8c-21c2cc4d1620"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step 523e7d44-3bda-4f2b-b27d-b37e2eec502a. Step input: What is the total amount of the 2023 Canadian federal budget multiplied by 3? Go step by step, using a tool to do any math.\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use the `canadian_budget_2023` tool to get the budget amount, and then the `multiply` tool to multiply it by 3.\n",
            "Action: canadian_budget_2023\n",
            "Action Input: {'input': 'What is the total amount of the 2023 Canadian federal budget?'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: The projected total revenue is $456.8 billion, and the projected total expenditures are $496.9 billion, resulting in a projected deficit of $40.1 billion.\n",
            "\n",
            "\u001b[0m> Running step e2580489-9b8a-4aa8-92ef-03b3c480bfec. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I have the total expenditure amount.  I will use this number and the `multiply` tool to calculate the result.\n",
            "Action: multiply\n",
            "Action Input: {'a': 496.9, 'b': 3}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 1490.6999999999998\n",
            "\u001b[0m> Running step 99058dcc-1416-43fa-8fe2-c6e31da351bc. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: The total amount of the 2023 Canadian federal budget ($496.9 billion) multiplied by 3 is approximately $1,490.7 billion.\n",
            "\u001b[0mThe total amount of the 2023 Canadian federal budget ($496.9 billion) multiplied by 3 is approximately $1,490.7 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLamaParse"
      ],
      "metadata": {
        "id": "nhAOj2IrToMB"
      }
    },
    {
      "source": [
        "!pip install nest_asyncio"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAnJXZkfVfBH",
        "outputId": "5435afba-f098-4b21-c26f-59c0b4348514"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "# Install nest_asyncio to handle nested event loops\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# LLamaParse API key\n",
        "\n",
        "LLAMA_CLOUD_API_KEY = userdata.get('LLAMA_CLOUD_API_KEY')\n",
        "\n",
        "# get API key and create embeddings\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# settings\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "\n",
        "# function tools\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
        "    return a * b\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
        "    return a + b\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "# rag pipeline\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "budget_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine,\n",
        "    name=\"canadian_budget_2023\",\n",
        "    description=\"A RAG engine with some basic facts about the 2023 Canadian federal budget.\",\n",
        ")\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    [multiply_tool, add_tool, budget_tool], verbose=True\n",
        ")\n",
        "\n",
        "response = query_engine.query(\"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\")\n",
        "print(response)\n",
        "\n",
        "documents2 = LlamaParse(result_type=\"markdown\", api_key=LLAMA_CLOUD_API_KEY).load_data(\"/content/data/2023_canadian_budget.pdf\")\n",
        "index2 = VectorStoreIndex.from_documents(documents2)\n",
        "query_engine2 = index2.as_query_engine()\n",
        "\n",
        "response2 = query_engine2.query(\"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\")\n",
        "print(response2)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Vpki_XUzVfn2",
        "outputId": "14546e56-7098-4091-dfce-239dfcefaad6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twenty billion dollars was allocated to a new fifteen percent refundable tax credit to promote investment in green technologies.\n",
            "\n",
            "Started parsing the file under job_id a6be113c-7958-46e3-ba24-32950d7fefe6\n",
            "$20 billion was allocated to a new 15 percent refundable tax credit to promote investment in green technologies.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Memory (Aģentu atmiņa)"
      ],
      "metadata": {
        "id": "tL3BATWMWDZG"
      }
    },
    {
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "# get API key and create embeddings\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# settings\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "\n",
        "# function tools\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
        "    return a * b\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
        "    return a + b\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "# rag pipeline\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "budget_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine,\n",
        "    name=\"canadian_budget_2023\",\n",
        "    description=\"A RAG engine with some basic facts about the 2023 Canadian federal budget.\",\n",
        ")\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    [multiply_tool, add_tool, budget_tool], verbose=False\n",
        ")\n",
        "\n",
        "response = agent.chat(\"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\")\n",
        "\n",
        "print(response)\n",
        "\n",
        "response = agent.chat(\"How much was allocated to a implement a means-tested dental care program in the 2023 Canadian federal budget?\")\n",
        "\n",
        "print(response)\n",
        "\n",
        "response = agent.chat(\"How much was the total of those two allocations added together? Use a tool to answer any questions.\")\n",
        "\n",
        "# 20bn $ + 13bn $ = 33bn $\n",
        "print(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "14b8973c-f268-41d1-81cf-b623ab8bbceb",
        "id": "1QrsBjjNW5Vf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2023 Canadian federal budget allocated twenty billion dollars to a tax credit to promote investment in green technologies.\n",
            "$13 billion was allocated to implement a means-tested dental care program in the 2023 Canadian federal budget.\n",
            "The total allocation for both the green technology tax credit and the means-tested dental care program was $33 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LlamaHub"
      ],
      "metadata": {
        "id": "FUp0hbW9Y5Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-tools-yahoo-finance"
      ],
      "metadata": {
        "id": "q_RxJQqdY0-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core import Settings\n",
        "from llama_index.tools.yahoo_finance import YahooFinanceToolSpec\n",
        "\n",
        "# get API key and create embeddings\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# settings\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# function tools\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
        "    return a * b\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
        "    return a + b\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "finance_tools = YahooFinanceToolSpec().to_tool_list()\n",
        "finance_tools.extend([multiply_tool, add_tool])\n",
        "\n",
        "agent = ReActAgent.from_tools(finance_tools, verbose=True)\n",
        "\n",
        "response = agent.chat(\"What is the current price of NVDA?\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "ZRoMFAqEavW9",
        "outputId": "468123f0-f5d9-4967-fba2-809a434e1743"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step c030f98c-94e7-4149-9816-5fd89b61a1b4. Step input: What is the current price of NVDA?\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to get the current price of NVDA.\n",
            "Action: stock_basic_info\n",
            "Action Input: {'ticker': 'NVDA'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Info: \n",
            "{'address1': '2788 San Tomas Expressway', 'city': 'Santa Clara', 'state': 'CA', 'zip': '95051', 'country': 'United States', 'phone': '408 486 2000', 'website': 'https://www.nvidia.com', 'industry': 'Semiconductors', 'industryKey': 'semiconductors', 'industryDisp': 'Semiconductors', 'sector': 'Technology', 'sectorKey': 'technology', 'sectorDisp': 'Technology', 'longBusinessSummary': \"NVIDIA Corporation provides graphics and compute and networking solutions in the United States, Taiwan, China, Hong Kong, and internationally. The Graphics segment offers GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions for gaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU or vGPU software for cloud-based visual and virtual computing; automotive platforms for infotainment systems; and Omniverse software for building and operating metaverse and 3D internet applications. The Compute & Networking segment comprises Data Center computing platforms and end-to-end networking platforms, including Quantum for InfiniBand and Spectrum for Ethernet; NVIDIA DRIVE automated-driving platform and automotive development agreements; Jetson robotics and other embedded platforms; NVIDIA AI Enterprise and other software; and DGX Cloud software and services. The company's products are used in gaming, professional visualization, data center, and automotive markets. It sells its products to original equipment manufacturers, original device manufacturers, system integrators and distributors, independent software vendors, cloud service providers, consumer internet companies, add-in board manufacturers, distributors, automotive manufacturers and tier-1 automotive suppliers, and other ecosystem participants. NVIDIA Corporation was incorporated in 1993 and is headquartered in Santa Clara, California.\", 'fullTimeEmployees': 29600, 'companyOfficers': [{'maxAge': 1, 'name': 'Mr. Jen-Hsun  Huang', 'age': 60, 'title': 'Co-Founder, CEO, President & Director', 'yearBorn': 1963, 'fiscalYear': 2024, 'totalPay': 7491487, 'exercisedValue': 217327152, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Colette M. Kress', 'age': 56, 'title': 'Executive VP & CFO', 'yearBorn': 1967, 'fiscalYear': 2024, 'totalPay': 1510765, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Debora  Shoquist', 'age': 68, 'title': 'Executive Vice President of Operations', 'yearBorn': 1955, 'fiscalYear': 2024, 'totalPay': 1371266, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Timothy S. Teter J.D.', 'age': 56, 'title': 'Executive VP, General Counsel & Secretary', 'yearBorn': 1967, 'fiscalYear': 2024, 'totalPay': 1360939, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Ajay K. Puri', 'age': 68, 'title': 'Executive Vice President of Worldwide Field Operations', 'yearBorn': 1955, 'fiscalYear': 2024, 'totalPay': 2295097, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Chris A. Malachowsky', 'title': 'Co-Founder', 'fiscalYear': 2024, 'totalPay': 320000, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Donald F. Robertson Jr.', 'age': 54, 'title': 'VP & Chief Accounting Officer', 'yearBorn': 1969, 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Prof. William J. Dally Ph.D.', 'age': 62, 'title': 'Chief Scientist & Senior VP of Research', 'yearBorn': 1961, 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Mylene  Mangalindan', 'title': 'VP of Corporate Communications', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Tommy  Lee', 'title': 'Senior Vice President of Systems Engineering & Application', 'fiscalYear': 2024, 'exercisedValue': 0, 'unexercisedValue': 0}], 'auditRisk': 7, 'boardRisk': 10, 'compensationRisk': 5, 'shareHolderRightsRisk': 6, 'overallRisk': 8, 'governanceEpochDate': 1730419200, 'compensationAsOfEpochDate': 1735603200, 'irWebsite': 'http://phx.corporate-ir.net/phoenix.zhtml?c=116466&p=irol-IRHome', 'maxAge': 86400, 'priceHint': 2, 'previousClose': 130.68, 'open': 129.81, 'dayLow': 128.22, 'dayHigh': 135.28, 'regularMarketPreviousClose': 130.68, 'regularMarketOpen': 129.81, 'regularMarketDayLow': 128.22, 'regularMarketDayHigh': 135.28, 'dividendRate': 0.04, 'dividendYield': 0.00029999999, 'exDividendDate': 1726099200, 'payoutRatio': 0.0111, 'fiveYearAvgDividendYield': 0.09, 'beta': 1.657, 'trailingPE': 53.241108, 'forwardPE': 30.40028, 'volume': 306528553, 'regularMarketVolume': 306528553, 'averageVolume': 234268879, 'averageVolume10days': 226469350, 'averageDailyVolume10Day': 226469350, 'bid': 131.5, 'ask': 136.0, 'bidSize': 200, 'askSize': 100, 'marketCap': 3298803056640, 'fiftyTwoWeekLow': 47.32, 'fiftyTwoWeekHigh': 152.89, 'priceToSalesTrailing12Months': 34.25299, 'fiftyDayAverage': 139.6206, 'twoHundredDayAverage': 116.38052, 'currency': 'USD', 'enterpriseValue': 3553896693760, 'profitMargins': 0.55041003, 'floatShares': 23541441000, 'sharesOutstanding': 24490000384, 'sharesShort': 246400791, 'sharesShortPriorMonth': 251090067, 'sharesShortPreviousMonthDate': 1727654400, 'dateShortInterest': 1730332800, 'sharesPercentSharesOut': 0.01, 'heldPercentInsiders': 0.0429, 'heldPercentInstitutions': 0.6617, 'shortRatio': 1.01, 'shortPercentOfFloat': 0.0105, 'impliedSharesOutstanding': 25413099520, 'bookValue': 1.744, 'priceToBook': 77.23624, 'lastFiscalYearEnd': 1706400000, 'nextFiscalYearEnd': 1738022400, 'mostRecentQuarter': 1722124800, 'earningsQuarterlyGrowth': 1.682, 'netIncomeToCommon': 53007998976, 'trailingEps': 2.53, 'forwardEps': 4.12, 'lastSplitFactor': '10:1', 'lastSplitDate': 1717977600, 'enterpriseToRevenue': 36.902, 'enterpriseToEbitda': 58.085, '52WeekChange': 1.7334158, 'SandP52WeekChange': 0.2421279, 'lastDividendValue': 0.01, 'lastDividendDate': 1733356800, 'exchange': 'NMS', 'quoteType': 'EQUITY', 'symbol': 'NVDA', 'underlyingSymbol': 'NVDA', 'shortName': 'NVIDIA Corporation', 'longName': 'NVIDIA Corporation', 'firstTradeDateEpochUtc': 917015400, 'timeZoneFullName': 'America/New_York', 'timeZoneShortName': 'EST', 'uuid': '7f5f6a07-b148-30f4-98a2-2caa3df2aed0', 'messageBoardId': 'finmb_32307', 'gmtOffSetMilliseconds': -18000000, 'currentPrice': 134.7, 'targetHighPrice': 220.0, 'targetLowPrice': 130.0, 'targetMeanPrice': 172.80222, 'targetMedianPrice': 175.0, 'recommendationMean': 1.3125, 'recommendationKey': 'strong_buy', 'numberOfAnalystOpinions': 54, 'totalCash': 34800001024, 'totalCashPerShare': 1.419, 'ebitda': 61184000000, 'totalDebt': 10014999552, 'quickRatio': 3.503, 'currentRatio': 4.269, 'totalRevenue': 96307003392, 'debtToEquity': 17.221, 'revenuePerShare': 3.91, 'returnOnAssets': 0.55258, 'returnOnEquity': 1.23767, 'freeCashflow': 33725874176, 'operatingCashflow': 48663998464, 'earningsGrowth': 1.68, 'revenueGrowth': 1.224, 'grossMargins': 0.75975996, 'ebitdaMargins': 0.6353, 'operatingMargins': 0.62057, 'financialCurrency': 'USD', 'trailingPegRatio': 0.9654}\n",
            "\u001b[0m> Running step b71a0b10-5075-4897-af74-d224e6a0d8f2. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: The current price of NVDA is $134.70.\n",
            "\u001b[0mThe current price of NVDA is $134.70.\n"
          ]
        }
      ]
    }
  ]
}