{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqJfDLhS6jzvAqAGDAikdd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenisVasil/LLM_examples/blob/main/Llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw_CNAC8xWxr"
      },
      "outputs": [],
      "source": [
        "! pip install llama-index-llms-gemini llama-index\n",
        "! pip install llama-index-embeddings-gemini\n",
        "! pip install llama-index 'google-generativeai>=0.3.0' matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Models (čata modeļi)"
      ],
      "metadata": {
        "id": "s0i6-5mW20nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "resp = llm.complete(\"Write a poem about a magic backpack\")\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "N1jbAjFLxglL",
        "outputId": "433684ae-fc98-4dab-89f0-92988004b834"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A backpack worn, of woven thread,\n",
            "Not leather tough, nor canvas spread,\n",
            "But shimmering silk, a mystic hue,\n",
            "With starlight stitched, and morning dew.\n",
            "\n",
            "No zippers found, no clasps to see,\n",
            "Just whispered words, a key to free\n",
            "Its hidden depths, a boundless space,\n",
            "Where time and tide find no embrace.\n",
            "\n",
            "A crumpled map, a dragon's scale,\n",
            "A phoenix feather, a whispered tale,\n",
            "A tiny vial, a moonbeam bright,\n",
            "All held within, in fading light.\n",
            "\n",
            "A wish unspoken, softly breathed,\n",
            "And from its folds, a treasure wreathed,\n",
            "A rainbow bridge, a silver stream,\n",
            "A sleeping giant, a waking dream.\n",
            "\n",
            "So carry it close, this magic hold,\n",
            "A story whispered, brave and bold,\n",
            "For in its depths, adventure sleeps,\n",
            "And every wonder, softly keeps.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
        "    ChatMessage(\n",
        "        role=\"user\", content=\"Help me decide what to have for dinner.\"\n",
        "    ),\n",
        "]\n",
        "resp = llm.chat(messages)\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "QxKeHp4Mz2Wl",
        "outputId": "d2036d7c-85e0-4b4d-e74b-d83d7f929916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Okay, let's figure out what you're in the mood for! To help me narrow it down, tell me:\n",
            "\n",
            "* **What kind of cuisine are you craving?** (e.g., Italian, Mexican, Asian, American, etc.)\n",
            "* **What ingredients do you have on hand?** (This will help avoid a trip to the store if possible.)\n",
            "* **How much time do you have to cook?** (Quick and easy, or something more elaborate?)\n",
            "* **What's your skill level in the kitchen?** (Beginner, intermediate, advanced?)\n",
            "* **Any dietary restrictions or preferences?** (Vegetarian, vegan, gluten-free, etc.)\n",
            "\n",
            "\n",
            "Once I have this information, I can give you some personalized suggestions!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resp = llm.stream_complete(\n",
        "    \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n",
        ")"
      ],
      "metadata": {
        "id": "cVN3IcXoz5z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in resp:\n",
        "    print(r.text, end=\"\")"
      ],
      "metadata": {
        "id": "RKU5QU430NO9",
        "outputId": "a91ab33e-2220-43d2-c51b-9721690e3132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The story of Sourcrust, the bread creature, is really interesting. It all started when a mischievous baker, old Barnaby Buttercup, accidentally dropped a sourdough starter into a vat of enchanted, glowing goo he'd been brewing for a particularly ambitious rye bread.  The goo, a byproduct of his experiments with moonbeams and forgotten baking secrets, reacted violently.  It bubbled, hissed, and pulsed with an eerie, yeasty light before solidifying into a surprisingly humanoid form – a creature made entirely of sourdough bread, with crusty skin, a doughy body, and eyes that shimmered like caramelized sugar.  Barnaby, initially terrified, soon discovered that Sourcrust wasn't malicious, but rather possessed a surprisingly gentle nature and an insatiable appetite for butter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "18NrXAxW3HaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# get API key and create embeddings\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "embeddings = embed_model.get_text_embedding(\"Google Gemini Embeddings.\")\n",
        "print(f\"Dimension of embeddings: {len(embeddings)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "596B0AbiS0rk",
        "outputId": "0f0b8ff0-de40-4d08-c764-a3147d944e34"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of embeddings: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering txt\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the first programs Paul Graham tried writing?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "B9M8JwM_bHvH",
        "outputId": "316d39b8-555e-4475-afba-54c445da418d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham's first programming attempts were on an IBM 1401, using an early version of Fortran.  These programs were written on punch cards, and their output was printed on a loud printer.  Due to limitations in input methods and his mathematical knowledge at the time, the programs were quite basic.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uBMJPe4sjJ6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "167b1ada-2170-499d-8e56-f995ef3ebba2",
        "id": "5UnF1TEBjKTa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The design goals of AUTO GEN STUDIO center around three core objectives:  rapid prototyping, developer tooling, and reusable templates.  Rapid prototyping focuses on creating a development environment where developers can quickly define agent configurations and assemble them into multi-agent workflows.  Developer tooling aims to provide resources that help developers understand and debug agent behaviors, thereby improving multi-agent systems.  Finally, reusable templates offer a gallery of shareable templates to jumpstart agent workflow creation, establishing shared standards and best practices within the field.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persisting the RAG\n",
        "import os.path\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "# check if storage already exists\n",
        "PERSIST_DIR = \"./storage\"\n",
        "if not os.path.exists(PERSIST_DIR):\n",
        "    # load the documents and create the index\n",
        "    documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # store it for later\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "else:\n",
        "    # load the existing index\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d160daac-306d-4bee-8f52-f71c13059d2f",
        "id": "EKETBF9NlkLV"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The design goals of AUTO GEN STUDIO center around three core objectives:  rapid prototyping, developer tooling, and reusable templates.  Rapid prototyping aims to create a development environment where developers can quickly define agent configurations and assemble them into multi-agent workflows.  Developer tooling focuses on providing resources to understand and debug agent behaviors, improving multi-agent systems.  Reusable templates offer a gallery of shareable templates to jumpstart agent workflow creation, establishing shared standards and best practices for wider adoption of multi-agent solutions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbots (čatboti)"
      ],
      "metadata": {
        "id": "qgXgnlvE3ODu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"best\", llm=llm, verbose=True)\n",
        "response = chat_engine.chat(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "amERfyUVzA_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"best\", llm=llm, verbose=True)\n",
        "\n",
        "while True:\n",
        "    text_input = input(\"User: \")\n",
        "    if text_input == \"exit\":\n",
        "        break\n",
        "    response = chat_engine.chat(text_input)\n",
        "    print(f\"Agent: {response}\")"
      ],
      "metadata": {
        "id": "-BW2Gfgh3kN-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}