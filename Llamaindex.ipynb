{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPumMTg701uv86Sh8RaKa4W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenisVasil/LLM_examples/blob/main/Llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw_CNAC8xWxr"
      },
      "outputs": [],
      "source": [
        "! pip install llama-index-llms-gemini llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install llama-index-embeddings-gemini"
      ],
      "metadata": {
        "id": "4zxvU-7ytSLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install llama-index 'google-generativeai>=0.3.0' matplotlib"
      ],
      "metadata": {
        "id": "j_73lilRtVT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Models (čata modeļi)"
      ],
      "metadata": {
        "id": "s0i6-5mW20nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "resp = llm.complete(\"Write a poem about a magic backpack\")\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "N1jbAjFLxglL",
        "outputId": "433684ae-fc98-4dab-89f0-92988004b834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A backpack worn, of woven thread,\n",
            "Not leather tough, nor canvas spread,\n",
            "But shimmering silk, a mystic hue,\n",
            "With starlight stitched, and morning dew.\n",
            "\n",
            "No zippers found, no clasps to see,\n",
            "Just whispered words, a key to free\n",
            "Its hidden depths, a boundless space,\n",
            "Where time and tide find no embrace.\n",
            "\n",
            "A crumpled map, a dragon's scale,\n",
            "A phoenix feather, a whispered tale,\n",
            "A tiny vial, a moonbeam bright,\n",
            "All held within, in fading light.\n",
            "\n",
            "A wish unspoken, softly breathed,\n",
            "And from its folds, a treasure wreathed,\n",
            "A rainbow bridge, a silver stream,\n",
            "A sleeping giant, a waking dream.\n",
            "\n",
            "So carry it close, this magic hold,\n",
            "A story whispered, brave and bold,\n",
            "For in its depths, adventure sleeps,\n",
            "And every wonder, softly keeps.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
        "    ChatMessage(\n",
        "        role=\"user\", content=\"Help me decide what to have for dinner.\"\n",
        "    ),\n",
        "]\n",
        "resp = llm.chat(messages)\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "QxKeHp4Mz2Wl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "90be75f4-1c85-4b22-8fa8-39d11a836750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Okay, let's figure out what you're in the mood for! To help me narrow it down, tell me:\n",
            "\n",
            "* **What kind of cuisine are you craving?** (e.g., Italian, Mexican, Asian, American, etc.)\n",
            "* **What ingredients do you have on hand?** (This will help avoid a trip to the store if possible.)\n",
            "* **How much time do you have to cook?** (Quick and easy, or something more elaborate?)\n",
            "* **What's your skill level in the kitchen?** (Beginner, intermediate, expert?)\n",
            "* **Any dietary restrictions or preferences?** (Vegetarian, vegan, gluten-free, etc.)\n",
            "\n",
            "\n",
            "Once I have this information, I can give you some personalized suggestions!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "messages = []  # Initialize an empty list to store messages\n",
        "\n",
        "while True:\n",
        "    text_input = input(\"User: \")\n",
        "    if text_input == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Create a ChatMessage object for the user's input\n",
        "    user_message = ChatMessage(role=\"user\", content=text_input)\n",
        "    messages.append(user_message)  # Add user message to history\n",
        "\n",
        "    response = llm.chat(messages)  # Pass the message history to llm.chat\n",
        "    print(f\"Agent: {response}\")\n",
        "\n",
        "    # Add assistant's response to history\n",
        "    messages.append(ChatMessage(role=\"assistant\", content=response.message.content))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "20Uscqh1vJ7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "resp = llm.stream_complete(\n",
        "    \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n",
        ")\n",
        "\n",
        "for r in resp:\n",
        "    print(r.text, end=\"\")"
      ],
      "metadata": {
        "id": "cVN3IcXoz5z7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "1630ee1b-78e7-4102-ede5-384cd68be794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The story of Sourcrust, the bread creature, is really interesting. It all started when a mischievous baker, old Barnaby Buttercup, accidentally dropped a sourdough starter into a vat of enchanted, glowing goo he'd been brewing for a particularly ambitious rye bread.  The goo, a byproduct of his experiments with moonbeams and forgotten baking secrets, reacted violently.  It bubbled, hissed, and pulsed with an eerie, yeasty light before solidifying into a surprisingly humanoid form – Sourcrust.  He wasn't quite bread, not quite goo, but a bizarre, sentient amalgamation of both, with a crusty exterior, a surprisingly soft, doughy interior, and eyes that glowed with the same ethereal light as the enchanted goo.  He smelled faintly of rye and had a perpetually grumpy expression, hence the name.  Barnaby, initially terrified, quickly realized Sourcrust possessed a surprisingly gentle soul, albeit one with a penchant for mischief and a bottomless appetite for butter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates (Veidnes Uzvednēm)"
      ],
      "metadata": {
        "id": "wqQUcegfI9KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.prompts import Prompt\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = Prompt(template=\"\"\"\n",
        "You are a helpful assistant.\n",
        "Tell me a joke about {topic}.\n",
        "\"\"\")\n",
        "\n",
        "# Render the prompt with the input variable\n",
        "topic = \"cats\"\n",
        "rendered_prompt = prompt_template.format(topic=topic)\n",
        "\n",
        "# Print the rendered prompt\n",
        "print(rendered_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAVYyWbFEqJ6",
        "outputId": "db40f5c6-5e51-4216-85f6-b308fb7b24b6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a helpful assistant.\n",
            "Tell me a joke about cats.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.prompts import Prompt\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "# Define the prompt template\n",
        "prompt_template = Prompt(template=\"\"\"\n",
        "You are a helpful assistant.\n",
        "Tell me a joke about {topic}.\n",
        "\"\"\")\n",
        "\n",
        "# Render the prompt with the input variable\n",
        "topic = \"cats\"\n",
        "rendered_prompt = prompt_template.format(topic=topic)\n",
        "result = llm.complete(rendered_prompt)\n",
        "\n",
        "# Print the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "VZY04kwvFT05",
        "outputId": "9313e63c-c364-4752-8887-381d16b1fab9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why are cats such bad dancers?  Because they have two left feet... and two left paws!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "18NrXAxW3HaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# get API key and create embeddings\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "embeddings = embed_model.get_text_embedding(\"Google Gemini Embeddings.\")\n",
        "print(f\"Dimension of embeddings: {len(embeddings)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "596B0AbiS0rk",
        "outputId": "0f0b8ff0-de40-4d08-c764-a3147d944e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of embeddings: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering txt\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the first programs Paul Graham tried writing?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "B9M8JwM_bHvH",
        "outputId": "316d39b8-555e-4475-afba-54c445da418d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham's first programming attempts were on an IBM 1401, using an early version of Fortran.  These programs were written on punch cards, and their output was printed on a loud printer.  Due to limitations in input methods and his mathematical knowledge at the time, the programs were quite basic.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "167b1ada-2170-499d-8e56-f995ef3ebba2",
        "id": "5UnF1TEBjKTa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The design goals of AUTO GEN STUDIO center around three core objectives:  rapid prototyping, developer tooling, and reusable templates.  Rapid prototyping focuses on creating a development environment where developers can quickly define agent configurations and assemble them into multi-agent workflows.  Developer tooling aims to provide resources that help developers understand and debug agent behaviors, thereby improving multi-agent systems.  Finally, reusable templates offer a gallery of shareable templates to jumpstart agent workflow creation, establishing shared standards and best practices within the field.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persisting the RAG\n",
        "import os.path\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "# check if storage already exists\n",
        "PERSIST_DIR = \"./storage\"\n",
        "if not os.path.exists(PERSIST_DIR):\n",
        "    # load the documents and create the index\n",
        "    documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # store it for later\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "else:\n",
        "    # load the existing index\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d160daac-306d-4bee-8f52-f71c13059d2f",
        "id": "EKETBF9NlkLV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The design goals of AUTO GEN STUDIO center around three core objectives:  rapid prototyping, developer tooling, and reusable templates.  Rapid prototyping aims to create a development environment where developers can quickly define agent configurations and assemble them into multi-agent workflows.  Developer tooling focuses on providing resources to understand and debug agent behaviors, improving multi-agent systems.  Reusable templates offer a gallery of shareable templates to jumpstart agent workflow creation, establishing shared standards and best practices for wider adoption of multi-agent solutions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "iP4sIGr75evr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-vector-stores-chroma"
      ],
      "metadata": {
        "id": "UV0xqMdD-D0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vectorstore\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "import os.path\n",
        "\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "# load some documents\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "\n",
        "# initialize client, setting path to save data\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# create collection\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "\n",
        "# assign chroma as the vector_store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# create your index\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")\n",
        "\n",
        "# create a query engine and query\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the first programs Paul Graham tried writing?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "QPUO2huq56Vx",
        "outputId": "82a51e4d-38ff-4ccd-82d4-0ae2d203e988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author's first programming attempts were on an IBM 1401, using an early version of Fortran.  These programs were written on punch cards, and their output was printed on a loud printer.  Due to limitations in input methods and the author's mathematical knowledge at the time, the programs were quite basic and not particularly memorable.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query existng vectorstore\n",
        "\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "# initialize client\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# get collection\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "\n",
        "# assign chroma as the vector_store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# load your index from stored vectors\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store, storage_context=storage_context\n",
        ")\n",
        "\n",
        "# create a query engine\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Which programming languages did Paul use?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "yHOp83M-_bII",
        "outputId": "389732de-0bdc-4ef4-982d-d69c453bfdee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author used Lisp (specifically, Bel, a Lisp he created), and Arc.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the vectortore exists and retrieve the data\n",
        "\n",
        "import os\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the Google API key\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Set up Gemini embeddings and LLM\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "# Path to the ChromaDB store\n",
        "chroma_db_path = \"./chroma_db\"\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "db = chromadb.PersistentClient(path=chroma_db_path)\n",
        "\n",
        "# Get or create the collection\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "\n",
        "# Assign Chroma as the vector store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Check if vector store exists by looking for the ChromaDB path\n",
        "if not os.path.exists(chroma_db_path):\n",
        "    # Create vector store if it doesn't exist\n",
        "    print(\"Vector store not found. Creating a new one...\")\n",
        "    # Load documents from a directory\n",
        "    documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "\n",
        "    # Create a new index\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents, storage_context=storage_context\n",
        "    )\n",
        "else:\n",
        "    # Load the existing vector store\n",
        "    print(\"Vector store found. Loading from existing store...\")\n",
        "    index = VectorStoreIndex.from_vector_store(\n",
        "        vector_store, storage_context=storage_context\n",
        "    )\n",
        "\n",
        "# Create a query engine\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Query the index\n",
        "query = \"What are the first programs Paul Graham tried writing?\"\n",
        "response = query_engine.query(query)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "peagUr88BPUF",
        "outputId": "a7074482-fc1f-4bde-e739-45c3bda0cbab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store found. Loading from existing store...\n",
            "Paul Graham's first programming attempts were on an IBM 1401, using an early version of Fortran.  He wrote programs that didn't require input, such as calculating approximations of pi, because the only input option was data from punched cards, which he didn't have access to.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbots (čatboti)"
      ],
      "metadata": {
        "id": "qgXgnlvE3ODu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"best\", llm=llm, verbose=True)\n",
        "response = chat_engine.chat(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "amERfyUVzA_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"best\", llm=llm, verbose=True)\n",
        "\n",
        "while True:\n",
        "    text_input = input(\"User: \")\n",
        "    if text_input == \"exit\":\n",
        "        break\n",
        "    response = chat_engine.chat(text_input)\n",
        "    print(f\"Agent: {response}\")"
      ],
      "metadata": {
        "id": "-BW2Gfgh3kN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents (Aģenti)"
      ],
      "metadata": {
        "id": "wLzlhD7SDUU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "# initialize the tools\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "# initializing the agent\n",
        "\n",
        "agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\n",
        "\n",
        "response = agent.chat(\"What is 20+(2*4)? Use a tool to calculate every step.\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "IXagIK_5DTTC",
        "outputId": "c8e1dfc4-1ff4-4416-842b-66c5462ac163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step 67e12878-d88a-4bed-8e82-76d68ecb14de. Step input: What is 20+(2*4)? Use a tool to calculate every step.\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to calculate 2 * 4 first, then add the result to 20.\n",
            "Action: multiply\n",
            "Action Input: {'a': 2, 'b': 4}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 8\n",
            "\u001b[0m> Running step 7ea6a509-241b-4809-8581-92086dd62179. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I have the result of 2 * 4, which is 8. Now I need to add 20 and 8.\n",
            "Action: add\n",
            "Action Input: {'a': 20, 'b': 8}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 28\n",
            "\u001b[0m> Running step 0358f6c2-7318-40b3-ab44-9c50d40b4d23. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: 28\n",
            "\u001b[0m28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents with RAG (Aģenti ar RAG)"
      ],
      "metadata": {
        "id": "rQ5Hhl_i3OpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering txt\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "# get API key and create embeddings\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# settings\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "\n",
        "# function tools\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
        "    return a * b\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
        "    return a + b\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "# rag pipeline\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "budget_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine,\n",
        "    name=\"canadian_budget_2023\",\n",
        "    description=\"A RAG engine with some basic facts about the 2023 Canadian federal budget.\",\n",
        ")\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    [multiply_tool, add_tool, budget_tool], verbose=True\n",
        ")\n",
        "\n",
        "response = agent.chat(\n",
        "    \"What is the total amount of the 2023 Canadian federal budget multiplied by 3? Go step by step, using a tool to do any math.\"\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "8IELeGxuEJ0o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "d674225c-76fa-4f38-ad8c-21c2cc4d1620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step 523e7d44-3bda-4f2b-b27d-b37e2eec502a. Step input: What is the total amount of the 2023 Canadian federal budget multiplied by 3? Go step by step, using a tool to do any math.\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use the `canadian_budget_2023` tool to get the budget amount, and then the `multiply` tool to multiply it by 3.\n",
            "Action: canadian_budget_2023\n",
            "Action Input: {'input': 'What is the total amount of the 2023 Canadian federal budget?'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: The projected total revenue is $456.8 billion, and the projected total expenditures are $496.9 billion, resulting in a projected deficit of $40.1 billion.\n",
            "\n",
            "\u001b[0m> Running step e2580489-9b8a-4aa8-92ef-03b3c480bfec. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I have the total expenditure amount.  I will use this number and the `multiply` tool to calculate the result.\n",
            "Action: multiply\n",
            "Action Input: {'a': 496.9, 'b': 3}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 1490.6999999999998\n",
            "\u001b[0m> Running step 99058dcc-1416-43fa-8fe2-c6e31da351bc. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: The total amount of the 2023 Canadian federal budget ($496.9 billion) multiplied by 3 is approximately $1,490.7 billion.\n",
            "\u001b[0mThe total amount of the 2023 Canadian federal budget ($496.9 billion) multiplied by 3 is approximately $1,490.7 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLamaParse"
      ],
      "metadata": {
        "id": "nhAOj2IrToMB"
      }
    },
    {
      "source": [
        "!pip install nest_asyncio"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAnJXZkfVfBH",
        "outputId": "5435afba-f098-4b21-c26f-59c0b4348514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "# Install nest_asyncio to handle nested event loops\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# LLamaParse API key\n",
        "\n",
        "LLAMA_CLOUD_API_KEY = userdata.get('LLAMA_CLOUD_API_KEY')\n",
        "\n",
        "# get API key and create embeddings\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# settings\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "\n",
        "# function tools\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
        "    return a * b\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
        "    return a + b\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "# rag pipeline\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "budget_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine,\n",
        "    name=\"canadian_budget_2023\",\n",
        "    description=\"A RAG engine with some basic facts about the 2023 Canadian federal budget.\",\n",
        ")\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    [multiply_tool, add_tool, budget_tool], verbose=True\n",
        ")\n",
        "\n",
        "response = query_engine.query(\"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\")\n",
        "print(response)\n",
        "\n",
        "documents2 = LlamaParse(result_type=\"markdown\", api_key=LLAMA_CLOUD_API_KEY).load_data(\"/content/data/2023_canadian_budget.pdf\")\n",
        "index2 = VectorStoreIndex.from_documents(documents2)\n",
        "query_engine2 = index2.as_query_engine()\n",
        "\n",
        "response2 = query_engine2.query(\"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\")\n",
        "print(response2)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Vpki_XUzVfn2",
        "outputId": "14546e56-7098-4091-dfce-239dfcefaad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twenty billion dollars was allocated to a new fifteen percent refundable tax credit to promote investment in green technologies.\n",
            "\n",
            "Started parsing the file under job_id a6be113c-7958-46e3-ba24-32950d7fefe6\n",
            "$20 billion was allocated to a new 15 percent refundable tax credit to promote investment in green technologies.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Memory (Aģentu atmiņa)"
      ],
      "metadata": {
        "id": "tL3BATWMWDZG"
      }
    },
    {
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "# get API key and create embeddings\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# settings\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "\n",
        "# function tools\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
        "    return a * b\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
        "    return a + b\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "# rag pipeline\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "budget_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine,\n",
        "    name=\"canadian_budget_2023\",\n",
        "    description=\"A RAG engine with some basic facts about the 2023 Canadian federal budget.\",\n",
        ")\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    [multiply_tool, add_tool, budget_tool], verbose=False\n",
        ")\n",
        "\n",
        "response = agent.chat(\"How much exactly was allocated to a tax credit to promote investment in green technologies in the 2023 Canadian federal budget?\")\n",
        "\n",
        "print(response)\n",
        "\n",
        "response = agent.chat(\"How much was allocated to a implement a means-tested dental care program in the 2023 Canadian federal budget?\")\n",
        "\n",
        "print(response)\n",
        "\n",
        "response = agent.chat(\"How much was the total of those two allocations added together? Use a tool to answer any questions.\")\n",
        "\n",
        "# 20bn $ + 13bn $ = 33bn $\n",
        "print(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "05079d68-06a1-47eb-9d73-d4ec4fe3a979",
        "id": "1QrsBjjNW5Vf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 2023 Canadian federal budget allocated twenty billion dollars to a tax credit to promote investment in green technologies.\n",
            "$13 billion was allocated to implement a means-tested dental care program in the 2023 Canadian federal budget.\n",
            "The total allocation for both the green technology tax credit and the means-tested dental care program was $33 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LlamaHub"
      ],
      "metadata": {
        "id": "FUp0hbW9Y5Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-tools-yahoo-finance"
      ],
      "metadata": {
        "id": "q_RxJQqdY0-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core import Settings\n",
        "from llama_index.tools.yahoo_finance import YahooFinanceToolSpec\n",
        "\n",
        "# get API key and create embeddings\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# settings\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# function tools\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
        "    return a * b\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
        "    return a + b\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "finance_tools = YahooFinanceToolSpec().to_tool_list()\n",
        "finance_tools.extend([multiply_tool, add_tool])\n",
        "\n",
        "agent = ReActAgent.from_tools(finance_tools, verbose=False)\n",
        "\n",
        "response = agent.chat(\"What is the current price of NVDA?\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ZRoMFAqEavW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install llama-index-tools-tavily-research"
      ],
      "metadata": {
        "id": "ASg2h9tVcbjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up Tavily tool\n",
        "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "# get API key and create embeddings\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# settings\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# Tavily API key\n",
        "\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "# Use the TAVILY_API_KEY variable instead of the string literal\n",
        "tavily_tool = TavilyToolSpec(\n",
        "    api_key=TAVILY_API_KEY,\n",
        ")\n",
        "\n",
        "tavily_tool_list = tavily_tool.to_tool_list()\n",
        "for tool in tavily_tool_list:\n",
        "    print(tool.metadata.name)\n",
        "print(\"---------------------\\n\")\n",
        "\n",
        "res = tavily_tool.search(\"What happened in the latest Burning Man festival?\", max_results=3)\n",
        "for doc in res:\n",
        "    print(doc)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "2Y0EEnRdglKM",
        "outputId": "232d081d-7db1-459b-dc92-5a7a76f95ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "search\n",
            "---------------------\n",
            "\n",
            "Doc ID: 155c5983-8e96-4659-8a81-3fe72b6ac59d\n",
            "Text: Thank you for registering Please refresh the page or navigate to\n",
            "another page on the site to be automatically logged inPlease refresh\n",
            "your browser to be logged in Burning Man attendees share ordeal with\n",
            "urine bottles as clean-up begins after exodus – updates Organisers\n",
            "have three weeks to clear the sprawling Nevada desert campsite\n",
            "Organisers hav...\n",
            "Doc ID: 807b6a47-af5d-4758-8b3b-a880fc0a7e4c\n",
            "Text: Burning Man Festival Tragedy: Woman Attendee Found Dead On First\n",
            "Day - Newsweek Attendees sit on a muddy desert plain on September 3,\n",
            "2023, after heavy rains turned the annual Burning Man festival site in\n",
            "Nevada's Black Rock desert into a mud pit. Attendees sit on a muddy\n",
            "desert plain on September 3, 2023, after heavy rains turned the annual\n",
            "Bur...\n",
            "Doc ID: 7ccc0ed8-52ab-4f71-b0b9-7fab31cbafa7\n",
            "Text: 2024 Burning Man opening day marred by attendee death, weather\n",
            "delay Opening day of Burning Man marred by woman's death, harsh\n",
            "weather conditions The opening day of 2024's week-long Burning Man\n",
            "festival was marred by tragedy and complications after one attendee\n",
            "died and gates opened 12 hours late due to weather conditions this\n",
            "weekend. The Burni...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "# Set up Tavily tool\n",
        "from llama_index.tools.tavily_research.base import TavilyToolSpec\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "# get API key and create embeddings\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# settings\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "# Tavily API key\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "# Use the TAVILY_API_KEY variable instead of the string literal\n",
        "tavily_tool = TavilyToolSpec(\n",
        "    api_key=TAVILY_API_KEY,\n",
        ")\n",
        "\n",
        "tavily_tool_list = tavily_tool.to_tool_list()\n",
        "for tool in tavily_tool_list:\n",
        "    print(tool.metadata.name)\n",
        "\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    tavily_tool_list, verbose=False\n",
        ")\n",
        "\n",
        "response = agent.chat(\n",
        "        \"Write a deep analysis in markdown syntax about the latest burning man floods\"\n",
        "    )\n",
        "print(response)"
      ],
      "metadata": {
        "id": "4zOmi5jefkD1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "2eb0b155-9be6-49e1-d8ea-63d141a0360d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "search\n",
            "# Burning Man 2023: A Deep Dive into the Devastating Floods\n",
            "\n",
            "The 2023 Burning Man festival, held in the Black Rock Desert of Nevada, experienced unprecedented rainfall, transforming the usually dry playa into a muddy quagmire and stranding tens of thousands of attendees. This event presented a unique confluence of logistical challenges, safety concerns, and a test of the festival's famed community spirit.\n",
            "\n",
            "## The Storm and its Impact:\n",
            "\n",
            "Unusually heavy rainfall, exacerbated by remnants of Tropical Storm Hilary, inundated the festival grounds.  The resulting mud rendered the playa largely impassable for vehicles, effectively trapping approximately 70,000 attendees.  This led to:\n",
            "\n",
            "* **Shelter-in-Place Orders:**  Festival organizers issued shelter-in-place orders, urging attendees to conserve food, water, and fuel.  The lack of vehicular access also hampered waste removal, leading to sanitation issues.\n",
            "* **Exodus Delays:** The planned \"Exodus,\" the mass departure of attendees at the festival's end, was significantly delayed.  Vehicles were unable to navigate the muddy conditions, leading to a multi-day standstill.\n",
            "* **Death Investigation:**  Tragically, at least one death occurred during the event, with authorities investigating the circumstances.\n",
            "* **Logistical Challenges:**  The mud presented significant challenges for emergency services, supply delivery, and general movement within the festival grounds.  Cell service was also severely disrupted.\n",
            "* **Community Response:** Despite the adversity, reports emerged highlighting the resilience and collaborative spirit of the Burning Man community.  Attendees helped each other, sharing resources and assisting those in need.  Celebrities like Diplo and Chris Rock even walked miles through the mud before receiving rides out.\n",
            "\n",
            "## Analysis:\n",
            "\n",
            "The 2023 Burning Man floods exposed several vulnerabilities:\n",
            "\n",
            "* **Preparedness for Extreme Weather:** While the festival has experienced rain in the past, the intensity and duration of this year's storm exceeded expectations.  The event's preparedness for such extreme weather conditions needs reevaluation.\n",
            "* **Infrastructure Limitations:** The festival's infrastructure, designed for a dry environment, proved inadequate to handle the volume of water.  Improved drainage systems and contingency plans for extreme weather are crucial.\n",
            "* **Communication Systems:**  The disruption of cell service highlighted the need for robust and redundant communication systems to ensure effective emergency response and information dissemination.\n",
            "* **Waste Management:** The overflow of toilets due to the inability to remove waste underscores the need for more resilient sanitation systems.\n",
            "\n",
            "## Lessons Learned:\n",
            "\n",
            "The 2023 Burning Man floods serve as a stark reminder of the importance of comprehensive disaster preparedness, robust infrastructure, and effective communication in large-scale events, even those held in seemingly predictable environments.  The event's organizers, local authorities, and attendees themselves must learn from this experience to mitigate the risks of future events.  This includes developing more sophisticated weather forecasting and response plans, investing in improved infrastructure, and ensuring reliable communication systems.  The remarkable community spirit displayed during the crisis, however, offers a beacon of hope and a testament to the human capacity for resilience in the face of adversity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflows (Darba plūsmas)"
      ],
      "metadata": {
        "id": "-rhsHqZ-dUsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install llama-index-utils-workflow"
      ],
      "metadata": {
        "id": "vJK8012Hd0GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-step workflow\n",
        "\n",
        "from llama_index.core.workflow import (\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Workflow,\n",
        "    step,\n",
        ")\n",
        "class MyWorkflow(Workflow):\n",
        "    @step\n",
        "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
        "        # do something here\n",
        "        return StopEvent(result=\"Hello, world!\")\n",
        "\n",
        "\n",
        "w = MyWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YK6--uEeMsV",
        "outputId": "23ae0ca0-5172-4645-8788-4b016e1ef513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualise the workflow\n",
        "\n",
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(MyWorkflow, filename=\"basic_workflow.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BYz_r0xpFS2",
        "outputId": "787e0f38-d014-4836-dd73-aecd4da292e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'NoneType'>\n",
            "<class '__main__.FirstEvent'>\n",
            "<class 'llama_index.core.workflow.events.StopEvent'>\n",
            "<class '__main__.SecondEvent'>\n",
            "basic_workflow.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-step workfow\n",
        "\n",
        "from llama_index.core.workflow import (\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Workflow,\n",
        "    step,\n",
        "    Event,\n",
        ")\n",
        "\n",
        "# Define custom events classes that inherit from Event class\n",
        "\n",
        "class FirstEvent(Event):\n",
        "    first_output: str\n",
        "\n",
        "\n",
        "class SecondEvent(Event):\n",
        "    second_output: str\n",
        "\n",
        "# Define the workflow that inherits from Workflow class\n",
        "\n",
        "class MyWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent) -> FirstEvent:\n",
        "        print(ev.first_input)\n",
        "        return FirstEvent(first_output=\"First step complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: FirstEvent) -> SecondEvent:\n",
        "        print(ev.first_output)\n",
        "        return SecondEvent(second_output=\"Second step complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_three(self, ev: SecondEvent) -> StopEvent:\n",
        "        print(ev.second_output)\n",
        "        return StopEvent(result=\"Workflow complete.\")\n",
        "\n",
        "\n",
        "w = MyWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run(first_input=\"Start the workflow.\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU6JOZDKpGYI",
        "outputId": "c7f766b8-fca5-4137-f203-2e32e8010456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start the workflow.\n",
            "First step complete.\n",
            "Second step complete.\n",
            "Workflow complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualise the workflow\n",
        "\n",
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(MyWorkflow, filename=\"multi_step_workflow.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVoPJA8Wpudl",
        "outputId": "e0ab7074-916a-4a96-dcc6-cc245b6d3959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'NoneType'>\n",
            "<class '__main__.FirstEvent'>\n",
            "<class 'llama_index.core.workflow.events.StopEvent'>\n",
            "<class '__main__.SecondEvent'>\n",
            "multi_step_workflow.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Branching and Looping (Zarošanās un cikli)"
      ],
      "metadata": {
        "id": "e-wuDDQZqx-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop flow (Cikliska  darba plūsma)"
      ],
      "metadata": {
        "id": "uxYdNAg5r6LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import (\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Workflow,\n",
        "    step,\n",
        "    Event,\n",
        ")\n",
        "import random\n",
        "\n",
        "# Define events\n",
        "\n",
        "class LoopEvent(Event):\n",
        "    loop_output: str\n",
        "\n",
        "class FirstEvent(Event):\n",
        "    first_output: str\n",
        "\n",
        "# Defie workfow\n",
        "\n",
        "class LoopWorkflow(Workflow):\n",
        "  @step\n",
        "  async def step_one(self, ev: StartEvent | LoopEvent) -> FirstEvent | LoopEvent:\n",
        "      if random.randint(0, 1) == 0:\n",
        "          print(\"Bad thing happened\")\n",
        "          return LoopEvent(loop_output=\"Back to step one.\")\n",
        "      else:\n",
        "          print(\"Good thing happened\")\n",
        "          return FirstEvent(first_output=\"First step complete.\")\n",
        "  @step\n",
        "  async def step_two(self, ev: FirstEvent) -> SecondEvent:\n",
        "        print(ev.first_output)\n",
        "        return SecondEvent(second_output=\"Second step complete.\")\n",
        "\n",
        "  @step\n",
        "  async def step_three(self, ev: SecondEvent) -> StopEvent:\n",
        "        print(ev.second_output)\n",
        "        return StopEvent(result=\"Workflow complete.\")\n",
        "\n",
        "\n",
        "w = LoopWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run(first_input=\"Start the workflow.\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a05444-7f17-4a04-e7f2-2088b8aa62c9",
        "id": "85INvd1r07Bq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good thing happened\n",
            "First step complete.\n",
            "Second step complete.\n",
            "Workflow complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(LoopWorkflow, filename=\"loop_workflow.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0SWNSrVtGOA",
        "outputId": "0919260d-8232-43fd-d81f-61e269dd3a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'NoneType'>\n",
            "<class '__main__.FirstEvent'>\n",
            "<class '__main__.LoopEvent'>\n",
            "<class 'llama_index.core.workflow.events.StopEvent'>\n",
            "<class '__main__.SecondEvent'>\n",
            "loop_workflow.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import (\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Workflow,\n",
        "    step,\n",
        "    Event,\n",
        ")\n",
        "import random\n",
        "\n",
        "class FailedEvent(Event):\n",
        "    error: str\n",
        "\n",
        "class QueryEvent(Event):\n",
        "    query: str\n",
        "\n",
        "class LoopExampleFlow(Workflow):\n",
        "\n",
        "    @step()\n",
        "    async def answer_query(self, ev: StartEvent | QueryEvent ) -> FailedEvent | StopEvent:\n",
        "        query = ev.query\n",
        "        # try to answer the query\n",
        "        random_number = random.randint(0, 1)\n",
        "        if (random_number == 0):\n",
        "            return FailedEvent(error=\"Failed to answer the query.\")\n",
        "        else:\n",
        "            return StopEvent(result=\"The answer to your query\")\n",
        "\n",
        "    @step()\n",
        "    async def improve_query(self, ev: FailedEvent) -> QueryEvent | StopEvent:\n",
        "        # improve the query or decide it can't be fixed\n",
        "        random_number = random.randint(0, 1)\n",
        "        if (random_number == 0):\n",
        "            return QueryEvent(query=\"Here's a better query.\")\n",
        "        else:\n",
        "            return StopEvent(result=\"Your query can't be fixed.\")\n",
        "\n",
        "l = LoopExampleFlow(timeout=10, verbose=True)\n",
        "result = await l.run(query=\"What's LlamaIndex?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkBt5BlbvTyU",
        "outputId": "14d54c01-5b67-4277-acbc-72fdac0c4d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step answer_query\n",
            "Step answer_query produced event FailedEvent\n",
            "Running step improve_query\n",
            "Step improve_query produced event StopEvent\n",
            "Your query can't be fixed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(LoopExampleFlow, filename=\"loop_example_workflow.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rdpxv7bvfQa",
        "outputId": "826143b5-7976-492c-9e1d-65626d02e6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'NoneType'>\n",
            "<class '__main__.FailedEvent'>\n",
            "<class 'llama_index.core.workflow.events.StopEvent'>\n",
            "<class '__main__.QueryEvent'>\n",
            "<class 'llama_index.core.workflow.events.StopEvent'>\n",
            "loop_example_workflow.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Branching flow (Zarošanās plūsmaplūsma)"
      ],
      "metadata": {
        "id": "qz2m1y_dycRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import (\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Workflow,\n",
        "    step,\n",
        "    Event,\n",
        ")\n",
        "\n",
        "class BranchA1Event(Event):\n",
        "    payload: str\n",
        "\n",
        "\n",
        "class BranchA2Event(Event):\n",
        "    payload: str\n",
        "\n",
        "\n",
        "class BranchB1Event(Event):\n",
        "    payload: str\n",
        "\n",
        "\n",
        "class BranchB2Event(Event):\n",
        "    payload: str\n",
        "\n",
        "\n",
        "class BranchWorkflow(Workflow):\n",
        "    @step\n",
        "    async def start(self, ev: StartEvent) -> BranchA1Event | BranchB1Event:\n",
        "        if random.randint(0, 1) == 0:\n",
        "            print(\"Go to branch A\")\n",
        "            return BranchA1Event(payload=\"Branch A\")\n",
        "        else:\n",
        "            print(\"Go to branch B\")\n",
        "            return BranchB1Event(payload=\"Branch B\")\n",
        "\n",
        "    @step\n",
        "    async def step_a1(self, ev: BranchA1Event) -> BranchA2Event:\n",
        "        print(ev.payload)\n",
        "        return BranchA2Event(payload=ev.payload)\n",
        "\n",
        "    @step\n",
        "    async def step_b1(self, ev: BranchB1Event) -> BranchB2Event:\n",
        "        print(ev.payload)\n",
        "        return BranchB2Event(payload=ev.payload)\n",
        "\n",
        "    @step\n",
        "    async def step_a2(self, ev: BranchA2Event) -> StopEvent:\n",
        "        print(ev.payload)\n",
        "        return StopEvent(result=\"Branch A complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_b2(self, ev: BranchB2Event) -> StopEvent:\n",
        "        print(ev.payload)\n",
        "        return StopEvent(result=\"Branch B complete.\")\n",
        "\n",
        "b = BranchWorkflow(timeout=10, verbose=True)\n",
        "result = await b.run(query=\"What's LlamaIndex?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziIF7gyUuXRG",
        "outputId": "d3ce02eb-bf8d-4a52-f4a2-fef63a7b6e42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running step start\n",
            "Go to branch B\n",
            "Step start produced event BranchB1Event\n",
            "Running step step_b1\n",
            "Branch B\n",
            "Step step_b1 produced event BranchB2Event\n",
            "Running step step_b2\n",
            "Branch B\n",
            "Step step_b2 produced event StopEvent\n",
            "Branch B complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(BranchWorkflow, filename=\"branch_workflow.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7LURfAFuhvc",
        "outputId": "4443bb84-43f2-4acb-d196-676d3c5d13ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'NoneType'>\n",
            "<class '__main__.BranchA1Event'>\n",
            "<class '__main__.BranchB1Event'>\n",
            "<class '__main__.BranchA2Event'>\n",
            "<class 'llama_index.core.workflow.events.StopEvent'>\n",
            "<class '__main__.BranchB2Event'>\n",
            "<class 'llama_index.core.workflow.events.StopEvent'>\n",
            "branch_workflow.html\n"
          ]
        }
      ]
    }
  ]
}