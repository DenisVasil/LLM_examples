{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXbBP77JBhige2BE03f0Rt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DenisVasil/LLM_examples/blob/main/Llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw_CNAC8xWxr"
      },
      "outputs": [],
      "source": [
        "! pip install llama-index-llms-gemini llama-index\n",
        "! pip install llama-index-embeddings-gemini\n",
        "! pip install llama-index 'google-generativeai>=0.3.0' matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Models (čata modeļi)"
      ],
      "metadata": {
        "id": "s0i6-5mW20nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "resp = llm.complete(\"Write a poem about a magic backpack\")\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "N1jbAjFLxglL",
        "outputId": "433684ae-fc98-4dab-89f0-92988004b834"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A backpack worn, of woven thread,\n",
            "Not leather tough, nor canvas spread,\n",
            "But shimmering silk, a mystic hue,\n",
            "With starlight stitched, and morning dew.\n",
            "\n",
            "No zippers found, no clasps to see,\n",
            "Just whispered words, a key to free\n",
            "Its hidden depths, a boundless space,\n",
            "Where time and tide find no embrace.\n",
            "\n",
            "A crumpled map, a dragon's scale,\n",
            "A phoenix feather, a whispered tale,\n",
            "A tiny vial, a moonbeam bright,\n",
            "All held within, in fading light.\n",
            "\n",
            "A wish unspoken, softly breathed,\n",
            "And from its folds, a treasure wreathed,\n",
            "A rainbow bridge, a silver stream,\n",
            "A sleeping giant, a waking dream.\n",
            "\n",
            "So carry it close, this magic hold,\n",
            "A story whispered, brave and bold,\n",
            "For in its depths, adventure sleeps,\n",
            "And every wonder, softly keeps.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "\n",
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
        "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
        "    ChatMessage(\n",
        "        role=\"user\", content=\"Help me decide what to have for dinner.\"\n",
        "    ),\n",
        "]\n",
        "resp = llm.chat(messages)\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "QxKeHp4Mz2Wl",
        "outputId": "d2036d7c-85e0-4b4d-e74b-d83d7f929916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Okay, let's figure out what you're in the mood for! To help me narrow it down, tell me:\n",
            "\n",
            "* **What kind of cuisine are you craving?** (e.g., Italian, Mexican, Asian, American, etc.)\n",
            "* **What ingredients do you have on hand?** (This will help avoid a trip to the store if possible.)\n",
            "* **How much time do you have to cook?** (Quick and easy, or something more elaborate?)\n",
            "* **What's your skill level in the kitchen?** (Beginner, intermediate, advanced?)\n",
            "* **Any dietary restrictions or preferences?** (Vegetarian, vegan, gluten-free, etc.)\n",
            "\n",
            "\n",
            "Once I have this information, I can give you some personalized suggestions!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resp = llm.stream_complete(\n",
        "    \"The story of Sourcrust, the bread creature, is really interesting. It all started when...\"\n",
        ")"
      ],
      "metadata": {
        "id": "cVN3IcXoz5z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in resp:\n",
        "    print(r.text, end=\"\")"
      ],
      "metadata": {
        "id": "RKU5QU430NO9",
        "outputId": "a91ab33e-2220-43d2-c51b-9721690e3132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The story of Sourcrust, the bread creature, is really interesting. It all started when a mischievous baker, old Barnaby Buttercup, accidentally dropped a sourdough starter into a vat of enchanted, glowing goo he'd been brewing for a particularly ambitious rye bread.  The goo, a byproduct of his experiments with moonbeams and forgotten baking secrets, reacted violently.  It bubbled, hissed, and pulsed with an eerie, yeasty light before solidifying into a surprisingly humanoid form – a creature made entirely of sourdough bread, with crusty skin, a doughy body, and eyes that shimmered like caramelized sugar.  Barnaby, initially terrified, soon discovered that Sourcrust wasn't malicious, but rather possessed a surprisingly gentle nature and an insatiable appetite for butter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "18NrXAxW3HaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "# get API key and create embeddings\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "embeddings = embed_model.get_text_embedding(\"Google Gemini Embeddings.\")\n",
        "print(f\"Dimension of embeddings: {len(embeddings)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "596B0AbiS0rk",
        "outputId": "0f0b8ff0-de40-4d08-c764-a3147d944e34"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of embeddings: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering txt\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the first programs Paul Graham tried writing?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "B9M8JwM_bHvH",
        "outputId": "316d39b8-555e-4475-afba-54c445da418d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham's first programming attempts were on an IBM 1401, using an early version of Fortran.  These programs were written on punch cards, and their output was printed on a loud printer.  Due to limitations in input methods and his mathematical knowledge at the time, the programs were quite basic.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uBMJPe4sjJ6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "167b1ada-2170-499d-8e56-f995ef3ebba2",
        "id": "5UnF1TEBjKTa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The design goals of AUTO GEN STUDIO center around three core objectives:  rapid prototyping, developer tooling, and reusable templates.  Rapid prototyping focuses on creating a development environment where developers can quickly define agent configurations and assemble them into multi-agent workflows.  Developer tooling aims to provide resources that help developers understand and debug agent behaviors, thereby improving multi-agent systems.  Finally, reusable templates offer a gallery of shareable templates to jumpstart agent workflow creation, establishing shared standards and best practices within the field.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persisting the RAG\n",
        "import os.path\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "# check if storage already exists\n",
        "PERSIST_DIR = \"./storage\"\n",
        "if not os.path.exists(PERSIST_DIR):\n",
        "    # load the documents and create the index\n",
        "    documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # store it for later\n",
        "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "else:\n",
        "    # load the existing index\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
        "    index = load_index_from_storage(storage_context)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d160daac-306d-4bee-8f52-f71c13059d2f",
        "id": "EKETBF9NlkLV"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The design goals of AUTO GEN STUDIO center around three core objectives:  rapid prototyping, developer tooling, and reusable templates.  Rapid prototyping aims to create a development environment where developers can quickly define agent configurations and assemble them into multi-agent workflows.  Developer tooling focuses on providing resources to understand and debug agent behaviors, improving multi-agent systems.  Reusable templates offer a gallery of shareable templates to jumpstart agent workflow creation, establishing shared standards and best practices for wider adoption of multi-agent solutions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "iP4sIGr75evr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-vector-stores-chroma"
      ],
      "metadata": {
        "id": "UV0xqMdD-D0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vectorstore\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "import os.path\n",
        "\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "# load some documents\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "\n",
        "# initialize client, setting path to save data\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# create collection\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "\n",
        "# assign chroma as the vector_store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# create your index\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")\n",
        "\n",
        "# create a query engine and query\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What are the first programs Paul Graham tried writing?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "QPUO2huq56Vx",
        "outputId": "82a51e4d-38ff-4ccd-82d4-0ae2d203e988"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author's first programming attempts were on an IBM 1401, using an early version of Fortran.  These programs were written on punch cards, and their output was printed on a loud printer.  Due to limitations in input methods and the author's mathematical knowledge at the time, the programs were quite basic and not particularly memorable.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query existng vectorstore\n",
        "\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "# initialize client\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# get collection\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "\n",
        "# assign chroma as the vector_store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# load your index from stored vectors\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store, storage_context=storage_context\n",
        ")\n",
        "\n",
        "# create a query engine\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"Which programming languages did Paul use?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "yHOp83M-_bII",
        "outputId": "389732de-0bdc-4ef4-982d-d69c453bfdee"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The author used Lisp (specifically, Bel, a Lisp he created), and Arc.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the vectortore exists and retrieve the data\n",
        "\n",
        "import os\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the Google API key\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Set up Gemini embeddings and LLM\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "# Path to the ChromaDB store\n",
        "chroma_db_path = \"./chroma_db\"\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "db = chromadb.PersistentClient(path=chroma_db_path)\n",
        "\n",
        "# Get or create the collection\n",
        "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
        "\n",
        "# Assign Chroma as the vector store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Check if vector store exists by looking for the ChromaDB path\n",
        "if not os.path.exists(chroma_db_path):\n",
        "    # Create vector store if it doesn't exist\n",
        "    print(\"Vector store not found. Creating a new one...\")\n",
        "    # Load documents from a directory\n",
        "    documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "\n",
        "    # Create a new index\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents, storage_context=storage_context\n",
        "    )\n",
        "else:\n",
        "    # Load the existing vector store\n",
        "    print(\"Vector store found. Loading from existing store...\")\n",
        "    index = VectorStoreIndex.from_vector_store(\n",
        "        vector_store, storage_context=storage_context\n",
        "    )\n",
        "\n",
        "# Create a query engine\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Query the index\n",
        "query = \"What are the first programs Paul Graham tried writing?\"\n",
        "response = query_engine.query(query)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "peagUr88BPUF",
        "outputId": "a7074482-fc1f-4bde-e739-45c3bda0cbab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store found. Loading from existing store...\n",
            "Paul Graham's first programming attempts were on an IBM 1401, using an early version of Fortran.  He wrote programs that didn't require input, such as calculating approximations of pi, because the only input option was data from punched cards, which he didn't have access to.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbots (čatboti)"
      ],
      "metadata": {
        "id": "qgXgnlvE3ODu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"best\", llm=llm, verbose=True)\n",
        "response = chat_engine.chat(\"What are the design goals? Please give me detailes about them\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "amERfyUVzA_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple RAG quering pdf\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "Settings.llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "model_name = \"models/embedding-001\"\n",
        "\n",
        "Settings.embed_model = GeminiEmbedding(\n",
        "    model_name=model_name, api_key=GOOGLE_API_KEY, title=\"this is a document\"\n",
        ")\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "chat_engine = index.as_chat_engine(chat_mode=\"best\", llm=llm, verbose=True)\n",
        "\n",
        "while True:\n",
        "    text_input = input(\"User: \")\n",
        "    if text_input == \"exit\":\n",
        "        break\n",
        "    response = chat_engine.chat(text_input)\n",
        "    print(f\"Agent: {response}\")"
      ],
      "metadata": {
        "id": "-BW2Gfgh3kN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents"
      ],
      "metadata": {
        "id": "wLzlhD7SDUU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# get API key and create embeddings\n",
        "\n",
        "llm = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "# initialize the tools\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "\n",
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "# initializing the agent\n",
        "\n",
        "agent = ReActAgent.from_tools([multiply_tool, add_tool], llm=llm, verbose=True)\n",
        "\n",
        "response = agent.chat(\"What is 20+(2*4)? Use a tool to calculate every step.\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "IXagIK_5DTTC",
        "outputId": "1b4116e4-957f-4c9e-c292-350aa06e2c30"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step c5542f0a-2094-42cc-959c-8c2962469962. Step input: What is 20+(2*4)? Use a tool to calculate every step.\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to calculate 2 * 4 first, then add the result to 20.\n",
            "Action: multiply\n",
            "Action Input: {'a': 2, 'b': 4}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 8\n",
            "\u001b[0m> Running step 2d3c2b98-1201-4fbf-be0f-7027e5347218. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: The result of 2 * 4 is 8. Now I need to add 20 and 8.\n",
            "Action: add\n",
            "Action Input: {'a': 20, 'b': 8}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 28\n",
            "\u001b[0m> Running step e8305654-268d-4cdf-82ed-de4823b14347. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: 28\n",
            "\u001b[0m28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8IELeGxuEJ0o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}